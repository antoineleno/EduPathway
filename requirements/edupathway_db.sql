-- MySQL dump 10.13  Distrib 8.0.40, for Linux (x86_64)
--
-- Host: localhost    Database: edupathway_db
-- ------------------------------------------------------
-- Server version	8.0.40-0ubuntu0.24.04.1

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8mb4 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `course`
--
use edupathway_db;
DROP TABLE IF EXISTS `course`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `course` (
  `name` varchar(45) NOT NULL,
  `description` text NOT NULL,
  `user_id` varchar(60) NOT NULL,
  `program_id` varchar(60) NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `name` (`name`),
  KEY `user_id` (`user_id`),
  KEY `program_id` (`program_id`),
  CONSTRAINT `course_ibfk_1` FOREIGN KEY (`user_id`) REFERENCES `user` (`id`) ON DELETE CASCADE,
  CONSTRAINT `course_ibfk_2` FOREIGN KEY (`program_id`) REFERENCES `program` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `course`
--

LOCK TABLES `course` WRITE;
/*!40000 ALTER TABLE `course` DISABLE KEYS */;
INSERT INTO `course` VALUES ('Advanced Machine Learning Techniques','Covers gradient boosting, ensemble methods, and SVMs.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','020578cb-0b83-465e-959d-400cecfb415e','2025-01-10 16:24:48','2025-01-11 00:24:48'),('Deep Learning with TensorFlow','Focuses on neural networks, CNNs, and RNNs using TensorFlow and Keras.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','09ffa3d0-3468-4cf9-8bf2-1acb59b10226','2025-01-10 16:24:51','2025-01-11 00:24:51'),('Statistical Modeling for Data Science','Explains regression, ANOVA, and time-series analysis techniques.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','160ee69a-9374-40d6-bd9e-9ec9e865da52','2025-01-10 16:24:36','2025-01-11 00:24:36'),('Artificial Intelligence in Games','Learn how to implement AI techniques in games. This includes creating NPC behaviors, pathfinding, decision trees, and using AI to make gameplay more challenging.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','16db8a4b-605d-4dfb-9ae3-b573f274f32d','2025-01-11 08:14:04','2025-01-11 16:14:04'),('Natural Language Processing (NLP)','Teaches text preprocessing, sentiment analysis, and topic modeling.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','1899ca2c-be52-4920-a133-a46ad7933850','2025-01-10 16:24:57','2025-01-11 00:24:57'),('Cloud Security','Focusing on securing cloud infrastructures and services. Topics include cloud service models (IaaS, PaaS, SaaS), cloud security controls, data security, identity management, and risk in the cloud.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','1dc30bb9-ba2d-4742-940d-a37865af1b30','2025-01-11 08:10:57','2025-01-11 16:10:57'),('Security Policies & Risk Management','Legal and policy issues in cybersecurity, focusing on risk assessment. Topics include risk management frameworks, legal regulations (GDPR, HIPAA, etc.), compliance, and security governance.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','26537c75-7730-4d73-832a-73ccb8e7009d','2025-01-11 08:10:56','2025-01-11 16:10:56'),('Advanced Threat Detection and Mitigation','Techniques to detect and respond to sophisticated cyber threats. Topics include advanced persistent threats (APT), threat hunting, SIEM, and anomaly detection.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','269900af-4d27-4fc5-8155-bc0e2a581874','2025-01-11 08:11:00','2025-01-11 16:11:00'),('Cloud Computing for Data Science','Covers deploying and scaling data science projects in the cloud.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','28e0e1f6-2cd3-470e-8e05-19332a95bf21','2025-01-10 16:25:03','2025-01-11 00:25:03'),('Calculus for Machine Learning','Focuses on differentiation, integration, and multivariate calculus with practical examples.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','2e4a681c-5ea5-4269-ad08-fa671500a5d1','2025-01-10 16:24:03','2025-01-11 00:24:03'),('3D Game Development','Learn the fundamentals of 3D game development, including 3D graphics, physics, animation, and object interaction within a game engine.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','3853c390-bfd3-4707-9f9f-33a6883b327d','2025-01-11 08:14:02','2025-01-11 16:14:02'),('Introduction to Cybersecurity','Overview of core concepts in cybersecurity, including the CIA triad, encryption, network security basics, authentication, and authorization.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','38fe95ce-41fd-426a-aa4a-21d6e8dc7be2','2025-01-11 08:10:48','2025-01-11 16:10:48'),('Game Physics','Learn about the physics systems that drive many of the interactions in modern games, including motion, collision detection, and realistic object behavior.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','39c4d21a-78e6-400a-85d9-64a53c033a44','2025-01-11 08:14:03','2025-01-11 16:14:03'),('Multiplayer Game Development','Explore the challenges and techniques in developing multiplayer games, including networking, client-server architecture, synchronization, and latency issues.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','3e373f54-cf16-4dfc-86c3-62a8c2e3d417','2025-01-11 08:14:05','2025-01-11 16:14:05'),('Supervised Learning: Regression','Teaches linear regression, decision trees, and random forests.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','4012e606-0ada-41c4-923b-3e128463e27a','2025-01-10 16:24:42','2025-01-11 00:24:42'),('Cryptography','Focuses on encryption techniques to secure communication. Topics include symmetric/asymmetric encryption, hashing, digital signatures, public key infrastructure (PKI), SSL/TLS, and cryptanalysis.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','40158ca1-fece-45f4-a35b-68d03f4fc958','2025-01-11 08:10:49','2025-01-11 16:10:49'),('Exploratory Data Analysis (EDA)','Covers identifying patterns, outliers, and insights in datasets.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','47973087-99e9-44c2-b87e-fe185054256d','2025-01-10 16:24:33','2025-01-11 00:24:33'),('Virtual Reality Game Development','Learn how to create immersive virtual reality (VR) games using Unity and other tools. Understand VR interaction, motion tracking, and VR design principles.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','48a4e72d-8651-424e-9b6a-63f26c8808be','2025-01-11 08:14:06','2025-01-11 16:14:06'),('Unsupervised Learning: Clustering','Explains clustering algorithms and dimensionality reduction techniques.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','509fa487-d1ff-433c-b3d4-2665f2d495a7','2025-01-10 16:24:45','2025-01-11 00:24:45'),('Data Manipulation with Pandas','Introduces data cleaning, merging, and reshaping using the Pandas library.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','563d1df9-c558-496e-bcce-da2c8a2c681b','2025-01-10 16:24:21','2025-01-11 00:24:21'),('Big Data Analytics with PySpark','Explains data analytics on large datasets using PySpark.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','5d35a77a-0981-45da-81b6-24aa87b34b09','2025-01-10 16:25:09','2025-01-11 00:25:09'),('Operating Systems & Security','Examining operating system security vulnerabilities and methods to protect systems. Topics include OS architecture, memory management, file systems, security patches, privilege escalation, and rootkits.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','630e2b0c-54b6-4854-abdf-27e003eec9f5','2025-01-11 08:10:50','2025-01-11 16:10:50'),('Reinforcement Learning Basics','Explores decision-making and reward systems in AI using RL algorithms.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','6b6ec940-2b7b-463f-9e45-ff201004f7d7','2025-01-10 16:25:00','2025-01-11 00:25:00'),('Malware Analysis and Prevention','Focuses on the analysis and prevention of malicious software. Topics include types of malware, reverse engineering, sandboxing, static and dynamic analysis, and malware detection techniques.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','7660f274-b16e-4bf9-9d61-f7b5c1540a5e','2025-01-11 08:10:58','2025-01-11 16:10:58'),('Interactive Dashboards with Tableau','Teaches how to create interactive dashboards for business insights.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','76b7a4c4-161b-4ed5-81ec-27193a924685','2025-01-10 16:24:30','2025-01-11 00:24:30'),('Data Visualization with Matplotlib','Focuses on creating visualizations for data exploration and presentation.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','2025-01-10 16:24:27','2025-01-11 00:24:27'),('Digital Forensics','The art of investigating cybercrimes and recovering digital evidence. Topics include disk forensics, network forensics, mobile device forensics, malware analysis, and legal considerations.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','7f551e99-cb1d-49da-9800-0c79f57deb88','2025-01-11 08:10:53','2025-01-11 16:10:53'),('Network Security','Deep dive into securing computer networks. Topics include firewalls, VPNs, IDS/IPS systems, access control, security policies, network monitoring, and threat analysis.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','808b1a42-1f58-4186-961a-4ff961ca3505','2025-01-11 08:10:51','2025-01-11 16:10:51'),('Probability and Statistics','Teaches Bayesian probability, distributions, and hypothesis testing for data analysis.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','83e631af-5b05-4a16-bd42-a85bdb3b542e','2025-01-10 16:24:06','2025-01-11 00:24:06'),('Big Data Tools: Hadoop & Spark','Explores distributed data processing and analysis using Hadoop and Spark.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','8634ec7e-c67d-47d3-8eed-337754f21202','2025-01-10 16:24:24','2025-01-11 00:24:24'),('Data Structures and Algorithms','Explores sorting, searching algorithms, and computational complexity.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','8df31585-b33f-40f8-a95a-b6f20a3435df','2025-01-10 16:24:12','2025-01-11 00:24:12'),('SQL for Data Science','Teaches SQL queries, joins, and data aggregation for database manipulation.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','8f9a4075-56f3-4a24-bca0-084cb2c1df0a','2025-01-10 16:24:18','2025-01-11 00:24:18'),('Game Monetization Strategies','Understand how to monetize games, including in-app purchases, advertising, and other business models that generate revenue from your game.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','93b2198b-184f-46c9-bdbf-dd083864a0aa','2025-01-11 08:14:07','2025-01-11 16:14:07'),('Networking Fundamentals','Introduces networking concepts that are crucial for cybersecurity professionals. Topics include OSI model, TCP/IP protocols, network topologies, IPv4 & IPv6, routers, firewalls, and DNS.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','9522d51c-10b9-4ba3-ad7b-68398d5ccb08','2025-01-11 08:10:47','2025-01-11 16:10:47'),('Privacy & Legal Aspects in Cybersecurity','Exploring privacy laws and ethical issues in cybersecurity. Topics include data privacy laws (GDPR, CCPA), digital rights, ethical dilemmas, and cybersecurity-related legal cases.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','96f8a969-e4ed-49b9-ac73-2eea4609963c','2025-01-11 08:11:01','2025-01-11 16:11:01'),('Capstone Project in Game Development','Complete a capstone project where students design, develop, and launch a game from concept to release, showcasing all the skills learned in the program.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','9c47e9ab-8d2b-418e-adb1-01fad08144aa','2025-01-11 08:14:08','2025-01-11 16:14:08'),('Penetration Testing','Learn how to identify vulnerabilities by simulating attacks on systems and networks. Topics include ethical hacking, tools like Metasploit, vulnerability scanning, exploits, and social engineering.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','a43bf1be-c004-441a-81db-7cd80c2219bc','2025-01-11 08:10:52','2025-01-11 16:10:52'),('Ethics in Data Science','Explores ethical considerations in AI, privacy, and data handling.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','af1709c2-9c15-4581-89bc-9acf66e06729','2025-01-10 16:25:12','2025-01-11 00:25:12'),('Capstone Project / Internship','Real-world application of knowledge in a practical project or internship. Work with a cybersecurity team to handle a real-world security issue, or research on advanced cybersecurity topics.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','af4caae6-c960-4f90-b6a8-2a35ad7db77f','2025-01-11 08:11:02','2025-01-11 16:11:02'),('Introduction to Game Development','An introduction to game development, focusing on fundamental game design principles, game programming, and building the first simple games.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','b353520d-c938-40ff-ab84-83512ff9124d','2025-01-11 08:13:59','2025-01-11 16:13:59'),('Incident Response & Disaster Recovery','Focuses on responding to security breaches and implementing recovery strategies. Topics include incident response plans, breach detection, recovery strategies, backups, and disaster recovery testing.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','beba9191-904d-4971-9d8e-de4a4dd1c0af','2025-01-11 08:10:54','2025-01-11 16:10:54'),('Foundations of Machine Learning','Introduces supervised and unsupervised learning concepts and algorithms.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','d25498d9-4c3c-4bf4-bf20-330c59578aa9','2025-01-10 16:24:39','2025-01-11 00:24:39'),('Linear Algebra for Data Science','Covers matrices, vector spaces, eigenvalues, and applications to data science.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','d756bb07-e5c7-40d9-8b31-dc9a859672a5','2025-01-10 16:24:00','2025-01-11 00:24:00'),('Time Series Analysis and Forecasting','Covers forecasting and trend analysis in time-based datasets.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','d75afbc3-5381-4d9c-9553-8a3a48021369','2025-01-10 16:24:54','2025-01-11 00:24:54'),('Intro to R for Data Analysis','Covers R programming basics for statistical analysis and data visualization.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','e95b2987-b1fc-4731-80cc-49f535b522fc','2025-01-10 16:24:15','2025-01-11 00:24:15'),('Game Programming with C++','Learn how to program games using C++. The course will cover C++ basics, object-oriented programming, and techniques used in game development.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','eecc4b45-f592-45e0-9589-179e53110481','2025-01-11 08:14:00','2025-01-11 16:14:00'),('Deploying Machine Learning Models','Teaches how to prepare and deploy machine learning models for production.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','2025-01-10 16:25:06','2025-01-11 00:25:06'),('Security in Web Development','Security challenges in web applications and how to mitigate them. Topics include SQL injection, XSS, CSRF, session hijacking, secure coding practices, and OWASP top 10.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','f546cfce-375b-40a0-8c4d-85b07e81bcd5','2025-01-11 08:10:59','2025-01-11 16:10:59'),('Intro to Python for Data Science','An introductory course on Python programming covering data types, loops, and functions.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981','f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','2025-01-10 16:24:09','2025-01-11 00:24:09'),('Game Design Fundamentals','Introduction to the principles of game design, including how to develop gameplay, story, and character concepts, and how to create a fun and engaging player experience.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','f88c0007-3d3b-470c-93b3-90b93d98c8c2','2025-01-11 08:14:01','2025-01-11 16:14:01'),('Introduction to Computer Science','Basic principles of programming and software development. Topics include data types, algorithms, basic programming (Python, C++), and object-oriented programming.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','f8fb365a-edd7-4081-bfb6-83138713c7f5','2025-01-11 08:10:46','2025-01-11 16:10:46'),('Ethical Hacking and Cyber Defense','Understanding and applying ethical hacking principles to defend against cyber threats. Topics include ethical hacking methodologies, vulnerability assessment, system hardening, and defense-in-depth strategies.','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','80392782-dfd2-4787-a9a3-567364ea0cbe','fb7a02f7-8eee-4c37-89b4-2183686f7057','2025-01-11 08:10:55','2025-01-11 16:10:55');
/*!40000 ALTER TABLE `course` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `enrollment`
--

DROP TABLE IF EXISTS `enrollment`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `enrollment` (
  `user_id` varchar(60) NOT NULL,
  `program_id` varchar(60) NOT NULL,
  `progress` int NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `user_id` (`user_id`),
  KEY `program_id` (`program_id`),
  CONSTRAINT `enrollment_ibfk_1` FOREIGN KEY (`user_id`) REFERENCES `user` (`id`) ON DELETE CASCADE,
  CONSTRAINT `enrollment_ibfk_2` FOREIGN KEY (`program_id`) REFERENCES `program` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `enrollment`
--

LOCK TABLES `enrollment` WRITE;
/*!40000 ALTER TABLE `enrollment` DISABLE KEYS */;
INSERT INTO `enrollment` VALUES ('fcff5d35-4d4f-4afa-a0dd-35d399d3884c','1c8b8375-f5f1-4539-82c2-75395f261981',4,'c27c8938-382c-4681-a34e-0d034bcb4ae5','2025-01-10 19:14:00','2025-01-11 03:14:00');
/*!40000 ALTER TABLE `enrollment` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `option`
--

DROP TABLE IF EXISTS `option`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `option` (
  `option` text NOT NULL,
  `quiz_id` varchar(60) NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `quiz_id` (`quiz_id`),
  CONSTRAINT `option_ibfk_1` FOREIGN KEY (`quiz_id`) REFERENCES `quiz` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `option`
--

LOCK TABLES `option` WRITE;
/*!40000 ALTER TABLE `option` DISABLE KEYS */;
INSERT INTO `option` VALUES ('Box Plot','752b1707-9577-462e-837d-c2044a97a932','00ed2777-985c-4ff0-9778-b231ffae5f56','2025-01-11 07:19:01','2025-01-11 15:19:01'),('They are used to increase the learning rate','fbeaf118-19a7-402f-b21b-07319c40d904','020347b4-1e50-4256-8bb5-90be31f29884','2025-01-11 05:05:36','2025-01-11 13:05:36'),('Vectors that have the same direction','8ae03fa3-c7c7-4887-b3e0-abda52780622','032dea87-0b60-41a6-9268-3a91bc3d9d0e','2025-01-10 19:00:58','2025-01-11 03:00:58'),('Array','91f55617-3e4a-4935-9d8c-cc7332c9a4ba','049a5289-f732-44a5-8607-e163f0ef16ce','2025-01-11 05:25:36','2025-01-11 13:25:36'),('The number of linearly independent rows or columns','0e89070d-41b4-462b-ab0e-7613defb0c7e','051fd712-bb83-46e9-b35b-d7ad3018645b','2025-01-10 18:51:42','2025-01-11 02:51:42'),('df.dropna()','cce41205-ba4d-4d1e-9cc4-65ba6b74d19d','05348ee7-e19b-4c78-ac15-64ffae78c931','2025-01-11 05:42:01','2025-01-11 13:42:01'),('Mean Squared Error (MSE)','efe5bf2a-42bb-4d85-9b5a-23cbe6d1595e','05bc0894-02a3-441c-a542-d7457ad99bcb','2025-01-11 07:16:19','2025-01-11 15:16:19'),('The result will be a matrix of size m x p','ba8f4b3c-33af-4998-8f22-8a528527b689','06d3e724-ffb3-42e3-b4ba-b150d148daa4','2025-01-10 18:53:50','2025-01-11 02:53:50'),('Matplotlib','066bc28a-fc4c-4a1f-9133-526c14e938c9','07844ddf-3098-43c9-92ae-69ede4c9ccae','2025-01-11 05:22:11','2025-01-11 13:22:11'),('Performing feature extraction using Principal Component Analysis (PCA)','6fa5d6fb-b048-4bef-8745-28c71ffc9e7e','07d50762-9bf8-4a37-a4cc-93debe68f170','2025-01-10 18:57:52','2025-01-11 02:57:52'),('Collecting data without considering the impact on privacy','52cc279d-69b8-454e-81c0-7222ef392280','082a4c27-9e77-4300-8593-09e689d27fd0','2025-01-11 07:06:44','2025-01-11 15:06:44'),('When an algorithm is trained on clean and balanced data','ce9b6bc5-8267-4e89-8955-e06dddc83bb3','08505288-2f48-4d3d-a553-710528aba413','2025-01-11 07:05:34','2025-01-11 15:05:34'),('The number of non-zero columns','0e89070d-41b4-462b-ab0e-7613defb0c7e','0b43f6da-45db-4cf6-a431-22b39b34f03a','2025-01-10 18:51:42','2025-01-11 02:51:42'),('df.merge()','1e3efca0-48a3-439d-bc99-982a357c5f89','0c189787-23e7-498e-8334-55a63bc57768','2025-01-11 05:45:16','2025-01-11 13:45:16'),('The area under a curve, representing the cumulative value','dcd98717-8f23-4303-ac04-80ccc5658f58','0c9d4992-4f45-4868-a377-60418c8ea9f6','2025-01-11 05:06:36','2025-01-11 13:06:36'),('To maximize the loss function','ea96ea25-d825-48a1-b823-1213a002f1e2','0d02e4c4-300c-49f6-8446-950160e43234','2025-01-11 05:04:43','2025-01-11 13:04:43'),('A one-dimensional array of numbers','f145621b-08ed-4d89-8e06-c8ee99885b3e','0df94448-ec30-40f8-8fb5-b0d51aad95ff','2025-01-10 18:47:55','2025-01-11 02:47:55'),('Normalizing input data','6fa5d6fb-b048-4bef-8745-28c71ffc9e7e','13e35825-6ebe-43bd-af95-e41677313fc6','2025-01-10 18:57:52','2025-01-11 02:57:52'),('Principal Component Analysis (PCA)','026761c4-f068-4a87-92cb-bd69ac4a984f','13ea90e8-b9b4-42b6-a831-ff9916475108','2025-01-11 05:16:32','2025-01-11 13:16:32'),('Matrix multiplication','924daf1c-d96d-4f5b-b75b-15b8278eb17b','1489105f-7574-4387-a641-281b322e9b22','2025-01-10 18:50:41','2025-01-11 02:50:41'),('Support Vector Machine','7c870830-47dd-427b-87c9-9474cb9f91d0','15ec4c51-6779-42da-bbf9-6aaa797a789d','2025-01-11 07:36:11','2025-01-11 15:36:11'),('To identify statistical relationships between features and churn','5d17d44d-557a-484c-acbd-ddffef22ec97','160edb4d-a8d1-487e-a721-3f2e94ac5937','2025-01-11 05:15:40','2025-01-11 13:15:40'),('WHERE','cf5dddf0-5e2e-4211-b0bf-f48ef0f7268c','1677a719-6267-4363-b675-147b5e505e36','2025-01-11 05:36:28','2025-01-11 13:36:28'),('To visualize the distribution of churn data','5d17d44d-557a-484c-acbd-ddffef22ec97','1b92e440-d4fc-43ff-9f24-dec2fcfbd481','2025-01-11 05:15:40','2025-01-11 13:15:40'),('Training the machine learning model','30e7dc37-d6e6-4676-9a48-753b981ed9eb','1eec8367-0758-4db8-bf47-907bd62e2f19','2025-01-11 07:54:34','2025-01-11 15:54:34'),('Linear regression','026761c4-f068-4a87-92cb-bd69ac4a984f','2135eb4f-f2aa-41f2-a7e1-2a52078355c9','2025-01-11 05:16:32','2025-01-11 13:16:32'),('To reduce the learning rate','032b83b7-ff53-4a00-b2ff-486f3a692d78','22b07471-667a-4efa-93f7-5970d8072390','2025-01-11 05:07:30','2025-01-11 13:07:30'),('Reward function','de38e5e8-9bc3-4a7a-b054-db1bb58a71c4','2952f20f-c407-4cb2-80f3-8b67be72f869','2025-01-11 07:46:55','2025-01-11 15:46:55'),('By splitting user-item interaction matrices into lower-dimensional matrices to identify patterns','dd2e2057-9fe9-444f-87cf-2255a84f786a','2a30298e-a58a-4a0c-960c-1a0efc3e9790','2025-01-10 18:59:53','2025-01-11 02:59:53'),('Publishing the model directly without using a web server','30e7dc37-d6e6-4676-9a48-753b981ed9eb','2ae1d4b6-fec3-405b-a993-e8db4fb7c647','2025-01-11 07:54:34','2025-01-11 15:54:34'),('Gaussian Mixture Models (GMM)','53dad63d-aafc-4ef6-b000-7cf09c9f8287','2afae9b8-6412-441f-a770-8f4402a1eda0','2025-01-11 07:10:02','2025-01-11 15:10:02'),('Transition matrix','de38e5e8-9bc3-4a7a-b054-db1bb58a71c4','2e7d8da0-833f-47bc-ac01-c554905669eb','2025-01-11 07:46:55','2025-01-11 15:46:55'),('They are used in dimensionality reduction techniques like PCA','a86af94e-a87e-41a2-9b2f-0e4a49bcc8e2','31e4113e-0fd0-4f90-9df0-d5828370bf25','2025-01-10 18:52:50','2025-01-11 02:52:50'),('Pandas','066bc28a-fc4c-4a1f-9133-526c14e938c9','33068a18-2d19-42f0-b5d0-8412345c1a66','2025-01-11 05:22:11','2025-01-11 13:22:11'),('spark.read.json()','7e226eb9-ca97-43d4-b14c-ba139d9f3c04','34e0aa53-bba6-4599-9cf0-1b0c3f6bef2b','2025-01-11 07:56:51','2025-01-11 15:56:51'),('Amazon SageMaker','0c1aceb6-f774-472e-a237-c77631fb4521','37633c69-82a0-40af-a707-27f1ddcb0697','2025-01-11 07:51:58','2025-01-11 15:51:58'),('Lasso Regression','00f34e62-b38c-4ad8-994d-e41aa1715c1a','37da33d7-a6cf-4f24-b28b-53d7832463ed','2025-01-11 07:33:30','2025-01-11 15:33:30'),('plt.plot()','dcb6084e-f432-44d6-932f-c31e9cb50ff3','3ced5301-35bd-4db5-b711-83f9628dccda','2025-01-11 05:55:21','2025-01-11 13:55:21'),('It acts as a neutral element in matrix multiplication (i.e., multiplying any matrix by the identity matrix leaves the matrix unchanged','590280a5-7a53-4d08-a3b8-26434b767581','40bdef4a-ba82-48ad-9157-90b9f306cd3e','2025-01-10 18:59:01','2025-01-11 02:59:01'),('To prevent overfitting by penalizing large coefficients','032b83b7-ff53-4a00-b2ff-486f3a692d78','4112db74-4b82-4c92-ba6f-a64edb60eb42','2025-01-11 05:07:30','2025-01-11 13:07:30'),('Regularly auditing algorithms for fairness and transparency','52cc279d-69b8-454e-81c0-7222ef392280','415d7c54-d816-4656-82bb-782695571a2f','2025-01-11 07:06:44','2025-01-11 15:06:44'),('When an algorithm systematically favors certain groups over others based on biased data','ce9b6bc5-8267-4e89-8955-e06dddc83bb3','42344d8b-7995-4d5a-a994-8a84f1934be2','2025-01-11 07:05:34','2025-01-11 15:05:34'),('Vectors that have the same magnitude','8ae03fa3-c7c7-4887-b3e0-abda52780622','46d1a05a-e202-4e32-8cd5-8b0e3a38ce04','2025-01-10 19:00:58','2025-01-11 03:00:58'),('SMOTE (Synthetic Minority Over-sampling Technique)','7c870830-47dd-427b-87c9-9474cb9f91d0','46e655ec-ba30-4c44-8804-8800f319f893','2025-01-11 07:36:11','2025-01-11 15:36:11'),('Using data without any consent from users','ecdc73ec-8484-4037-af4a-cb73b7d21b56','4a67589f-ed6e-4086-b87a-c2fc5c47ad4b','2025-01-11 07:04:47','2025-01-11 15:04:47'),('df.split()','1e3efca0-48a3-439d-bc99-982a357c5f89','4aaacba8-cfb7-4228-a33e-3925a53bdf70','2025-01-11 05:45:16','2025-01-11 13:45:16'),('Binary Heap','91f55617-3e4a-4935-9d8c-cc7332c9a4ba','4fc879aa-ea1e-4974-8929-045949d2d852','2025-01-11 05:25:36','2025-01-11 13:25:36'),('ORDER BY','cf5dddf0-5e2e-4211-b0bf-f48ef0f7268c','515e04c3-6e70-445f-9231-19d1ca2a3c14','2025-01-11 05:36:28','2025-01-11 13:36:28'),('They convert a non-linear model to linear','fbeaf118-19a7-402f-b21b-07319c40d904','55751a5c-d8fe-4e6e-8793-4d0247ba4e67','2025-01-11 05:05:36','2025-01-11 13:05:36'),('tidyverse','67e21b13-db92-4030-929e-d7f5628fc017','56edca87-038a-4d32-9584-790f3ff266a4','2025-01-11 05:32:08','2025-01-11 13:32:08'),('LIMIT','cf5dddf0-5e2e-4211-b0bf-f48ef0f7268c','576d750f-f8a5-4bff-bc12-404ab72aada4','2025-01-11 05:36:28','2025-01-11 13:36:28'),('Agglomerative Clustering','53dad63d-aafc-4ef6-b000-7cf09c9f8287','58836201-bc9c-4cd2-be1f-33296ad351ba','2025-01-11 07:10:02','2025-01-11 15:10:02'),('Precision','b80e2450-b531-4112-bdea-83054a3ce989','5bc23dc9-808c-4eb8-b0cf-a748c37e881f','2025-01-11 07:22:51','2025-01-11 15:22:51'),('Vectors that are perpendicular to each other','8ae03fa3-c7c7-4887-b3e0-abda52780622','5c6b48af-7660-4175-b19b-bd26cb96139a','2025-01-10 19:00:58','2025-01-11 03:00:58'),('import()','d25242e9-dbde-4d65-bed7-256d4dba7caf','632ce234-f6dd-4f09-b914-a2812bba6337','2025-01-11 05:31:04','2025-01-11 13:31:04'),('Random Forest','7c870830-47dd-427b-87c9-9474cb9f91d0','641c24a5-14bf-4ab7-a553-2161eef6c134','2025-01-11 07:36:11','2025-01-11 15:36:11'),('To provide activation functions to the network','58fb606f-bbb0-499e-b632-50bdc927d6e8','658fe6fa-e8fc-4b8f-a4ea-caaf2da37ad5','2025-01-11 07:39:03','2025-01-11 15:39:03'),('df.fillna() and df.drop_duplicates()','cce41205-ba4d-4d1e-9cc4-65ba6b74d19d','66534c5c-2734-41a9-a66b-a9b707484a24','2025-01-11 05:42:01','2025-01-11 13:42:01'),('Performing matrix factorization for dimensionality reduction','c79f5023-0180-4646-9ecf-35e4819dcba6','66ee6497-59cc-4db9-82f7-324fdfb19d0f','2025-01-10 18:56:57','2025-01-11 02:56:57'),('XGBoost','7c870830-47dd-427b-87c9-9474cb9f91d0','676a693e-6d07-4b69-8991-7a076d57948b','2025-01-11 07:36:11','2025-01-11 15:36:11'),('ARIMA','1cb38fb0-7085-420b-86fe-0907091daa66','685e35ef-6477-4879-bad7-976b6e3f77bb','2025-01-11 07:41:54','2025-01-11 15:41:54'),('Q-table','de38e5e8-9bc3-4a7a-b054-db1bb58a71c4','6b3710d2-49ac-4a34-8e3b-f16edd2069b4','2025-01-11 07:46:55','2025-01-11 15:46:55'),('Logistic regression','026761c4-f068-4a87-92cb-bd69ac4a984f','6e604df8-155f-4e10-8f94-711ee2fd3ebb','2025-01-11 05:16:32','2025-01-11 13:16:32'),('spark.read.parquet()','7e226eb9-ca97-43d4-b14c-ba139d9f3c04','6ea006e1-3752-4227-8a11-dc3eaf6c8272','2025-01-11 07:56:51','2025-01-11 15:56:51'),(' Containerizing the app with Docker','30e7dc37-d6e6-4676-9a48-753b981ed9eb','6f46d3e2-96df-4855-b828-b93700e700b3','2025-01-11 07:54:34','2025-01-11 15:54:34'),('Vectors that are linearly dependent','8ae03fa3-c7c7-4887-b3e0-abda52780622','6fd4504f-9a0f-400a-8de0-2e601fe399dd','2025-01-10 19:00:58','2025-01-11 03:00:58'),('When algorithms are transparent and explainable','ce9b6bc5-8267-4e89-8955-e06dddc83bb3','71f8dd46-4453-4414-a5cd-e31070f9f650','2025-01-11 07:05:34','2025-01-11 15:05:34'),('data.load()','d25242e9-dbde-4d65-bed7-256d4dba7caf','72ee9fb6-7f17-4330-801d-dadbae123b7e','2025-01-11 05:31:04','2025-01-11 13:31:04'),('Linear Regression','00f34e62-b38c-4ad8-994d-e41aa1715c1a','75143536-90ed-4c5d-b64b-5ee4072ad453','2025-01-11 07:33:30','2025-01-11 15:33:30'),('Determining the decision boundaries of models','6fa5d6fb-b048-4bef-8745-28c71ffc9e7e','753471ca-f20b-4823-a5b6-37a8cd6e99ba','2025-01-10 18:57:52','2025-01-11 02:57:52'),('Word Cloud','752b1707-9577-462e-837d-c2044a97a932','7833959d-0924-4ab4-a063-a01e2380104d','2025-01-11 07:19:01','2025-01-11 15:19:01'),('To increase model overfitting','032b83b7-ff53-4a00-b2ff-486f3a692d78','7b461e81-176b-410f-b268-458429639cd1','2025-01-11 05:07:30','2025-01-11 13:07:30'),('spark.createDataFrame()','7e226eb9-ca97-43d4-b14c-ba139d9f3c04','7ce38751-329d-4df3-bb35-806026edec7e','2025-01-11 07:56:51','2025-01-11 15:56:51'),('Only using data from one demographic group to train the model','52cc279d-69b8-454e-81c0-7222ef392280','7ce3e0f8-0fc7-49f6-88f8-045e24571f27','2025-01-11 07:06:44','2025-01-11 15:06:44'),('spark.read.csv()','7e226eb9-ca97-43d4-b14c-ba139d9f3c04','7d18bbe7-2771-4ee5-b9a5-dfdaba4ce81d','2025-01-11 07:56:51','2025-01-11 15:56:51'),('F1 Score','efe5bf2a-42bb-4d85-9b5a-23cbe6d1595e','7d8cb74c-9db8-46d9-8fdb-fc38e33b6ca6','2025-01-11 07:16:19','2025-01-11 15:16:19'),('NumPy','066bc28a-fc4c-4a1f-9133-526c14e938c9','81e22c8f-b46f-4d03-ae1c-6763c3cd3084','2025-01-11 05:22:11','2025-01-11 13:22:11'),('Seaborn','066bc28a-fc4c-4a1f-9133-526c14e938c9','83ae60b6-2528-41b7-a74e-dfb554272aa4','2025-01-11 05:22:11','2025-01-11 13:22:11'),('DBSCAN','53dad63d-aafc-4ef6-b000-7cf09c9f8287','84d263e0-5b1f-482e-a56b-3962534d3817','2025-01-11 07:10:02','2025-01-11 15:10:02'),('The number of rows in the matrix','0e89070d-41b4-462b-ab0e-7613defb0c7e','85248732-a38e-4d01-bdcc-0d2ebbbc808a','2025-01-10 18:51:42','2025-01-11 02:51:42'),('It changes the result of matrix multiplication','590280a5-7a53-4d08-a3b8-26434b767581','86524a85-cb25-4a23-9dfc-5ecd5e13bfe3','2025-01-10 18:59:01','2025-01-11 02:59:01'),('Dashboard Filters','156ee616-9c3d-4773-b2be-be71a08f221f','8700aaf5-aba5-478a-bf09-00cdd133bb4e','2025-01-11 07:28:10','2025-01-11 15:28:10'),('df.join()','1e3efca0-48a3-439d-bc99-982a357c5f89','88853cc6-22e1-4311-9e13-b9217e0e7caf','2025-01-11 05:45:16','2025-01-11 13:45:16'),('Reducing the number of features in the dataset','c79f5023-0180-4646-9ecf-35e4819dcba6','88b76096-9216-41ea-ba9a-49ad80a37784','2025-01-10 18:56:57','2025-01-11 02:56:57'),('To minimize the training error of the model','5d17d44d-557a-484c-acbd-ddffef22ec97','8cc30de4-dda3-48be-96fd-bebca6995e65','2025-01-11 05:15:40','2025-01-11 13:15:40'),('TF-IDF','e9e63bf8-558c-4a40-b166-6ea52d6da75d','8ce5bac7-a372-4676-af15-876a7cbcdd24','2025-01-11 07:44:23','2025-01-11 15:44:23'),('Data Blending','156ee616-9c3d-4773-b2be-be71a08f221f','8e1dc700-cd5b-45e6-8af9-4fb5bb3bf5c1','2025-01-11 07:28:10','2025-01-11 15:28:10'),('Ensuring data privacy and protection','ecdc73ec-8484-4037-af4a-cb73b7d21b56','8ea4fca8-d207-4697-8800-3788968a965b','2025-01-11 07:04:47','2025-01-11 15:04:47'),('KMeans','e9e63bf8-558c-4a40-b166-6ea52d6da75d','8f421817-b5a6-4748-9834-7ecf846ea67e','2025-01-11 07:44:23','2025-01-11 15:44:23'),('To filter data based on specific conditions.','05cb4c04-4581-4aa5-baed-4f7ebe11b31d','8fe1771e-105d-4c7e-8e6e-e654efcfeb24','2025-01-11 05:44:12','2025-01-11 13:44:12'),('They help in minimizing the training dataset size','fbeaf118-19a7-402f-b21b-07319c40d904','8feafd60-e6a1-490a-831b-1b6f7b6ff3db','2025-01-11 05:05:36','2025-01-11 13:05:36'),('To remove duplicates from a DataFrame.','05cb4c04-4581-4aa5-baed-4f7ebe11b31d','97f7ef99-5d47-4ea2-92de-8f60e71ef254','2025-01-11 05:44:12','2025-01-11 13:44:12'),('Amazon EC2','0c1aceb6-f774-472e-a237-c77631fb4521','9b519f6f-a729-4c03-aaa6-0087d85510a8','2025-01-11 07:51:58','2025-01-11 15:51:58'),('Amazon Lambda','0c1aceb6-f774-472e-a237-c77631fb4521','9bed25ba-a636-40d1-95e3-666113f4bc82','2025-01-11 07:51:58','2025-01-11 15:51:58'),('Euclidean Distance','e9e63bf8-558c-4a40-b166-6ea52d6da75d','9bfe25ea-f68a-4837-8219-f777ce4669fd','2025-01-11 07:44:23','2025-01-11 15:44:23'),('Element-wise addition','924daf1c-d96d-4f5b-b75b-15b8278eb17b','a1f906c9-45c8-4ced-aeac-4c36b6eb5f4c','2025-01-10 18:50:41','2025-01-11 02:50:41'),('The slope of a function','dcd98717-8f23-4303-ac04-80ccc5658f58','a23fb5cb-98d8-4e8f-a2b5-e7b9ae7eb59d','2025-01-11 05:06:36','2025-01-11 13:06:36'),('Accuracy','b80e2450-b531-4112-bdea-83054a3ce989','a2b7b549-5952-48bf-9dc2-36bb59031a9a','2025-01-11 07:22:51','2025-01-11 15:22:51'),('The mean of the data','dcd98717-8f23-4303-ac04-80ccc5658f58','a2d97d98-ca88-4c50-bb64-baeebbf0272e','2025-01-11 05:06:36','2025-01-11 13:06:36'),('DataFrame','ebe549e5-4600-4c7b-8c23-27dfb205a5d9','a6beac74-78e4-40a3-a6f8-5654e66a0434','2025-01-11 05:49:53','2025-01-11 13:49:53'),('A function','f145621b-08ed-4d89-8e06-c8ee99885b3e','a7480645-5fa6-49c1-bbe7-f3c53287ad60','2025-01-10 18:47:55','2025-01-11 02:47:55'),('To minimize the model\'s complexity','ea96ea25-d825-48a1-b823-1213a002f1e2','a8dab7ec-12c3-4b59-86d7-17acecf68913','2025-01-11 05:04:43','2025-01-11 13:04:43'),('Maximizing profits from data collection','ecdc73ec-8484-4037-af4a-cb73b7d21b56','aa3b218e-4eb7-4b5c-bda8-7d852f68d66b','2025-01-11 07:04:47','2025-01-11 15:04:47'),('Scalar multiplication','924daf1c-d96d-4f5b-b75b-15b8278eb17b','ab3e5882-84ae-4ac3-8d88-a07324957bfd','2025-01-10 18:50:41','2025-01-11 02:50:41'),('Random Forest','1cb38fb0-7085-420b-86fe-0907091daa66','aba7deae-3ee4-42f1-b9f5-cbf6cb78b033','2025-01-11 07:41:54','2025-01-11 15:41:54'),('To minimize the loss function by updating model parameters','ea96ea25-d825-48a1-b823-1213a002f1e2','ac5848c7-159d-4fa5-8547-2399d003ea14','2025-01-11 05:04:43','2025-01-11 13:04:43'),('The number of rows and columns','0e89070d-41b4-462b-ab0e-7613defb0c7e','b00dff34-fa3c-4c78-97e9-88bf66509b4d','2025-01-10 18:51:42','2025-01-11 02:51:42'),('By calculating the optimal weights for each feature','dd2e2057-9fe9-444f-87cf-2255a84f786a','b3dc1603-6eed-4f4a-8fb9-7de47ac2b514','2025-01-10 18:59:53','2025-01-11 02:59:53'),('By minimizing the cost function of the model','dd2e2057-9fe9-444f-87cf-2255a84f786a','b4c5f771-1ebd-4987-8e4b-6d83c9af381b','2025-01-10 18:59:53','2025-01-11 02:59:53'),('The result will be a matrix of size n x m','ba8f4b3c-33af-4998-8f22-8a528527b689','b5cff1b0-b2c9-43f4-b1f5-32958a2e56a7','2025-01-10 18:53:50','2025-01-11 02:53:50'),('Identifying outliers in the dataset','c79f5023-0180-4646-9ecf-35e4819dcba6','b69d64f9-959a-4cde-be5d-55dc907bdbf0','2025-01-10 18:56:57','2025-01-11 02:56:57'),('They measure how much the loss function changes with respect to each feature','fbeaf118-19a7-402f-b21b-07319c40d904','b7416fd8-ff9d-4e19-99cc-4441956289a4','2025-01-11 05:05:36','2025-01-11 13:05:36'),('Action list','de38e5e8-9bc3-4a7a-b054-db1bb58a71c4','b745cde1-3785-4409-bdf4-f7e1411fa499','2025-01-11 07:46:55','2025-01-11 15:46:55'),('Element-wise multiplication','924daf1c-d96d-4f5b-b75b-15b8278eb17b','b7c787ce-263f-41c4-9eac-5b012f6e5439','2025-01-10 18:50:41','2025-01-11 02:50:41'),('Amazon S3','0c1aceb6-f774-472e-a237-c77631fb4521','b8d4d93d-203d-4d94-b1d9-e3061761c737','2025-01-11 07:51:58','2025-01-11 15:51:58'),('The multiplication is not possible for these dimensions','ba8f4b3c-33af-4998-8f22-8a528527b689','b9b0b1df-2a7d-4165-aeb1-b045e86a9476','2025-01-10 18:53:50','2025-01-11 02:53:50'),('Hive','ebe549e5-4600-4c7b-8c23-27dfb205a5d9','baf0685d-8128-4655-a16c-76feaff00138','2025-01-11 05:49:53','2025-01-11 13:49:53'),('MapReduce','ebe549e5-4600-4c7b-8c23-27dfb205a5d9','bb7860af-26d6-4c6c-a883-7fb698bd4326','2025-01-11 05:49:53','2025-01-11 13:49:53'),('They help determine the optimal learning rate','a86af94e-a87e-41a2-9b2f-0e4a49bcc8e2','bd41b27b-cc7c-4b80-b980-451802125ba2','2025-01-10 18:52:50','2025-01-11 02:52:50'),('To normalize the input data','58fb606f-bbb0-499e-b632-50bdc927d6e8','bebda742-a5f7-493c-8e8b-57040de50e9a','2025-01-11 07:39:03','2025-01-11 15:39:03'),('Writing a function to collect user input','30e7dc37-d6e6-4676-9a48-753b981ed9eb','bf47e9b7-0f04-443d-94d4-0af7fd404f13','2025-01-11 07:54:34','2025-01-11 15:54:34'),('To merge multiple DataFrames.','05cb4c04-4581-4aa5-baed-4f7ebe11b31d','bf634f9b-2a38-4a94-b0cc-0c6d49eb06d0','2025-01-11 05:44:12','2025-01-11 13:44:12'),('Log Loss','efe5bf2a-42bb-4d85-9b5a-23cbe6d1595e','c0e885d9-b760-4fb7-991b-6cd96ef463f7','2025-01-11 07:16:19','2025-01-11 15:16:19'),('Stack','91f55617-3e4a-4935-9d8c-cc7332c9a4ba','c12b5028-45e0-484d-aa57-e4756189a611','2025-01-11 05:25:36','2025-01-11 13:25:36'),('They indicate the accuracy of the model','a86af94e-a87e-41a2-9b2f-0e4a49bcc8e2','c183387f-39f8-43b6-b017-6b687ec928a0','2025-01-10 18:52:50','2025-01-11 02:52:50'),('A two-dimensional matrix','f145621b-08ed-4d89-8e06-c8ee99885b3e','c47d9b69-42ce-419a-988f-ea1d9ae9cb13','2025-01-10 18:47:55','2025-01-11 02:47:55'),('read.csv()','d25242e9-dbde-4d65-bed7-256d4dba7caf','c4933d9a-d501-4f62-a753-563e0b98f344','2025-01-11 05:31:04','2025-01-11 13:31:04'),('R-Squared','b80e2450-b531-4112-bdea-83054a3ce989','c5ea8a12-6cbf-4bb8-a51c-cf54361c3bd3','2025-01-11 07:22:51','2025-01-11 15:22:51'),('df.fillna()','cce41205-ba4d-4d1e-9cc4-65ba6b74d19d','c8ea1ca8-f45e-42b9-908f-fbbe0fb7efd3','2025-01-11 05:42:01','2025-01-11 13:42:01'),('Ridge Regression','00f34e62-b38c-4ad8-994d-e41aa1715c1a','c90491db-34e1-49c0-bd4d-a5727196ed74','2025-01-11 07:33:30','2025-01-11 15:33:30'),('Computing the matrix inverse','c79f5023-0180-4646-9ecf-35e4819dcba6','c9f16818-a911-47f0-a116-06d408468b56','2025-01-10 18:56:57','2025-01-11 02:56:57'),('Parameter Controls','156ee616-9c3d-4773-b2be-be71a08f221f','ca052e85-a89f-4148-826d-33a32496689b','2025-01-11 07:28:10','2025-01-11 15:28:10'),('plt.hist()','dcb6084e-f432-44d6-932f-c31e9cb50ff3','cb472ac3-adb6-4be0-a4e4-54615d32ca28','2025-01-11 05:55:21','2025-01-11 13:55:21'),('Pie Chart','752b1707-9577-462e-837d-c2044a97a932','cbcdb116-6632-4c5b-8383-834365f21a60','2025-01-11 07:19:01','2025-01-11 15:19:01'),('K-Means','53dad63d-aafc-4ef6-b000-7cf09c9f8287','cd472952-e4dc-4488-84db-fedc0a343b6e','2025-01-11 07:10:02','2025-01-11 15:10:02'),('To improve the model\'s interpretability','032b83b7-ff53-4a00-b2ff-486f3a692d78','ce9907d9-461a-461d-9b23-a19a07f4bf4d','2025-01-11 05:07:30','2025-01-11 13:07:30'),('A scalar value','f145621b-08ed-4d89-8e06-c8ee99885b3e','cf8ec4a3-172d-4beb-97f2-c6496cbfbfc3','2025-01-10 18:47:55','2025-01-11 02:47:55'),('df.drop_duplicates()','cce41205-ba4d-4d1e-9cc4-65ba6b74d19d','cf94bc41-fdd8-4d9c-ac8d-8f44152a1605','2025-01-11 05:42:01','2025-01-11 13:42:01'),('plt.bar()','dcb6084e-f432-44d6-932f-c31e9cb50ff3','d099f9c8-6653-4b50-9375-8259356df461','2025-01-11 05:55:21','2025-01-11 13:55:21'),('Histogram','752b1707-9577-462e-837d-c2044a97a932','d21277d6-a0a0-4717-9da2-3ce2f5bc7b82','2025-01-11 07:19:01','2025-01-11 15:19:01'),('ggplot2','67e21b13-db92-4030-929e-d7f5628fc017','d3bc7a58-c40d-41af-8647-8990dcb6c875','2025-01-11 05:32:08','2025-01-11 13:32:08'),('PCA','e9e63bf8-558c-4a40-b166-6ea52d6da75d','d9257f7d-8d65-4ffb-b74d-52e18c7c6132','2025-01-11 07:44:23','2025-01-11 15:44:23'),('Action Filters','156ee616-9c3d-4773-b2be-be71a08f221f','da380312-de49-4b4f-ad93-83bda6ba213e','2025-01-11 07:28:10','2025-01-11 15:28:10'),('The first derivative of a function','dcd98717-8f23-4303-ac04-80ccc5658f58','da9a73b3-59c5-4b1b-8774-c9be91fa7209','2025-01-11 05:06:36','2025-01-11 13:06:36'),('K-means clustering','026761c4-f068-4a87-92cb-bd69ac4a984f','dc7f7e3f-3a90-4309-9bb8-e21766237def','2025-01-11 05:16:32','2025-01-11 13:16:32'),('load.data()','d25242e9-dbde-4d65-bed7-256d4dba7caf','ddd9ff0d-0800-42ea-bc63-59cc732721b2','2025-01-11 05:31:04','2025-01-11 13:31:04'),('Generating random samples for training data','6fa5d6fb-b048-4bef-8745-28c71ffc9e7e','df450c77-5049-4803-b70f-bd10b550fd37','2025-01-10 18:57:52','2025-01-11 02:57:52'),(' It is used in the computation of the determinant','590280a5-7a53-4d08-a3b8-26434b767581','df818c6a-2ea7-46b8-8bd2-e4877854dd20','2025-01-10 18:59:01','2025-01-11 02:59:01'),('Accuracy','efe5bf2a-42bb-4d85-9b5a-23cbe6d1595e','e03c0b10-9a74-44ca-b761-1983cde6eac8','2025-01-11 07:16:19','2025-01-11 15:16:19'),('It is used to calculate eigenvalues','590280a5-7a53-4d08-a3b8-26434b767581','e09b3833-46ec-44b0-9f09-5f1e6a96c2f0','2025-01-10 18:59:01','2025-01-11 02:59:01'),('Mean Squared Error','b80e2450-b531-4112-bdea-83054a3ce989','e0b0a548-af2a-4361-97cf-6207ef0db37e','2025-01-11 07:22:51','2025-01-11 15:22:51'),('Increasing model accuracy at all costs','ecdc73ec-8484-4037-af4a-cb73b7d21b56','e17bfeba-25ec-44ca-aac7-6792ce6928af','2025-01-11 07:04:47','2025-01-11 15:04:47'),('dplyr','67e21b13-db92-4030-929e-d7f5628fc017','e21f0128-a4ba-4150-a9a2-78272888ce53','2025-01-11 05:32:08','2025-01-11 13:32:08'),(' By improving the training speed of the model','dd2e2057-9fe9-444f-87cf-2255a84f786a','e3f8ca04-1023-49fd-a6f7-c10757f9071c','2025-01-10 18:59:53','2025-01-11 02:59:53'),('Exponential Smoothing','1cb38fb0-7085-420b-86fe-0907091daa66','e5b04158-af0b-4aac-b19a-938163ef3269','2025-01-11 07:41:54','2025-01-11 15:41:54'),('Ignoring missing data and training the model anyway','52cc279d-69b8-454e-81c0-7222ef392280','e5ef0f89-1776-47be-a9e0-41b660203c7a','2025-01-11 07:06:44','2025-01-11 15:06:44'),('They define the type of machine learning algorithm','a86af94e-a87e-41a2-9b2f-0e4a49bcc8e2','e8e4f7c6-5130-4a57-9d73-a21f827f1f3a','2025-01-10 18:52:50','2025-01-11 02:52:50'),('Linear Regression','1cb38fb0-7085-420b-86fe-0907091daa66','eb6c18c0-7def-479f-a2bd-e89c3723aa75','2025-01-11 07:41:53','2025-01-11 15:41:53'),('To determine the model\'s accuracy','5d17d44d-557a-484c-acbd-ddffef22ec97','ec3da457-39c7-44ca-a2bc-4023886b41bf','2025-01-11 05:15:40','2025-01-11 13:15:40'),('df.concat()','1e3efca0-48a3-439d-bc99-982a357c5f89','ede87421-2f5d-4bad-a339-914dd18d7397','2025-01-11 05:45:16','2025-01-11 13:45:16'),('SparkContext','ebe549e5-4600-4c7b-8c23-27dfb205a5d9','f1948c54-b5a7-44fc-bf27-e0161e35268a','2025-01-11 05:49:53','2025-01-11 13:49:53'),('To compute the cumulative distribution of errors','ea96ea25-d825-48a1-b823-1213a002f1e2','f331a1ca-54b3-4b3f-b65c-381fe8bc7489','2025-01-11 05:04:43','2025-01-11 13:04:43'),(' When an algorithm is optimized to increase fairness','ce9b6bc5-8267-4e89-8955-e06dddc83bb3','f3456f31-4145-4f18-bd48-dcc86cdc772c','2025-01-11 07:05:34','2025-01-11 15:05:34'),('Linked List','91f55617-3e4a-4935-9d8c-cc7332c9a4ba','f43d3549-ca92-4716-b76d-3184f06fea74','2025-01-11 05:25:36','2025-01-11 13:25:36'),('Logistic Regression','00f34e62-b38c-4ad8-994d-e41aa1715c1a','f4970048-44bc-4c30-94e3-cef8b5022702','2025-01-11 07:33:30','2025-01-11 15:33:30'),('HAVING','cf5dddf0-5e2e-4211-b0bf-f48ef0f7268c','f5e8fac6-b0d3-4e94-9fc3-0feed5278d9d','2025-01-11 05:36:28','2025-01-11 13:36:28'),('To split data into groups based on a specific column and then perform an aggregation on each group.','05cb4c04-4581-4aa5-baed-4f7ebe11b31d','f65c51f0-a90b-4488-b15d-1b230955f006','2025-01-11 05:44:12','2025-01-11 13:44:12'),('plt.scatter()','dcb6084e-f432-44d6-932f-c31e9cb50ff3','f6f976b0-d864-44b9-bfc6-621b7d59f14f','2025-01-11 05:55:21','2025-01-11 13:55:21'),('The result will be a matrix of size m x n','ba8f4b3c-33af-4998-8f22-8a528527b689','f9acfc53-70dc-4f8b-af29-fc610e798dcd','2025-01-10 18:53:50','2025-01-11 02:53:50'),('To extract features such as edges, textures, and patterns from the input images','58fb606f-bbb0-499e-b632-50bdc927d6e8','fbaae908-ee94-4478-b83f-91d4d98d289b','2025-01-11 07:39:03','2025-01-11 15:39:03'),('To reduce the dimensions of the data','58fb606f-bbb0-499e-b632-50bdc927d6e8','fd83061e-9577-49d6-9e92-20eaade279f6','2025-01-11 07:39:03','2025-01-11 15:39:03'),('data.table','67e21b13-db92-4030-929e-d7f5628fc017','fdc339f4-e800-4edc-89bc-5e586cc29037','2025-01-11 05:32:08','2025-01-11 13:32:08');
/*!40000 ALTER TABLE `option` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `program`
--

DROP TABLE IF EXISTS `program`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `program` (
  `name` varchar(45) NOT NULL,
  `rating` int NOT NULL,
  `program_picture` varchar(65) NOT NULL,
  `description` text NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `name` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `program`
--

LOCK TABLES `program` WRITE;
/*!40000 ALTER TABLE `program` DISABLE KEYS */;
INSERT INTO `program` VALUES ('Game Development',4,'010.png','Create immersive, high-quality video games by mastering industry-standard game engines like Unity and Unreal Engine. This course will guide you through the process of game development from start to finish, covering key areas such as advanced graphics programming, physics simulations, artificial intelligence, and character design. You\'ll learn how to create stunning visuals, develop interactive environments, and write complex game logic. Additionally, you\'ll explore the art of interactive storytelling, including narrative-driven game design, branching storylines, and creating emotional player experiences. By the end of the course, you\'ll be equipped with the skills to build dynamic, engaging video games that captivate players and deliver unforgettable gaming experiences.','02c94fb2-ca4f-40c4-b1a9-7cae35f76aa1','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Music Production',4,'029.png','Compose, mix, and produce music using industry-standard software, exploring sound design and audio engineering.','1055a874-1f6f-4657-826a-e3b7458d63ce','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Cloud Computing',4,'005.png','Master the concepts of cloud infrastructure and services like AWS, Azure, and Google Cloud, enabling scalable and cost-effective solutions.','1c573a27-a91a-4e63-9dba-53eec84be5ae','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Data Science',5,'001.png','This course offers a comprehensive exploration of data science, covering essential topics such as machine learning, statistical analysis, and data visualization. Students will gain practical experience through hands-on projects, learning how to work with real-world datasets, apply various algorithms, and interpret results. The course equips learners with the skills to tackle complex data challenges and make informed, data-driven decisions in any industry.','1c8b8375-f5f1-4539-82c2-75395f261981','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Business Administration',5,'012.png','Understand machine learning algorithms and apply them to real-world problems using tools like TensorFlow and Scikit-learn.','2a73ec7a-64fa-4e1a-a8b6-ef5a546df75f','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Big Data Analytics',5,'009.png','Learn to manage and analyze large datasets using Hadoop, Spark, and other big data tools to drive business insights.','3174b224-06d7-4e5f-9a4b-196ad0d619db','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Digital Marketing',4,'006.png','Gain expertise in SEO, content marketing, email campaigns, and social media strategy to effectively promote brands online.','38fe8f4a-8411-46d9-b94f-f2c357d4fe29','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Mobile App Development',5,'007.png','Develop mobile applications for Android and iOS platforms, focusing on user experience and efficient coding practices.','4037cb81-fd7a-472b-bb08-00071454c5b6','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Full Stack Development',4,'002.png','Learn to build complete web applications from scratch, mastering both frontend technologies like HTML, CSS, JavaScript, and backend frameworks.','42595fa8-26d3-475b-abe9-9dbe719def4f','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Data Visualization',4,'014.png','Learn to transform raw data into insightful visualizations using tools like Tableau, Power BI, and Matplotlib.','56cd9378-81bb-4452-b6ec-2f7117eadfe0','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Blockchain Technology',5,'008.png','Dive into the decentralized world of blockchain, learning about smart contracts, cryptocurrencies, and distributed ledger technologies.','56f052a0-f96d-4847-8123-c486f8edad5b','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Photography and Editing',4,'020.png','Capture stunning photographs and enhance them with editing software, learning composition, lighting, and storytelling techniques.','5ad3e8f7-577d-484e-a645-7569bed2cb22','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Renewable Energy Systems',5,'027.png','Understand renewable energy technologies and design sustainable solutions to reduce environmental impact.','5f7d8f8a-f30c-4604-ac28-ee48d2b72528','2025-01-06 08:39:11','2025-01-06 16:39:11'),('IoT Development',5,'022.png','Develop IoT solutions by integrating sensors, microcontrollers, and cloud platforms to create smart systems.','68736bff-f66b-442a-836e-bf6af767f86e','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Cybersecurity',5,'003.png','This course provides an in-depth understanding of the key principles involved in safeguarding networks, systems, and data from a variety of cyber threats. Students will explore the fundamentals of cybersecurity, including threat analysis, risk management, and defensive techniques. Real-world security scenarios will be used to illustrate how to recognize vulnerabilities, implement robust protection strategies, and respond effectively to incidents. By the end of the course, learners will be equipped with the essential skills needed to build and maintain secure infrastructures in today\'s rapidly evolving digital landscape.','80392782-dfd2-4787-a9a3-567364ea0cbe','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Business Analytics',4,'015.png','Bridge the gap between business and data analysis, leveraging tools like Excel, SQL, and predictive modeling.','83d70056-b5b6-492e-ae8b-f4fc1d52f477','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Financial Modeling',4,'026.png','Master the art of financial analysis and forecasting by building complex financial models using Excel and other tools.','849d4127-5978-4191-b329-4d4f9b16081f','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Video Editing and Production',4,'016.png','Learn the essentials of video editing and production, including storyboarding, post-processing, and special effects.','8557d347-6515-4cd0-9366-ff6b92c0c93d','2025-01-06 08:39:11','2025-01-06 16:39:11'),('E-commerce Development',4,'019.png','Design and develop scalable e-commerce platforms, implementing payment gateways and inventory management systems.','86746e33-5710-4898-a516-8a6b6870bc6a','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Content Writing',4,'025.png','Learn to create compelling written content, mastering SEO techniques, storytelling, and editing for diverse platforms.','9e6b3b81-c123-49ba-9677-c4efd8fe02f6','2025-01-06 08:39:11','2025-01-06 16:39:11'),('3D Printing and Design',4,'028.png','Learn the principles of 3D modeling and printing, creating prototypes and functional objects for various applications.','a3ae724e-9cc1-4325-bb92-4d442b11de69','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Artificial Intelligence',5,'004.png','Explore the foundations of AI, including neural networks, natural language processing, and deep learning, to build intelligent systems.','a8e2a03a-a6c4-4cfd-b52d-63d8c72455e7','2025-01-06 08:39:11','2025-01-06 16:39:11'),('UI/UX Design',4,'011.png','Master the art of designing user-friendly interfaces and experiences, emphasizing prototyping, wireframing, and usability testing.','ad9aeda1-5df9-4485-983c-02046ff5610e','2025-01-06 08:39:11','2025-01-06 16:39:11'),('AR/VR Development',5,'024.png','Design immersive augmented and virtual reality experiences using tools like Unity, Oculus, and ARKit.','b28e1e19-a3ef-47d1-b737-a361a54692bc','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Natural Language Processing',5,'023.png','Analyze and process human language using NLP techniques, building chatbots, and sentiment analysis tools.','c0c56560-5927-4d40-9cd0-cb95e412a061','2025-01-06 08:39:11','2025-01-06 16:39:11'),('DevOps Engineering',5,'013.png','Streamline software development processes with CI/CD pipelines, containerization, and orchestration using Docker and Kubernetes.','c4954a62-2a03-4c2f-8547-df3a5832c093','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Ethical Hacking',5,'017.png','Learn to identify vulnerabilities and secure systems by simulating attacks, while adhering to ethical principles.','d3b04ca0-b7e0-4599-b86c-85f993d145ea','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Robotics',5,'018.png','Build and program robots, focusing on sensors, actuators, and AI integration for autonomous behavior.','d53086ed-b0e8-4515-b9f0-6d5eebf0cc40','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Entrepreneurship',5,'030.png','Develop the skills to start and grow a business, focusing on innovation, marketing, and financial planning.','ea084567-f492-4888-a467-7ad4eeea0ae7','2025-01-06 08:39:11','2025-01-06 16:39:11'),('Digital Art and Animation',4,'021.png','Create captivating digital art and animations using tools like Photoshop, Illustrator, and After Effects.','fa00517b-1042-4b9c-b4ff-5bcac2c160bc','2025-01-06 08:39:11','2025-01-06 16:39:11');
/*!40000 ALTER TABLE `program` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `project`
--

DROP TABLE IF EXISTS `project`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `project` (
  `title` varchar(45) NOT NULL,
  `task_name` varchar(60) NOT NULL,
  `description` text NOT NULL,
  `course_id` varchar(60) NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `course_id` (`course_id`),
  CONSTRAINT `project_ibfk_1` FOREIGN KEY (`course_id`) REFERENCES `course` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `project`
--

LOCK TABLES `project` WRITE;
/*!40000 ALTER TABLE `project` DISABLE KEYS */;
INSERT INTO `project` VALUES ('ustomer Churn Prediction with Advanced ML','  ','## **Project: Predictive Model for Customer Churn Using Advanced Machine Learning Techniques**\r\n\r\n### **Overview**\r\nIn this project, you will apply advanced machine learning techniques to predict customer churn for a telecommunications company. The goal is to build a model that can predict whether a customer is likely to churn based on their historical behavior and usage data. You will use techniques like ensemble methods, deep learning, and hyperparameter optimization to improve model performance.\r\n\r\n### **Objective**\r\n- Build a predictive model using a combination of advanced machine learning techniques.\r\n- Apply ensemble methods (e.g., Random Forest, XGBoost) to improve predictive accuracy.\r\n- Explore deep learning methods such as neural networks to model complex relationships in data.\r\n- Use hyperparameter optimization techniques (e.g., GridSearchCV, RandomizedSearchCV) to fine-tune model performance.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection**\r\n- Use a dataset such as the **Telco Customer Churn Dataset** from Kaggle or a similar telecommunications dataset containing customer information.\r\n  - The dataset should include customer demographics, account information, usage patterns, and the target variable (whether the customer churned or not).\r\n\r\n#### 2. **Data Preprocessing**\r\n- Clean the data by handling missing values, duplicates, and erroneous entries.\r\n  - Perform data imputation for missing values (e.g., using mean, median, or mode imputation).\r\n  - Remove or handle outliers.\r\n- Perform feature engineering to create new features that might be useful for model prediction (e.g., tenure-based features or total usage).\r\n- Encode categorical variables (e.g., one-hot encoding for gender, contract type).\r\n- Scale numerical features if necessary to ensure that they are on a similar scale for machine learning models.\r\n\r\n#### 3. **Exploratory Data Analysis (EDA)**\r\n- Conduct an EDA to understand the distribution of key variables and how they relate to customer churn.\r\n  - Visualize the target variable distribution and its relationship with predictors.\r\n  - Use correlation heatmaps to identify relationships between features.\r\n  - Perform hypothesis testing to identify statistically significant variables.\r\n\r\n#### 4. **Model Building**\r\n- Implement and train different machine learning models to predict customer churn:\r\n  - **Logistic Regression**: As a baseline model.\r\n  - **Random Forest**: An ensemble method that aggregates multiple decision trees to improve prediction accuracy.\r\n  - **XGBoost**: An advanced gradient boosting algorithm known for its performance in predictive tasks.\r\n  - **Neural Networks**: Build a deep learning model using Keras or TensorFlow for churn prediction.\r\n- Train the models on the dataset and tune their hyperparameters using techniques such as **GridSearchCV** or **RandomizedSearchCV**.\r\n\r\n#### 5. **Model Evaluation**\r\n- Evaluate the performance of the models using various metrics such as:\r\n  - **Accuracy**\r\n  - **Precision**\r\n  - **Recall**\r\n  - **F1-Score**\r\n  - **AUC-ROC Curve** (Area Under the Curve)\r\n- Compare model performance and choose the best model based on evaluation metrics.\r\n- Use cross-validation to ensure robustness and avoid overfitting.\r\n\r\n#### 6. **Model Interpretation**\r\n- Interpret the results from the machine learning models:\r\n  - For tree-based models (e.g., Random Forest, XGBoost), analyze feature importance to understand which features contribute most to the prediction.\r\n  - For deep learning models, visualize the models behavior with techniques such as **SHAP (Shapley Additive Explanations)** or **LIME (Local Interpretable Model-agnostic Explanations)**.\r\n  - Discuss the interpretability of the models and what the insights imply about customer behavior.\r\n\r\n#### 7. **Model Deployment (Optional)**\r\n- If applicable, deploy the trained model using a framework like **Flask** or **FastAPI** to create a REST API that can accept input data and return churn predictions in real-time.\r\n- You can also use cloud services like **AWS SageMaker** or **Google AI Platform** for model deployment and scaling.\r\n\r\n#### 8. **Conclusion and Insights**\r\n- Summarize the findings, including which features are most important in predicting customer churn.\r\n- Discuss the strengths and limitations of the models used, and suggest improvements for future work (e.g., using more advanced techniques like **AutoML** or **reinforcement learning**).\r\n- Provide actionable insights that can help the telecommunications company reduce churn (e.g., target specific customer segments for retention efforts).\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks containing data preprocessing, model building, and evaluation.\r\n2. **Documentation**: A report detailing the project workflow, methodology, results, and insights.\r\n3. **Presentation**: A slide deck summarizing the project, model performance, and key findings.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Understanding and application of advanced machine learning techniques such as ensemble methods, gradient boosting, and deep learning.\r\n- Experience with hyperparameter optimization techniques to fine-tune model performance.\r\n- Skills in model evaluation and interpretation, including the use of advanced metrics and explainability methods.\r\n- Knowledge of deploying machine learning models for real-world applications.\r\n\r\n','020578cb-0b83-465e-959d-400cecfb415e','1f42afce-059d-4f1c-a3a1-edac600f7f3c','2025-01-11 07:35:22','2025-01-11 15:35:22'),('Movie Data Analysis with Python',' ','## **Project: Analyzing and Visualizing Movie Data**\r\n\r\n### **Overview**  \r\nIn this project, you will apply your Python skills to analyze and visualize a movie dataset, providing insights into various aspects of the film industry such as popularity, ratings, genres, and box office performance. You will use data analysis techniques, basic statistics, and visualizations to explore the dataset, clean the data, and generate insights.\r\n\r\n---\r\n\r\n### **Objectives**\r\n- Learn how to manipulate data using **Pandas** and **NumPy**.\r\n- Apply basic **Python programming** concepts for data processing.\r\n- Perform **data cleaning** and handling missing values.\r\n- Use **Matplotlib** and **Seaborn** for data visualization.\r\n- Develop basic statistical insights using Python.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection**\r\n- Find a publicly available movie dataset (e.g., from Kaggle: \"TMDb Movie Dataset\" or \"IMDb Dataset\").\r\n- The dataset should include columns like **movie title**, **release year**, **genres**, **budget**, **revenue**, **ratings**, and **runtime**.\r\n\r\n#### 2. **Data Cleaning**\r\n- Inspect the dataset for missing or inconsistent data.\r\n- Handle missing values using appropriate strategies (e.g., filling missing values, dropping rows).\r\n- Convert data types if necessary (e.g., converting budget and revenue columns to numerical types).\r\n\r\n#### 3. **Exploratory Data Analysis (EDA)**\r\n- Analyze the distribution of numerical features (e.g., **ratings**, **budget**, **revenue**).\r\n- Explore relationships between variables, such as how **budget** correlates with **revenue** or how **ratings** are distributed across **genres**.\r\n- Use basic statistical functions in **Pandas** to compute mean, median, and standard deviation of key features.\r\n\r\n#### 4. **Data Visualization**\r\n- Create visualizations to represent key data patterns and relationships:\r\n  - **Bar charts** for the most popular movie genres.\r\n  - **Histograms** for the distribution of **ratings** and **revenue**.\r\n  - **Scatter plots** to show the correlation between **budget** and **revenue**.\r\n  - **Line plots** showing the trend of movie releases over the years.\r\n\r\n#### 5. **Statistical Analysis**\r\n- Perform basic statistical tests:\r\n  - **Correlation analysis** to check how different features (e.g., budget, revenue, and ratings) are related.\r\n  - Apply simple **linear regression** to predict **revenue** based on **budget**.\r\n\r\n#### 6. **Insights and Conclusions**\r\n- Based on the analysis, write a summary of your findings.\r\n  - Which genres tend to perform the best at the box office?\r\n  - Is there a correlation between **budget** and **revenue**?\r\n  - What factors seem to affect the **ratings** most?\r\n\r\n#### 7. **Project Documentation**\r\n- Document your code and analysis clearly in a **Jupyter notebook** or **Python script**.\r\n- Include markdown cells explaining each step of your process, from data loading to visualization and analysis.\r\n\r\n---\r\n\r\n### **Expected Deliverables**\r\n1. A clean and preprocessed movie dataset ready for analysis.\r\n2. Jupyter notebooks or Python scripts with detailed explanations of the analysis.\r\n3. Visualizations like bar charts, histograms, and scatter plots.\r\n4. A report summarizing key insights and recommendations based on the analysis.\r\n\r\n---\r\n\r\n### **Tools and Libraries**\r\n- **Programming Language:** Python\r\n- **Libraries:** Pandas, NumPy, Matplotlib, Seaborn\r\n- **Development Environment:** Jupyter Notebook\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Learn to manipulate and clean data using **Pandas**.\r\n- Gain experience in performing **Exploratory Data Analysis (EDA)**.\r\n- Develop strong skills in data visualization using **Matplotlib** and **Seaborn**.\r\n- Learn to perform basic statistical analysis, including **correlation** and **regression**.\r\n- Understand the process of **documenting** and **presenting data insights** clearly.\r\n','f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','24fd10ac-282f-42d7-b8e0-b338430c1541','2025-01-11 05:20:59','2025-01-11 13:20:59'),('Housing Price Prediction Model',' ','## **Project: Predicting Housing Prices with Statistical Modeling**\r\n\r\n### **Overview**\r\nIn this project, you will build a statistical model to predict housing prices based on various features like the number of bedrooms, location, square footage, and more. Using statistical modeling techniques, you will explore the relationships between different variables and develop a model to make predictions about housing prices.\r\n\r\n### **Objective**\r\n- Apply statistical methods to model real-world data.\r\n- Learn about hypothesis testing, regression models, and model evaluation techniques.\r\n- Predict housing prices and interpret the statistical significance of various predictors.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection**\r\n- Use a publicly available dataset such as the **Boston Housing dataset** or **Kaggles Housing Prices dataset**.\r\n- The dataset should contain features such as square footage, number of bedrooms, neighborhood, age of the house, etc.\r\n\r\n#### 2. **Data Preprocessing**\r\n- Clean the data by handling missing values, duplicates, and erroneous entries.\r\n  - Fill in missing data using imputation methods (e.g., mean imputation or regression imputation).\r\n  - Remove or handle outliers in the dataset.\r\n- Standardize numerical features (e.g., scaling continuous variables).\r\n- Encode categorical variables if needed (e.g., one-hot encoding for neighborhood).\r\n\r\n#### 3. **Exploratory Data Analysis (EDA)**\r\n- Use statistical visualizations to explore relationships between the features and the target variable (housing prices).\r\n  - Visualize the distribution of key features (e.g., histograms, box plots).\r\n  - Check correlations between numerical features and the target variable.\r\n  - Conduct hypothesis testing to check the significance of relationships between variables and price.\r\n\r\n#### 4. **Statistical Modeling**\r\n- Apply linear regression to predict housing prices based on the features.\r\n  - Fit a simple linear regression model and evaluate its performance.\r\n  - Explore multiple linear regression by adding more features to the model.\r\n  - Use regularization techniques like **Ridge** or **Lasso** regression to improve the models generalization.\r\n- If applicable, explore other statistical models like **Poisson regression** or **Logistic regression** for classification tasks (e.g., predicting price categories).\r\n\r\n#### 5. **Model Evaluation**\r\n- Evaluate the performance of your model using common regression metrics such as:\r\n  - **R-Squared** (Goodness of Fit)\r\n  - **Mean Squared Error (MSE)**\r\n  - **Root Mean Squared Error (RMSE)**\r\n  - **Adjusted R-Squared**\r\n- Perform **cross-validation** to test the models robustness.\r\n\r\n#### 6. **Model Interpretation**\r\n- Interpret the results from your regression models:\r\n  - Analyze the coefficients to understand the impact of each predictor variable on housing prices.\r\n  - Check for multicollinearity using **VIF (Variance Inflation Factor)** to assess the independence of predictor variables.\r\n  - Identify significant predictors and discuss their real-world relevance.\r\n\r\n#### 7. **Advanced Statistical Methods (Optional)**\r\n- If applicable, explore advanced techniques such as **Principal Component Analysis (PCA)** for dimensionality reduction or **Time Series Analysis** for predicting price trends over time.\r\n- Use **Bayesian Statistics** to incorporate prior knowledge into the model.\r\n\r\n#### 8. **Conclusion and Insights**\r\n- Summarize the findings, including which features are most important for predicting housing prices.\r\n- Discuss the potential impact of external factors like location, economic conditions, or market trends.\r\n- Suggest improvements for future modeling, such as incorporating more data or using more sophisticated techniques.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks containing data cleaning, modeling, and evaluation steps.\r\n2. **Documentation**: A report explaining the methodology, results, and insights from the statistical models.\r\n3. **Presentation**: A slide deck summarizing the key findings and the final model\'s performance.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Understanding of fundamental statistical modeling techniques such as linear regression, hypothesis testing, and model evaluation.\r\n- Experience in data preprocessing, feature selection, and interpretation of regression results.\r\n- Knowledge of regularization techniques and advanced statistical methods to improve model performance.\r\n\r\n','160ee69a-9374-40d6-bd9e-9ec9e865da52','32f0a073-3fd8-46d6-916c-27b4482198c0','2025-01-11 07:32:33','2025-01-11 15:32:33'),('Predicting Customer Churn with Statistics',' ','## **Project: Analyzing and Predicting Customer Churn Using Probability and Statistics**\r\n\r\n### **Overview**\r\nCustomer churn is a critical metric for businesses, indicating the percentage of customers who stop using a product or service over time. By applying probability and statistical methods, this project will analyze customer behavior, identify key factors influencing churn, and build a predictive model using statistical techniques to forecast which customers are at risk of leaving. This project demonstrates how probability and statistics can be directly applied to solve real-world business problems.\r\n\r\n---\r\n\r\n### **Objectives**\r\n- Apply probability theory and statistical methods to analyze customer churn data.\r\n- Use hypothesis testing to identify factors influencing churn.\r\n- Implement regression and classification models to predict customer churn.\r\n- Evaluate model performance using appropriate statistical metrics.\r\n- Visualize data distributions, relationships, and model outcomes.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Problem Definition and Data Collection**\r\n- Define the problem of customer churn and explain why it\'s important for businesses.\r\n- Collect a relevant dataset, such as from Kaggle (e.g., \"Customer Churn Prediction\" datasets) or any other source with customer features like age, subscription plan, usage patterns, and customer service interactions.\r\n\r\n#### 2. **Exploratory Data Analysis (EDA)**\r\n- Conduct an EDA to understand the data, check for missing values, and identify any outliers.\r\n- Use probability distributions (e.g., normal distribution, binomial distribution) to understand customer behavior patterns.\r\n- Visualize data distributions with histograms, boxplots, and scatter plots.\r\n\r\n#### 3. **Hypothesis Testing**\r\n- Formulate hypotheses related to customer churn (e.g., \"Customers who have a higher number of customer service complaints are more likely to churn\").\r\n- Perform **t-tests**, **chi-squared tests**, or **ANOVA** to test these hypotheses and assess the statistical significance of different features.\r\n\r\n#### 4. **Feature Selection and Engineering**\r\n- Select features that may influence churn based on EDA and hypothesis testing.\r\n- Create new features if necessary, such as customer tenure, frequency of service usage, or engagement scores.\r\n- Apply statistical methods (e.g., **correlation analysis**) to identify relationships between features.\r\n\r\n#### 5. **Model Development**\r\n- Develop a **logistic regression** model to predict whether a customer will churn or not.\r\n- Use **decision trees** or **random forests** for classification tasks.\r\n- Incorporate **Naive Bayes** to assess the performance of probabilistic models.\r\n- Implement **k-fold cross-validation** to evaluate model stability and prevent overfitting.\r\n\r\n#### 6. **Model Evaluation**\r\n- Use performance metrics like **accuracy**, **precision**, **recall**, **F1-score**, and **AUC-ROC** to evaluate the predictive power of the model.\r\n- Analyze confusion matrices and the trade-offs between false positives and false negatives.\r\n\r\n#### 7. **Statistical Inference**\r\n- Analyze the significance of model coefficients (in case of logistic regression) or feature importance (in case of decision trees and random forests).\r\n- Apply **confidence intervals** and **p-values** to interpret the model results.\r\n\r\n#### 8. **Predictive Analysis and Optimization**\r\n- Use the model to predict the churn probability for individual customers.\r\n- Identify customer segments that are more likely to churn and recommend retention strategies.\r\n- Optimize the models prediction threshold for better business decision-making.\r\n\r\n#### 9. **Data Visualization and Reporting**\r\n- Create dashboards or detailed visualizations (e.g., **ROC curves**, **Precision-Recall curves**, feature importance plots).\r\n- Present findings in an easily understandable format, showing the key statistical insights and predictions.\r\n- Summarize the analysis, methods used, and final recommendations for reducing churn.\r\n\r\n---\r\n\r\n### **Expected Deliverables**\r\n1. A cleaned and preprocessed dataset ready for analysis.\r\n2. Jupyter notebooks with clear explanations of statistical methods and code.\r\n3. Visualizations such as probability distributions, hypothesis test results, model performance, and feature importance.\r\n4. A detailed report summarizing findings, insights, and strategies to reduce churn.\r\n5. A predictive model that forecasts customer churn probability.\r\n\r\n---\r\n\r\n### **Tools and Libraries**\r\n- **Programming Language:** Python or R\r\n- **Libraries:** Pandas, NumPy, Matplotlib, Seaborn, SciPy, Statsmodels, Scikit-learn\r\n- **Development Environment:** Jupyter Notebook or RStudio\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Application of probability distributions and statistical tests in a business context.\r\n- Hands-on experience in feature engineering, model selection, and evaluation.\r\n- Understanding of hypothesis testing and statistical inference.\r\n- Experience in implementing predictive models and interpreting results.\r\n- Practical knowledge of how statistics and probability help businesses make data-driven decisions.\r\n','83e631af-5b05-4a16-bd42-a85bdb3b542e','3711b318-66c3-4c53-afac-5015d7fe324b','2025-01-11 05:14:18','2025-01-11 13:14:18'),('QL Data Exploration',' ','# ## **Project: Analyzing and Querying a Public Dataset Using SQL**\r\n\r\n## ### **Overview**\r\nIn this project, you will practice SQL queries to analyze a real-world dataset and gain insights through various operations such as filtering, sorting, joining, grouping, and aggregating. The goal is to develop the ability to write complex SQL queries for data exploration and analysis, essential for working with databases in the field of data science.\r\n\r\n---\r\n\r\n## ### **Objectives**\r\n- Use SQL to query and manipulate data in a relational database.\r\n- Join multiple tables to retrieve related data.\r\n- Filter and sort data based on specific conditions.\r\n- Aggregate data to extract insights.\r\n- Write subqueries and advanced SQL statements for data analysis.\r\n\r\n---\r\n\r\n## ### **Tasks**\r\n\r\n### #### 1. **Dataset Selection**\r\n- Choose a public dataset with at least two related tables to practice SQL queries. Potential datasets include:\r\n  - **Northwind Database**: A sample database for practicing SQL queries with tables like `Orders`, `Customers`, `Products`, and `Employees`.\r\n  - **Sakila Database**: A film rental database with tables like `Films`, `Actors`, `Payments`, `Rentals`, and `Customers`.\r\n  - **Sales Data**: A dataset containing sales transactions, customers, products, and stores.\r\n\r\n### #### 2. **Data Exploration**\r\n- Import the dataset into a SQL database (MySQL, PostgreSQL, or SQLite).\r\n- Perform the following queries:\r\n  - **SELECT** all records from a specific table.\r\n  - Use **WHERE** clauses to filter data based on conditions (e.g., sales greater than $100).\r\n  - Use **ORDER BY** to sort data in ascending or descending order.\r\n\r\n### #### 3. **Joins and Relationships**\r\n- Explore relationships between tables and use different types of joins:\r\n  - **INNER JOIN**: Combine records from multiple tables that match on a given condition.\r\n  - **LEFT JOIN**: Retrieve all records from the left table and matched records from the right table.\r\n  - **RIGHT JOIN**: Retrieve all records from the right table and matched records from the left table.\r\n  - **FULL JOIN**: Retrieve all records when there is a match in either left or right table.\r\n\r\n### #### 4. **Aggregations and Grouping**\r\n- Use **GROUP BY** and aggregation functions to summarize data:\r\n  - **COUNT()**, **SUM()**, **AVG()**, **MAX()**, **MIN()**\r\n  - Example: Find the total sales for each product category.\r\n- Apply **HAVING** to filter results after aggregation.\r\n\r\n### #### 5. **Subqueries**\r\n- Write subqueries to perform complex queries:\r\n  - Example: Find the top-selling products by querying the sales data for the highest revenue.\r\n  - Example: Use a subquery to retrieve all customers who have made purchases in the last month.\r\n\r\n### #### 6. **Data Cleaning and Updates**\r\n- Use SQL commands to clean data, such as:\r\n  - **UPDATE**: Modify existing records (e.g., correcting a customer\'s address).\r\n  - **DELETE**: Remove unwanted records.\r\n  - **INSERT INTO**: Add new records into a table.\r\n\r\n### #### 7. **Final Report and Visualization**\r\n- Summarize the results of your analysis and present the insights.\r\n- Optionally, visualize your findings using a tool like Pythons `matplotlib` or Tableau (if applicable).\r\n\r\n---\r\n\r\n## ### **Expected Deliverables**\r\n1. A SQL script with all the queries and commands used to analyze the dataset.\r\n2. A report summarizing the results and insights from the analysis.\r\n3. Visualizations (optional) of key findings.\r\n\r\n---\r\n\r\n## ### **Tools and Libraries**\r\n- **SQL Database:** MySQL, PostgreSQL, or SQLite.\r\n- **Data Analysis Tools:** Optional integration with Python (`pandas`, `matplotlib`) for additional analysis or visualization.\r\n- **SQL Clients:** MySQL Workbench, pgAdmin, or any database management tool you prefer.\r\n\r\n---\r\n\r\n## ### **Skills Gained**\r\n- Gain proficiency in SQL and database operations for data analysis.\r\n- Learn to write complex queries including joins, aggregations, and subqueries.\r\n- Develop the ability to analyze and extract meaningful insights from large datasets.\r\n- Understand how to manipulate data using SQL and clean data in relational databases.\r\n','8f9a4075-56f3-4a24-bca0-084cb2c1df0a','58b2f83e-80ff-4629-9245-4fbed6e30256','2025-01-11 05:35:14','2025-01-11 13:35:14'),('Data Structures and Algo Analyzer',' ','### ## **Project: Implementing and Analyzing Data Structures**\r\n\r\n#### ## **Overview**\r\nIn this project, you will implement key data structures from scratch and analyze their performance using different algorithms. The goal is to understand how each data structure works, how to efficiently implement them, and how their performance varies under different conditions. You will focus on both the implementation and analysis of algorithms for searching, sorting, and other common operations on data structures.\r\n\r\n---\r\n\r\n#### ## **Objectives**\r\n- Implement fundamental data structures such as **arrays**, **linked lists**, **stacks**, **queues**, **binary trees**, and **hash tables**.\r\n- Analyze the time and space complexity of various operations on these data structures.\r\n- Implement and analyze common algorithms like **sorting** and **searching** on these data structures.\r\n- Evaluate and compare the performance of different data structures for different tasks.\r\n\r\n---\r\n\r\n#### ## **Tasks**\r\n\r\n##### ### 1. **Data Structure Implementation**\r\n- Implement the following data structures from scratch in Python:\r\n  - **Array**: Implement basic operations like insertion, deletion, and access.\r\n  - **Linked List**: Implement singly and doubly linked lists, and include operations like insertion, deletion, and traversal.\r\n  - **Stack**: Implement a stack with basic operations such as push, pop, and peek.\r\n  - **Queue**: Implement a queue with operations like enqueue, dequeue, and peek.\r\n  - **Binary Tree**: Implement a binary tree and include operations like insertion, searching, and traversal (in-order, pre-order, post-order).\r\n  - **Hash Table**: Implement a hash table with collision handling (e.g., linear probing or chaining).\r\n\r\n##### ### 2. **Algorithm Implementation**\r\n- Implement and analyze the following algorithms using the data structures youve created:\r\n  - **Sorting Algorithms**: Implement and compare the performance of **Bubble Sort**, **Merge Sort**, **Quick Sort**, and **Insertion Sort** on an array.\r\n  - **Searching Algorithms**: Implement and compare the performance of **Linear Search** and **Binary Search** on an array.\r\n  - **Tree Traversal**: Implement the three main types of tree traversal (in-order, pre-order, and post-order) on a binary tree.\r\n  - **Graph Algorithms**: Implement **Breadth-First Search (BFS)** and **Depth-First Search (DFS)** on a graph.\r\n\r\n##### ### 3. **Performance Analysis**\r\n- Perform time complexity analysis (Big-O notation) for each operation in the data structures and algorithms implemented.\r\n- Measure and compare the execution time of different algorithms on the same dataset using Python\'s **time module**.\r\n- Create a table summarizing the time complexities of various operations on different data structures.\r\n\r\n##### ### 4. **Problem-Solving with Data Structures**\r\n- Solve a few problems that require the use of different data structures, such as:\r\n  - Implement a **queue using two stacks**.\r\n  - Implement a **priority queue** using a binary heap.\r\n  - Solve the **balanced parentheses problem** using a stack.\r\n  - Solve the **LCS (Longest Common Subsequence)** problem using dynamic programming and arrays.\r\n\r\n##### ### 5. **Project Documentation**\r\n- Document your code and the reasoning behind your choice of data structure for each problem.\r\n- Include time and space complexity analysis for each data structure and algorithm.\r\n- Provide sample input and output for each implementation.\r\n- Include insights from the performance tests, such as which algorithms or data structures perform better under different conditions.\r\n\r\n---\r\n\r\n#### ## **Expected Deliverables**\r\n1. A Python project with implemented data structures (arrays, linked lists, stacks, queues, binary trees, hash tables).\r\n2. Python code for various algorithms like sorting, searching, and tree traversal.\r\n3. Performance analysis, including time and space complexity analysis.\r\n4. A report summarizing the findings from problem-solving exercises.\r\n\r\n---\r\n\r\n#### ## **Tools and Libraries**\r\n- **Programming Language:** Python\r\n- **Libraries:** time (for performance testing), pytest (for unit tests)\r\n\r\n---\r\n\r\n#### ## **Skills Gained**\r\n- Understand how to implement and manipulate fundamental data structures.\r\n- Learn the performance implications of different data structures and algorithms.\r\n- Gain experience in analyzing time and space complexity of algorithms.\r\n- Improve problem-solving skills using appropriate data structures.\r\n- Develop skills in performance testing and optimization.\r\n','8df31585-b33f-40f8-a95a-b6f20a3435df','6ac3d9bc-756b-47be-a069-05b2967d556a','2025-01-11 05:24:36','2025-01-11 13:24:36'),('Global Health Data Exploration',' ','## **Project: Exploratory Data Analysis on a Global Health Dataset**\r\n\r\n### **Overview**\r\nIn this project, you will perform an exploratory data analysis (EDA) on a publicly available health dataset. You will clean and preprocess the data, identify key features, and visualize various patterns and insights that can help understand global health trends. This project will help you develop a strong foundation in EDA techniques, data cleaning, and visualization, which are essential skills in data science.\r\n\r\n### **Objective**\r\n- Understand the structure and features of a dataset.\r\n- Perform data cleaning and handle missing or incorrect values.\r\n- Explore the relationships between variables using visualizations.\r\n- Derive insights from the data to answer specific questions.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection and Preprocessing**\r\n- Choose a publicly available health-related dataset such as the **World Health Organization (WHO) Global Health Dataset**, **UNICEF Health Indicators**, or **Kaggle\'s Global Health Data**.\r\n- Load the data and examine the first few rows to understand its structure.\r\n- Clean the dataset by handling missing values, removing duplicates, and ensuring that data types are correct.\r\n\r\n#### 2. **Data Cleaning and Transformation**\r\n- Handle missing data by filling in, interpolating, or dropping missing values based on the nature of the data.\r\n- Transform features as needed, such as encoding categorical variables, scaling numerical variables, or aggregating data.\r\n\r\n#### 3. **Univariate Analysis**\r\n- Perform univariate analysis on the key features, such as age, life expectancy, infant mortality rate, etc.\r\n  - Visualize the distribution of variables using histograms, box plots, or density plots.\r\n  - Summarize statistics such as mean, median, mode, standard deviation, and range.\r\n\r\n#### 4. **Bivariate Analysis**\r\n- Explore relationships between two variables, especially those that might influence health outcomes.\r\n  - Use scatter plots, bar plots, or correlation matrices to visualize relationships.\r\n  - For example, analyze the relationship between GDP per capita and life expectancy or the impact of education level on health outcomes.\r\n\r\n#### 5. **Multivariate Analysis**\r\n- Explore the relationships among multiple features, such as education, income, and access to healthcare.\r\n  - Use techniques like pair plots or heatmaps to visualize correlations and interactions.\r\n  - Investigate how multiple features influence key health indicators.\r\n\r\n#### 6. **Feature Engineering**\r\n- Create new features that could provide additional insights, such as:\r\n  - A ratio between healthcare spending and GDP.\r\n  - A combined health score based on multiple health indicators.\r\n\r\n#### 7. **Outlier Detection**\r\n- Identify any outliers or unusual data points that might skew the analysis.\r\n  - Use box plots, z-scores, or IQR methods to detect and handle outliers.\r\n\r\n#### 8. **Data Visualization**\r\n- Visualize the data using a variety of plots to uncover patterns and trends.\r\n  - Use bar plots, line charts, and geographic visualizations (if location data is available) to explore regional trends in health outcomes.\r\n  - Use libraries like **Matplotlib**, **Seaborn**, or **Plotly** to create rich, interactive visualizations.\r\n\r\n#### 9. **Hypothesis Testing (Optional)**\r\n- If applicable, perform statistical hypothesis testing to validate any hypotheses based on your analysis.\r\n  - For example, test if there is a statistically significant difference in life expectancy between high-income and low-income countries.\r\n\r\n#### 10. **Conclusion and Insights**\r\n- Summarize your findings and insights from the EDA process.\r\n  - Identify key health trends, disparities, or patterns.\r\n  - Provide recommendations for policymakers, healthcare providers, or organizations based on your analysis.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks containing data cleaning, analysis, and visualization steps.\r\n2. **Documentation**: A detailed report describing your methodology, findings, and insights from the EDA.\r\n3. **Presentation**: A slide deck summarizing key visualizations and insights from your analysis.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Proficiency in data cleaning and preprocessing techniques.\r\n- Knowledge of univariate, bivariate, and multivariate analysis.\r\n- Experience in visualizing data patterns and trends using Python libraries.\r\n- Ability to derive actionable insights from raw data.\r\n\r\n','47973087-99e9-44c2-b87e-fe185054256d','6c8ac793-cbe1-4565-8846-bfb74edd4a5a','2025-01-11 07:18:04','2025-01-11 15:18:04'),('Big Data Analytics with PySpark','  ','## **Project: Big Data Analytics with PySpark**\r\n\r\n### **Overview**\r\nIn this project, you will perform big data analytics on large datasets using **PySpark**. PySpark is the Python API for **Apache Spark**, a distributed computing system that allows for efficient processing of massive datasets. The goal is to explore the basics of PySpark, from data loading and transformation to applying machine learning algorithms and performing data analysis.\r\n\r\n### **Objective**\r\n- Understand the basics of PySpark and its ecosystem.\r\n- Learn how to process and analyze large datasets using Spark DataFrames.\r\n- Apply machine learning algorithms on big data using PySpark MLlib.\r\n- Gain experience in performing big data analysis tasks like aggregation, filtering, and joining large datasets.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Setting up PySpark**\r\n- Install and configure PySpark on your local machine or cloud environment (e.g., AWS EMR, Databricks).\r\n- Set up a **SparkSession** to start using Spark with Python.\r\n- Familiarize yourself with the core components of Spark, such as **SparkContext**, **RDDs**, and **DataFrames**.\r\n\r\n#### 2. **Loading and Exploring Big Data**\r\n- Use PySpark to load large datasets (e.g., **CSV**, **JSON**, **Parquet**) into a **DataFrame**.\r\n- Perform basic exploratory data analysis (EDA) to understand the structure of the data.\r\n- Check the size of the data, inspect the schema, and handle any missing or null values in the dataset.\r\n  \r\n#### 3. **Data Cleaning and Transformation**\r\n- Perform data transformation tasks such as:\r\n  - Removing or imputing missing values.\r\n  - Filtering out irrelevant or noisy data.\r\n  - Aggregating the data using group by and aggregating functions.\r\n  - Handling categorical variables using one-hot encoding or other transformations.\r\n  \r\n#### 4. **Data Aggregation and Analysis**\r\n- Apply common data aggregation functions like **sum**, **average**, **count**, and **groupBy** to summarize the dataset.\r\n- Use **window functions** to perform advanced aggregation tasks, such as calculating moving averages or running totals.\r\n- Join multiple datasets to combine information from different sources, leveraging the distributed nature of Spark.\r\n\r\n#### 5. **Machine Learning with PySpark MLlib**\r\n- Choose a dataset suitable for machine learning tasks (e.g., classification or regression).\r\n- Preprocess the data for machine learning by transforming features, normalizing, or scaling data.\r\n- Train a machine learning model using **PySpark MLlib** (e.g., **Logistic Regression**, **Random Forest**, or **KMeans clustering**).\r\n- Evaluate the model using metrics such as accuracy, precision, recall, or F1 score.\r\n\r\n#### 6. **Optimizing PySpark Code**\r\n- Use **Sparks caching** and **partitioning techniques** to optimize performance and reduce memory consumption.\r\n- Explore the use of **broadcast joins** and **shuffling** for handling large datasets effectively.\r\n\r\n#### 7. **Visualization**\r\n- Use **Matplotlib** or **Seaborn** for visualizing PySpark DataFrames and the results of your analysis.\r\n- Visualize the distribution of variables, correlations, and results of machine learning models.\r\n\r\n#### 8. **Deployment and Scaling**\r\n- Experiment with running PySpark jobs in a cloud environment (e.g., AWS, GCP, or Azure).\r\n- Explore the possibilities of scaling up your data processing tasks by leveraging **Spark on Kubernetes** or cloud-based clusters.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Source Code**: PySpark code for data loading, transformation, aggregation, and machine learning tasks.\r\n2. **Data Analysis Report**: Insights derived from the big data, including visualizations and key findings.\r\n3. **Machine Learning Model**: A trained machine learning model, including evaluation metrics and performance.\r\n4. **Deployment Documentation**: Instructions on how to run the project locally or on a cloud environment.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Proficiency in using **PySpark** for big data processing and analysis.\r\n- Understanding how to handle large datasets and scale processing using Sparks distributed computing framework.\r\n- Experience in machine learning and data transformations using **MLlib**.\r\n- Knowledge of performance optimization techniques for big data jobs.\r\n- Hands-on experience with big data analytics in a real-world scenario.\r\n\r\n---','5d35a77a-0981-45da-81b6-24aa87b34b09','6d533a2f-4bc7-416d-9bba-7ae27422a966','2025-01-11 07:56:03','2025-01-11 15:56:03'),('Cloud-based Data Science Pipeline with AWS',' ','## **Project: Setting Up a Cloud-based Data Science Pipeline Using AWS**\r\n\r\n### **Overview**\r\nIn this project, you will create a data science pipeline in the cloud using **AWS (Amazon Web Services)** to manage, process, and analyze large datasets. You will utilize various cloud services to build an end-to-end data pipeline that includes data storage, computation, and result visualization. The goal is to demonstrate how cloud computing can be leveraged for scalable data science projects.\r\n\r\n### **Objective**\r\n- Learn how to set up cloud infrastructure for data science projects.\r\n- Use cloud services like **Amazon S3** for data storage, **AWS Lambda** for serverless computation, and **Amazon SageMaker** for machine learning.\r\n- Understand how to build scalable and cost-effective data pipelines using cloud tools.\r\n- Analyze large datasets and visualize the results with cloud-based services.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Set Up Cloud Storage (Amazon S3)**\r\n- Create an Amazon S3 bucket to store raw and processed datasets.\r\n- Upload a large dataset to the S3 bucket (e.g., a dataset with at least 1GB of data).\r\n- Configure bucket policies and permissions to ensure proper access control.\r\n- Set up versioning for the data stored in S3 to keep track of changes.\r\n\r\n#### 2. **Data Preprocessing with AWS Lambda**\r\n- Use **AWS Lambda** to create a serverless function that processes the data stored in Amazon S3.\r\n- Set up an event trigger so that when new data is uploaded to the S3 bucket, the Lambda function is automatically invoked to preprocess the data (e.g., clean, filter, and transform the data).\r\n- The Lambda function should output the processed data into another S3 bucket.\r\n\r\n#### 3. **Data Analysis Using Amazon SageMaker**\r\n- Use **Amazon SageMaker** to create a Jupyter Notebook environment for data analysis.\r\n- Load the preprocessed data from S3 into SageMaker and perform basic exploratory data analysis (EDA).\r\n- Implement a simple machine learning model (e.g., linear regression or classification) using SageMakers built-in algorithms.\r\n- Evaluate the models performance using metrics such as accuracy, precision, and recall.\r\n\r\n#### 4. **Data Visualization with Amazon QuickSight**\r\n- Use **Amazon QuickSight** to create interactive dashboards for data visualization.\r\n- Load the analysis results into QuickSight and create visualizations such as bar charts, line graphs, or heatmaps to display key insights.\r\n- Design an interactive dashboard that allows users to explore the data and the models predictions in real time.\r\n\r\n#### 5. **Automate the Pipeline**\r\n- Automate the entire pipeline using **AWS Step Functions** to create a workflow that runs the following sequence:\r\n  - Data is uploaded to S3.\r\n  - AWS Lambda preprocesses the data.\r\n  - The processed data is analyzed using SageMaker.\r\n  - Results are visualized in QuickSight.\r\n  \r\n- Configure the Step Function to trigger each service in the correct sequence, ensuring a fully automated end-to-end data pipeline.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts for AWS Lambda functions, SageMaker notebooks, and Step Functions workflow.\r\n2. **Documentation**: Detailed setup instructions and an explanation of the cloud services used in the project.\r\n3. **Cloud Environment Setup**: Screenshots or a video showing the setup of S3 buckets, Lambda functions, SageMaker notebooks, and QuickSight dashboards.\r\n4. **Presentation**: A summary of the projects architecture, the tools used, and the results obtained from data analysis and visualization.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Experience in using AWS cloud services like S3, Lambda, SageMaker, and QuickSight for data science projects.\r\n- Understanding of cloud infrastructure and how to integrate different AWS services into an end-to-end pipeline.\r\n- Hands-on experience in automating data workflows and building scalable cloud solutions.\r\n- Knowledge of building machine learning models and visualizing results in a cloud-based environment.\r\n\r\n---','28e0e1f6-2cd3-470e-8e05-19332a95bf21','755a30c0-e416-4297-83c5-ba6c75d938ca','2025-01-11 07:51:06','2025-01-11 15:51:06'),('House Price Prediction with Regression',' ','## **Project: Predicting House Prices Using Regression**\r\n\r\n### **Overview**\r\nIn this project, you will use supervised learning techniques to predict house prices based on various features such as location, square footage, number of bedrooms, and more. The project will guide you through applying regression models such as Linear Regression, Decision Trees, and Random Forests to predict house prices. You will evaluate the models and improve their performance by tuning hyperparameters and using techniques like cross-validation.\r\n\r\n### **Objective**\r\n- Apply regression models to predict house prices based on multiple features.\r\n- Evaluate and compare different regression algorithms.\r\n- Use performance metrics like RMSE (Root Mean Squared Error) to assess model accuracy.\r\n- Improve the model by tuning parameters and feature engineering.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection and Preprocessing**\r\n- Choose a dataset that contains house pricing data. A good dataset for this project is the **Kaggle House Prices Dataset** or **Boston Housing Dataset**.\r\n- Clean the data by handling missing values, converting categorical variables into numerical features, and scaling numerical features where necessary.\r\n\r\n#### 2. **Exploratory Data Analysis (EDA)**\r\n- Perform EDA to understand the relationships between features and the target variable (house price).\r\n- Visualize the data using histograms, scatter plots, and correlation heatmaps to identify potential patterns.\r\n\r\n#### 3. **Model Building with Linear Regression**\r\n- Implement a **Linear Regression model** to predict house prices based on the features.\r\n  - Evaluate the model using metrics such as R-squared and RMSE (Root Mean Squared Error).\r\n  - Visualize the residuals to check for patterns that could indicate a poor model fit.\r\n\r\n#### 4. **Model Building with Decision Trees and Random Forests**\r\n- Implement a **Decision Tree Regressor** and a **Random Forest Regressor**.\r\n  - Compare the performance of these models to the linear regression model.\r\n  - Use cross-validation to tune hyperparameters and avoid overfitting.\r\n\r\n#### 5. **Feature Engineering**\r\n- Apply feature engineering to improve model performance.\r\n  - Create new features based on existing ones (e.g., combining square footage and number of rooms).\r\n  - Perform feature selection to reduce the number of irrelevant or highly correlated features.\r\n\r\n#### 6. **Model Evaluation**\r\n- Evaluate the performance of all the regression models you built using metrics like R-squared, MAE (Mean Absolute Error), and RMSE.\r\n- Compare the models and select the one that performs best on unseen data.\r\n\r\n#### 7. **Model Improvement**\r\n- Use techniques like **Grid Search** or **Randomized Search** for hyperparameter tuning.\r\n- Try different regression models (e.g., Ridge or Lasso Regression) and evaluate their performance.\r\n\r\n#### 8. **Visualization of Results**\r\n- Plot the predicted house prices against the actual values to visualize how well your model performed.\r\n- Use residual plots to check the accuracy and assumptions of the regression models.\r\n\r\n#### 9. **Conclusion and Recommendations**\r\n- Summarize your findings and recommend the best model for predicting house prices.\r\n  - Discuss the impact of different features on house prices.\r\n  - Provide insights into how businesses or individuals can use the model to make better pricing decisions.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks for data preprocessing, model training, evaluation, and visualization.\r\n2. **Documentation**: A detailed report explaining the data, models used, evaluation metrics, and findings.\r\n3. **Presentation**: A slide deck summarizing your approach, results, and insights.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Hands-on experience with regression techniques like Linear Regression, Decision Trees, and Random Forests.\r\n- Understanding of model evaluation metrics such as RMSE and R-squared.\r\n- Experience in feature engineering and model tuning to improve performance.\r\n- Skills in presenting results and insights from regression models.\r\n\r\n','4012e606-0ada-41c4-923b-3e128463e27a','7a00a298-97ab-4421-97f2-eb157fb69907','2025-01-11 07:12:07','2025-01-11 15:12:07'),('Customer Churn Prediction Model',' ','## **Project: Predicting Customer Churn with Machine Learning**\r\n\r\n### **Overview**\r\nIn this project, you will build a machine learning model to predict customer churn for a subscription-based service. By applying various machine learning algorithms, you will explore classification techniques, model evaluation, and feature engineering to improve model performance. This project will help you understand the fundamental concepts and techniques in machine learning.\r\n\r\n### **Objective**\r\n- Apply foundational machine learning algorithms to a real-world classification problem.\r\n- Learn to preprocess data, feature engineering, and model selection.\r\n- Evaluate the performance of different models using appropriate metrics.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection and Preprocessing**\r\n- Choose a publicly available dataset like the **Telco Customer Churn dataset** or **Customer Churn Prediction dataset** on Kaggle.\r\n- Load the dataset and inspect its structure.\r\n- Clean the data by handling missing values, encoding categorical features, and normalizing numerical features.\r\n\r\n#### 2. **Exploratory Data Analysis (EDA)**\r\n- Conduct EDA to understand the key features of the data and their relationships.\r\n  - Use visualizations like histograms, box plots, and correlation matrices.\r\n  - Identify any potential feature engineering opportunities or patterns in the data.\r\n\r\n#### 3. **Feature Engineering**\r\n- Create new features that might improve model accuracy, such as:\r\n  - Age of the customer based on their contract duration.\r\n  - Combined features like \"tenure + monthly charges\" to capture customer spending patterns.\r\n\r\n#### 4. **Splitting the Data**\r\n- Split the dataset into training and testing sets (e.g., 80% training and 20% testing).\r\n\r\n#### 5. **Model Building**\r\n- Implement different machine learning models for classification:\r\n  - **Logistic Regression**\r\n  - **Decision Trees**\r\n  - **Random Forest**\r\n  - **Support Vector Machine (SVM)**\r\n  - **K-Nearest Neighbors (KNN)**\r\n\r\n#### 6. **Model Evaluation**\r\n- Evaluate the models using classification metrics like:\r\n  - Accuracy\r\n  - Precision, Recall, F1-Score\r\n  - ROC-AUC\r\n- Compare the performance of different models and identify the best one.\r\n\r\n#### 7. **Model Tuning**\r\n- Fine-tune hyperparameters using techniques like Grid Search or Randomized Search to improve model performance.\r\n\r\n#### 8. **Final Model Deployment (Optional)**\r\n- If applicable, deploy the final model as a REST API or web app using **Flask** or **FastAPI** for real-time predictions.\r\n\r\n#### 9. **Conclusion and Insights**\r\n- Summarize the findings, including the best-performing model and any insights into customer churn patterns.\r\n- Discuss potential next steps for improving model accuracy or addressing business problems related to customer churn.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks containing data cleaning, EDA, model building, and evaluation.\r\n2. **Documentation**: A detailed report discussing the data analysis, machine learning techniques, and results.\r\n3. **Presentation**: A slide deck summarizing key findings, insights, and the final model performance.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Understanding of key machine learning concepts like supervised learning and classification.\r\n- Proficiency in data preprocessing, feature engineering, and model evaluation.\r\n- Knowledge of different machine learning algorithms and their use cases.\r\n- Experience in model optimization and performance improvement.\r\n\r\n','d25498d9-4c3c-4bf4-bf20-330c59578aa9','7d1fe466-b47d-43a4-aa12-1d9b78cae8f1','2025-01-11 07:21:49','2025-01-11 15:21:49'),('Algebraic Insights for Data Science',' ','### Overview:\r\nLinear Algebra plays a critical role in data science, particularly in machine learning and deep learning, where it is used to understand and manipulate datasets, build models, and optimize performance. The project outlined below will cover key aspects of Linear Algebra for Data Science, including the application of matrices, vectors, eigenvalues, and singular value decomposition (SVD) in building machine learning models, dimensionality reduction, and more.\r\n\r\n---\r\n\r\n### 1. **Linear Algebra Fundamentals for Data Science:**\r\n#### Key Concepts:\r\n- **Vectors**: Representing data points, features, and model parameters.\r\n- **Matrices**: Organizing data, operations for transformations, and linear systems.\r\n- **Eigenvalues and Eigenvectors**: Essential for Principal Component Analysis (PCA) and spectral clustering.\r\n- **Determinants and Inverses**: Used for understanding matrix properties and solving systems of linear equations.\r\n- **Singular Value Decomposition (SVD)**: Key for dimensionality reduction, image compression, and recommender systems.\r\n  \r\n---\r\n\r\n#### 2. **Data Representation Using Matrices and Vectors:**\r\n#### Project Goals:\r\n1. **Matrix Representation of Data**:\r\n   - Learn how to represent datasets as matrices, with rows as data points and columns as features.\r\n   - Apply matrix operations like multiplication and addition to transform data.\r\n   \r\n2. **Vectorization**:\r\n   - Understand how vectors represent single data points, and how vectorized operations speed up computation.\r\n   - Learn about the use of dot products, vector norms, and angle between vectors in similarity measures and model optimization.\r\n\r\n3. **Dimensionality Reduction**:\r\n   - Use SVD and PCA to reduce the dimensionality of large datasets, retaining the most important information while discarding noise.\r\n   - Implement a PCA-based dimensionality reduction method on a dataset and visualize the results.\r\n\r\n---\r\n\r\n### 3. **Machine Learning Algorithms Involving Linear Algebra:**\r\n#### Key Algorithms:\r\n- **Linear Regression**:\r\n  - Understand the linear regression model using matrix equations (normal equation).\r\n  - Implement the model and solve using matrix algebra techniques.\r\n  \r\n- **Logistic Regression**:\r\n  - Solve logistic regression problems using linear algebra methods, like gradient descent.\r\n  - Investigate how the sigmoid function interacts with the feature vectors.\r\n\r\n- **Principal Component Analysis (PCA)**:\r\n  - Implement PCA using SVD to extract the most important components from a dataset and reduce its dimensionality.\r\n  - Use PCA for data preprocessing in machine learning models to improve performance and speed.\r\n  \r\n- **Singular Value Decomposition (SVD)**:\r\n  - Implement collaborative filtering for a recommendation system using SVD.\r\n  - Decompose large matrices of user-item ratings and reconstruct approximations of missing values.\r\n\r\n---\r\n\r\n### 4. **Building and Deploying a Data Science Model:**\r\n\r\n#### 1. **Data Collection and Preprocessing**:\r\n   - Use a public dataset (e.g., customer churn, housing prices) and apply data cleaning, normalization, and transformation.\r\n   - Handle missing data, outliers, and apply scaling techniques to the data (standardization, min-max scaling).\r\n\r\n#### 2. **Model Training**:\r\n   - Train machine learning models such as linear regression, k-means clustering, and PCA using matrix operations.\r\n   - Apply cross-validation techniques to select the best hyperparameters.\r\n\r\n#### 3. **Model Evaluation and Performance Metrics**:\r\n   - Evaluate model performance using metrics like mean squared error (MSE), R-squared for regression tasks, and silhouette scores for clustering.\r\n\r\n#### 4. **Deployment**:\r\n   - Package the model into a REST API using Flask or FastAPI.\r\n   - Containerize the model using Docker for portability and ease of deployment.\r\n   - Deploy the model to a cloud service (AWS, GCP, or Azure) and expose endpoints for real-time predictions.\r\n\r\n---\r\n\r\n### 5. **Key Linear Algebra Applications in Data Science:**\r\n#### 1. **Dimensionality Reduction**:\r\n   - Implement PCA to reduce the number of features in a dataset, improving model training time and avoiding overfitting.\r\n   - Apply dimensionality reduction to high-dimensional datasets like images, using techniques like t-SNE for visualization.\r\n\r\n#### 2. **Recommendation Systems**:\r\n   - Build a recommendation engine using matrix factorization techniques like SVD.\r\n   - Use collaborative filtering methods to predict user preferences and recommend items (movies, books, etc.).\r\n\r\n#### 3. **Image Processing and Compression**:\r\n   - Implement image compression techniques using SVD to reduce the size of image files while retaining critical information.\r\n   - Visualize how the image quality changes with different numbers of singular values retained.\r\n\r\n#### 4. **Optimization in Machine Learning**:\r\n   - Use gradient descent methods and matrix operations to optimize machine learning algorithms, particularly in neural networks.\r\n   - Apply matrix calculus for backpropagation and optimization in deep learning models.\r\n\r\n---\r\n','d756bb07-e5c7-40d9-8b31-dc9a859672a5','9363066d-2902-46c4-af5f-883406483bdb','2025-01-10 18:45:54','2025-01-11 02:45:54'),('R Data Insights',' ','## **Project: Exploring and Analyzing a Public Dataset in R**\r\n\r\n### **Overview**\r\nIn this project, you will apply the concepts of data manipulation, visualization, and analysis using R. You will select a public dataset, clean and explore the data, perform statistical analysis, and visualize the results to extract meaningful insights. The project will demonstrate your ability to use R for data analysis tasks such as data cleaning, hypothesis testing, regression analysis, and visualization.\r\n\r\n---\r\n\r\n### **Objectives**\r\n- Load and clean a real-world dataset using R.\r\n- Perform exploratory data analysis (EDA) to summarize the dataset.\r\n- Visualize the data using different types of plots.\r\n- Perform statistical tests and hypothesis testing.\r\n- Build and evaluate a simple predictive model.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Dataset Selection**\r\n- Choose a public dataset from sources like Kaggle, UCI Machine Learning Repository, or data.gov.\r\n- Examples of datasets you can use:\r\n  - **Titanic dataset**: A dataset containing information about passengers on the Titanic and whether they survived.\r\n  - **Iris dataset**: A well-known dataset for classification, containing measurements of iris flowers.\r\n  - **Airbnb listings**: A dataset containing Airbnb listing information (price, location, ratings, etc.).\r\n\r\n#### 2. **Data Loading and Cleaning**\r\n- Load the dataset into R using the `read.csv()` function or similar functions for other formats (e.g., `read.xlsx()` for Excel files).\r\n- Clean the dataset by:\r\n  - Handling missing values (e.g., removing or imputing missing data).\r\n  - Converting data types (e.g., ensuring numerical data is in the correct format).\r\n  - Removing duplicates or outliers if necessary.\r\n\r\n#### 3. **Exploratory Data Analysis (EDA)**\r\n- Conduct basic exploratory data analysis:\r\n  - Summarize the dataset using functions like `summary()`, `str()`, and `glimpse()`.\r\n  - Generate descriptive statistics like mean, median, standard deviation, and correlations.\r\n  - Visualize distributions of variables (e.g., histograms, boxplots) and relationships between variables (e.g., scatter plots, pair plots).\r\n\r\n#### 4. **Data Visualization**\r\n- Create insightful visualizations using `ggplot2` or base R plotting functions:\r\n  - **Bar Plots** for categorical data.\r\n  - **Histograms** for distributions of numerical data.\r\n  - **Box Plots** for visualizing data spread and outliers.\r\n  - **Scatter Plots** for relationships between two variables.\r\n  - **Heatmaps** for correlation matrices.\r\n\r\n#### 5. **Statistical Analysis and Hypothesis Testing**\r\n- Perform basic statistical analysis:\r\n  - Test for correlation between variables using Pearsons or Spearmans correlation.\r\n  - Conduct hypothesis testing (e.g., t-tests, chi-square tests) to answer specific questions about the dataset.\r\n  - Interpret the p-values and statistical significance of the tests.\r\n\r\n#### 6. **Building a Predictive Model**\r\n- Build a simple regression or classification model:\r\n  - Use **linear regression** for predicting a continuous outcome variable.\r\n  - Use **logistic regression** or decision trees for binary classification tasks.\r\n- Split the dataset into training and testing sets using the `sample()` function.\r\n- Evaluate the model using metrics like accuracy, precision, recall, or RMSE (Root Mean Squared Error).\r\n\r\n#### 7. **Project Documentation**\r\n- Document the steps you took for data cleaning, exploration, visualization, and analysis.\r\n- Provide detailed explanations of the insights gained from the data and your conclusions.\r\n- Include code comments and clear explanations of your analysis process.\r\n- Submit the final R script or R Markdown file along with the visualizations and analysis.\r\n\r\n---\r\n\r\n### **Expected Deliverables**\r\n1. An R script or R Markdown file containing the code for data loading, cleaning, analysis, and modeling.\r\n2. Visualizations that illustrate key findings from the data.\r\n3. A report summarizing the steps, methods used, and insights gained from the analysis.\r\n4. A predictive model with performance metrics (if applicable).\r\n\r\n---\r\n\r\n### **Tools and Libraries**\r\n- **Programming Language:** R\r\n- **Libraries:**\r\n  - `tidyverse` (for data manipulation and visualization).\r\n  - `ggplot2` (for data visualization).\r\n  - `dplyr` (for data manipulation).\r\n  - `caret` (for machine learning tasks).\r\n  - `shiny` (optional, for creating interactive visualizations).\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Understand how to load and clean real-world datasets in R.\r\n- Perform basic statistical analysis and hypothesis testing.\r\n- Visualize data in R using different plotting techniques.\r\n- Build and evaluate predictive models using regression or classification algorithms.\r\n- Gain practical experience with R and the `tidyverse` library for data analysis tasks.\r\n','e95b2987-b1fc-4731-80cc-49f535b522fc','94d5112e-bff6-44c1-b764-ad73cccff311','2025-01-11 05:29:48','2025-01-11 13:29:48'),('Ethical Practices in Data Science',' ','## **Project: Addressing Ethical Issues in Data Science**\r\n\r\n### **Overview**\r\nIn this project, you will explore and address ethical challenges faced by data scientists in the real world. The focus will be on understanding ethical considerations around data collection, data privacy, algorithmic bias, and fairness. You will analyze a dataset through the lens of ethical concerns and propose solutions for mitigating ethical risks.\r\n\r\n### **Objective**\r\n- Identify and analyze potential ethical issues related to data science and machine learning models.\r\n- Understand how bias, fairness, and privacy concerns can impact the integrity and fairness of data-driven systems.\r\n- Propose strategies for mitigating these risks and promoting ethical practices in data science.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Research Ethical Issues in Data Science**\r\n- Explore the following topics:\r\n  - **Data Privacy**: How is sensitive data handled? Are users\' privacy rights respected?\r\n  - **Bias and Fairness**: How do biases in data impact machine learning models? What steps can be taken to reduce bias?\r\n  - **Transparency**: Are the methods and algorithms used in data science applications transparent and understandable?\r\n  - **Accountability**: Who is responsible when algorithms make harmful decisions?\r\n\r\n#### 2. **Select a Dataset**\r\n- Choose a dataset that is relevant to an ethical issue in data science. Some options include:\r\n  - **Healthcare Data**: Explore ethical concerns around patient privacy and data usage in healthcare systems.\r\n  - **Facial Recognition Data**: Examine biases in facial recognition algorithms that might lead to discrimination based on race or gender.\r\n  - **Hiring Data**: Analyze potential biases in recruitment algorithms that could favor certain demographics over others.\r\n  - **Criminal Justice Data**: Assess the fairness of predictive algorithms used to assess the risk of offenders in judicial decisions.\r\n\r\n#### 3. **Data Preprocessing with Ethical Considerations**\r\n- Investigate and address any ethical issues in the dataset itself:\r\n  - Ensure the data is anonymized or pseudonymized to protect individual privacy.\r\n  - Identify any existing biases in the data, such as skewed representation of certain groups.\r\n  - Handle missing data or outliers in ways that do not disproportionately impact specific groups or individuals.\r\n\r\n#### 4. **Data Analysis and Bias Detection**\r\n- Use statistical or machine learning methods to identify biases within the dataset.\r\n  - For example, explore if certain demographic groups are underrepresented or overrepresented.\r\n  - Check for imbalance in outcomes (e.g., if certain groups experience worse outcomes than others).\r\n- Analyze the potential implications of these biases in real-world decision-making.\r\n\r\n#### 5. **Propose Ethical Solutions**\r\n- Based on your findings, propose solutions for reducing bias, improving fairness, and protecting privacy. Some solutions may include:\r\n  - Using more representative datasets to reduce bias.\r\n  - Implementing algorithms that can explain their decisions (Explainable AI).\r\n  - Adopting fairness constraints when training models.\r\n  - Incorporating privacy-preserving techniques like differential privacy.\r\n\r\n#### 6. **Report and Presentation**\r\n- Write a report summarizing your analysis, ethical concerns, and proposed solutions.\r\n  - Discuss the ethical risks posed by the data and model.\r\n  - Reflect on the importance of ethics in data science and how it impacts society.\r\n- Create a presentation to share your findings and solutions, including visualizations to communicate your points effectively.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks showing data preprocessing, bias detection, and model analysis.\r\n2. **Documentation**: A report detailing your findings, ethical considerations, and proposed solutions.\r\n3. **Presentation**: A slide deck summarizing the project, including ethical concerns, analysis, and proposed ethical guidelines.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Awareness of ethical issues in data science and machine learning.\r\n- Understanding of data privacy, fairness, and bias detection.\r\n- Ability to implement ethical practices in data preprocessing and analysis.\r\n- Skills in communicating complex ethical issues in data science clearly to a broader audience.\r\n\r\n','af1709c2-9c15-4581-89bc-9acf66e06729','9bc41687-6ff4-400a-89d4-748c1a898ae9','2025-01-11 07:03:42','2025-01-11 15:03:42'),('Real-World Data Visualization with Matplotlib',' ','## **Project: Visualizing Real-World Data with Matplotlib**\r\n\r\n### **Overview**\r\nIn this project, you will use **Matplotlib** to create insightful and meaningful visualizations from real-world datasets. The goal is to enhance your understanding of various plotting techniques and how to apply them for effective data communication.\r\n\r\n### **Objective**\r\n- Explore datasets from different domains such as finance, health, and sports.\r\n- Create different types of visualizations to represent the data, such as bar charts, line charts, scatter plots, and histograms.\r\n- Use **Matplotlib** features like titles, legends, gridlines, and annotations to make your visualizations clear and effective.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Choose a Dataset**\r\n- Select a publicly available dataset. Some options include:\r\n  - **COVID-19 Data**: A dataset that includes daily infection rates, recovery rates, and deaths across countries.\r\n  - **Stock Market Data**: Historical data on stock prices from Yahoo Finance or Google Finance.\r\n  - **Iris Dataset**: A classic dataset for classification and clustering with flowers\' physical measurements.\r\n  - **NBA Player Stats**: Historical performance data for NBA players.\r\n  - **US Education Data**: Data on high school graduation rates, school funding, and student performance.\r\n\r\n#### 2. **Data Preprocessing**\r\n- Clean the dataset (if necessary):\r\n  - Remove missing or incorrect data.\r\n  - Transform the data into a format thats easy to work with (e.g., converting timestamps, normalizing values).\r\n- Use **Pandas** or **NumPy** to manipulate and prepare the data for visualization.\r\n\r\n#### 3. **Create Basic Visualizations**\r\n- Use **Matplotlib** to create the following types of visualizations:\r\n  - **Line Charts**: Plot time-series data to show trends over time (e.g., stock prices, COVID-19 cases).\r\n  - **Bar Charts**: Show comparisons between different categories (e.g., sales data across regions).\r\n  - **Histograms**: Visualize the distribution of a dataset (e.g., age distribution, test scores).\r\n  - **Scatter Plots**: Show relationships between two numerical variables (e.g., height vs weight).\r\n\r\n#### 4. **Advanced Visualizations**\r\n- Customize your plots by adding:\r\n  - Titles, axis labels, and legends.\r\n  - Gridlines and annotations to highlight key points.\r\n  - Color schemes, markers, and styles to make the charts more visually appealing.\r\n- Explore additional visualization types such as:\r\n  - **Pie Charts**: Represent proportions of a whole (e.g., market share of companies).\r\n  - **Box Plots**: Show data distribution, median, and outliers.\r\n  - **Heatmaps**: Visualize correlation matrices or geographical data.\r\n\r\n#### 5. **Subplots**\r\n- Use **subplots** to display multiple visualizations in a single figure.\r\n- Create a dashboard-like view of your visualizations with related charts next to each other.\r\n\r\n#### 6. **Interactivity (Optional)**\r\n- Use interactive Matplotlib features, or integrate with **Plotly** or **Dash**, to make your plots interactive (zooming, hover effects, etc.).\r\n\r\n#### 7. **Analysis and Insights**\r\n- Based on your visualizations, derive insights from the data.\r\n  - For example, if analyzing stock prices, identify patterns or trends.\r\n  - For health-related data, compare trends in infection rates across countries.\r\n- Summarize the findings in a written report.\r\n\r\n#### 8. **Report and Presentation**\r\n- Write a report detailing your process, the tools used, and the insights gained from the data analysis and visualizations.\r\n- Create a presentation summarizing your visualizations and the key takeaways from your analysis.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: A set of Python scripts or Jupyter Notebooks for the data processing and visualization tasks.\r\n2. **Documentation**: A report explaining your visualizations, insights, and any challenges you encountered.\r\n3. **Presentation**: A slide deck summarizing the visualizations and insights derived from the project.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Proficiency in **Matplotlib** and its various plotting techniques.\r\n- Ability to clean and preprocess data for visualization.\r\n- Strong understanding of how to represent data visually for better understanding and communication.\r\n- Hands-on experience in presenting data insights through interactive and static visualizations.\r\n\r\n','7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','b13e91c4-fea8-4e3f-92ce-6eedaea08a3b','2025-01-11 05:53:58','2025-01-11 13:53:58'),('Customer Segmentation with Clustering',' ','## **Project: Customer Segmentation Using Clustering**\r\n\r\n### **Overview**\r\nIn this project, you will apply unsupervised learning techniques to segment customers into different groups based on their purchasing behavior. You will explore the use of clustering algorithms such as K-Means and DBSCAN to identify natural groupings within the data. This segmentation can help businesses tailor their marketing strategies to different customer groups more effectively.\r\n\r\n### **Objective**\r\n- Apply clustering techniques to segment data into meaningful groups.\r\n- Understand and implement K-Means and DBSCAN clustering algorithms.\r\n- Evaluate the effectiveness of the clustering models.\r\n- Visualize the results of clustering on high-dimensional data.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection and Preprocessing**\r\n- Choose a dataset related to customer behavior, such as transaction data or demographic data with information about customer purchases. \r\n- Datasets like the **Mall Customer Segmentation Data** or **Retail Data** are good candidates.\r\n- Preprocess the data by handling missing values, scaling the features, and encoding categorical variables if necessary.\r\n\r\n#### 2. **Exploratory Data Analysis (EDA)**\r\n- Perform exploratory data analysis to understand the relationships between features and discover patterns.\r\n- Use techniques like pair plots, heatmaps, and summary statistics to analyze the data.\r\n\r\n#### 3. **Clustering with K-Means Algorithm**\r\n- Implement the **K-Means clustering algorithm** to segment customers into clusters.\r\n  - Choose an optimal number of clusters using methods like the **Elbow Method** or **Silhouette Score**.\r\n  - Visualize the clusters in 2D or 3D space using dimensionality reduction techniques like **PCA**.\r\n\r\n#### 4. **Clustering with DBSCAN Algorithm**\r\n- Implement the **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** algorithm, which is particularly useful for discovering clusters of varying shapes and identifying outliers.\r\n  - Tune the parameters of DBSCAN (like epsilon and minimum points) to identify meaningful clusters.\r\n  - Visualize the results and compare them with K-Means clustering.\r\n\r\n#### 5. **Cluster Profiling**\r\n- Analyze each cluster and provide insights into what each customer group represents.\r\n  - For example, describe characteristics like average spending, frequency of purchases, or demographic details.\r\n  - Identify any potential outliers or unusual patterns.\r\n\r\n#### 6. **Model Evaluation**\r\n- Evaluate the performance of both clustering algorithms.\r\n  - If you have access to labeled data (for example, customer behavior data with existing segment labels), use clustering validation metrics like **Adjusted Rand Index (ARI)** or **Normalized Mutual Information (NMI)**.\r\n  - For unlabeled data, visualize how well the clusters make sense in the context of your data.\r\n\r\n#### 7. **Visualization of Results**\r\n- Visualize the clusters using scatter plots, PCA, t-SNE, or other dimensionality reduction techniques to provide clear insights into the segmentation.\r\n- Use color coding to represent different clusters and interpret the meaning behind each cluster based on the customers characteristics.\r\n\r\n#### 8. **Conclusion and Recommendations**\r\n- Summarize your findings and provide business recommendations based on the customer segmentation.\r\n  - Which customer groups are most likely to respond to specific marketing campaigns?\r\n  - How can the business use this information to improve customer experience or increase sales?\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks for data preprocessing, clustering, and evaluation.\r\n2. **Documentation**: A detailed report explaining the clustering process, findings, and business recommendations.\r\n3. **Presentation**: A slide deck summarizing your analysis, insights, and conclusions, including visualizations of the clusters.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Understanding of unsupervised learning and clustering algorithms.\r\n- Practical experience in applying K-Means and DBSCAN to real-world data.\r\n- Knowledge of evaluating clustering results and interpreting the business implications.\r\n- Skills in visualizing high-dimensional data and presenting the findings effectively.\r\n\r\n','509fa487-d1ff-433c-b3d4-2665f2d495a7','b33dea59-d50b-431f-97f6-a58e74bf833e','2025-01-11 07:08:46','2025-01-11 15:08:46'),('Time Series Analysis and Forecasting:',' ','## **Project: Stock Price Prediction using Time Series Forecasting**\r\n\r\n### **Overview**\r\nIn this project, you will analyze stock price data using time series forecasting techniques. The goal is to develop a model that predicts future stock prices of a company based on historical data. You will employ models like **ARIMA**, **Exponential Smoothing**, and **LSTM (Long Short-Term Memory)** networks to forecast stock prices.\r\n\r\n### **Objective**\r\n- Understand and implement time series analysis techniques to forecast future values.\r\n- Use historical stock price data to predict future stock prices.\r\n- Evaluate the models using error metrics such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE).\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection**\r\n- Obtain historical stock price data for a company (e.g., **Apple**, **Microsoft**, **Google**) using APIs like **Yahoo Finance** or **Alpha Vantage**.\r\n- Ensure the dataset includes daily stock prices: **Open**, **High**, **Low**, **Close**, and **Volume** for a sufficient time period (preferably over several years).\r\n\r\n#### 2. **Data Preprocessing**\r\n- Clean the dataset by handling missing values, outliers, and any incorrect entries.\r\n- Convert the dataset into a time series format and set the **Date** column as the index.\r\n\r\n#### 3. **Exploratory Data Analysis (EDA)**\r\n- Visualize the historical stock prices to understand the trend, seasonality, and volatility.\r\n- Decompose the time series data into its components: trend, seasonal, and residual.\r\n- Use statistical tests like the **Augmented Dickey-Fuller (ADF) test** to check for stationarity and decide if differencing is required.\r\n\r\n#### 4. **Model Building**\r\n- **ARIMA (AutoRegressive Integrated Moving Average)**: \r\n  - Fit an ARIMA model to predict future stock prices using past observations.\r\n  - Choose the appropriate order of the ARIMA model by analyzing the ACF and PACF plots.\r\n\r\n- **Exponential Smoothing**: \r\n  - Use single, double, or triple exponential smoothing to forecast stock prices based on trends and seasonality.\r\n  - Implement models like **Holt-Winters** method to capture seasonality and trends.\r\n\r\n- **LSTM (Long Short-Term Memory)**: \r\n  - Build an LSTM neural network to model and forecast future stock prices based on past sequences.\r\n  - Use frameworks like **TensorFlow** or **Keras** to implement the LSTM network for sequential prediction.\r\n\r\n#### 5. **Model Evaluation**\r\n- Evaluate the performance of each model using error metrics such as:\r\n  - **Mean Squared Error (MSE)**\r\n  - **Root Mean Squared Error (RMSE)**\r\n  - **Mean Absolute Error (MAE)**\r\n  - **R-squared**\r\n\r\n- Visualize the forecasted stock prices versus actual stock prices on test data to assess the models\' predictive power.\r\n\r\n#### 6. **Model Optimization (Optional)**\r\n- Fine-tune the hyperparameters of the models using techniques like **GridSearchCV** or **RandomizedSearchCV**.\r\n- Experiment with different window sizes, learning rates, and batch sizes for the LSTM model to optimize performance.\r\n\r\n#### 7. **Visualization**\r\n- Plot the historical stock prices and predicted prices to visually assess model performance.\r\n- Use **Matplotlib** and **Seaborn** for creating visualizations of the time series data and model predictions.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks containing the full model training, evaluation, and forecasting process.\r\n2. **Documentation**: A detailed report that includes methodology, performance analysis, and findings.\r\n3. **Presentation**: A slide deck summarizing the project and demonstrating the models performance and key findings.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Experience with time series forecasting models like ARIMA, Exponential Smoothing, and LSTM.\r\n- Hands-on practice in data collection, cleaning, and preprocessing.\r\n- Knowledge of evaluating model performance using various metrics.\r\n- Ability to fine-tune and optimize models for better prediction accuracy.\r\n- Experience visualizing time series data and forecasted results.\r\n\r\n---\r\n\r\n### **Project Name**\r\n**Stock Price Forecasting with Time Series Analysis**\r\n\r\n','d75afbc3-5381-4d9c-9553-8a3a48021369','bf13aabc-2ada-43d6-8462-1d6b222f903e','2025-01-11 07:41:08','2025-01-11 15:41:08'),(' Image Classification with CNNs in TensorFlow',' ','## **Project: Image Classification with Convolutional Neural Networks (CNNs) using TensorFlow**\r\n\r\n### **Overview**\r\nIn this project, you will use TensorFlow to build and train a Convolutional Neural Network (CNN) to classify images from the **CIFAR-10 dataset**. The goal is to create a deep learning model capable of classifying images into one of 10 categories, such as airplanes, cars, birds, and more. You\'ll implement and fine-tune the model using TensorFlow and evaluate its performance.\r\n\r\n### **Objective**\r\n- Build a Convolutional Neural Network (CNN) model for image classification.\r\n- Train and evaluate the model on the CIFAR-10 dataset using TensorFlow and Keras.\r\n- Apply techniques like data augmentation and dropout to improve the model\'s performance.\r\n- Experiment with different architectures and hyperparameters.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection**\r\n- Use the **CIFAR-10 dataset**, a popular dataset in machine learning containing 60,000 32x32 color images in 10 different classes (airplanes, cars, birds, etc.).\r\n  - You can load the CIFAR-10 dataset directly from TensorFlow using `tensorflow.keras.datasets.cifar10`.\r\n\r\n#### 2. **Data Preprocessing**\r\n- Normalize the images by scaling pixel values to the range of [0, 1] to improve training efficiency.\r\n- Split the dataset into training and testing sets (typically 50,000 images for training and 10,000 for testing).\r\n- One-hot encode the target labels to represent them as binary vectors.\r\n\r\n#### 3. **Model Architecture**\r\n- Build a CNN model using TensorFlow/Keras with the following layers:\r\n  - **Convolutional layers** to detect features like edges and textures.\r\n  - **MaxPooling layers** to reduce the spatial dimensions of the image.\r\n  - **Dropout layers** to prevent overfitting.\r\n  - **Fully connected layers** to combine the learned features and output the final predictions.\r\n  - A **Softmax activation function** in the output layer to classify the image into one of 10 categories.\r\n\r\n#### 4. **Model Training**\r\n- Compile the model with an appropriate optimizer (e.g., Adam) and loss function (e.g., categorical crossentropy).\r\n- Train the model on the training data, and monitor the training and validation loss/accuracy during each epoch.\r\n- Apply techniques like **data augmentation** (e.g., random rotations, flips, and zooms) to artificially increase the dataset size and prevent overfitting.\r\n\r\n#### 5. **Model Evaluation**\r\n- Evaluate the trained model on the test dataset and calculate performance metrics such as:\r\n  - **Accuracy**\r\n  - **Confusion Matrix**\r\n  - **Precision** and **Recall**\r\n  - **F1 Score**\r\n\r\n#### 6. **Model Optimization**\r\n- Experiment with different CNN architectures (e.g., adding more layers or changing the number of filters).\r\n- Use techniques like **batch normalization** and **learning rate scheduling** to improve performance.\r\n- Fine-tune the hyperparameters like the learning rate, batch size, and number of epochs using **GridSearchCV** or **RandomizedSearchCV**.\r\n\r\n#### 7. **Model Visualization**\r\n- Visualize the training process using **TensorBoard** for a detailed view of the models performance.\r\n- Plot the loss and accuracy curves over the training epochs to analyze the model\'s learning.\r\n- Visualize the filters learned by the convolutional layers to understand what features the model is detecting.\r\n\r\n#### 8. **Model Deployment (Optional)**\r\n- Once the model is trained and evaluated, you can deploy it to a web or mobile application.\r\n- Use **TensorFlow Lite** for deploying the model to mobile devices.\r\n- Create a REST API for the model using **Flask** or **FastAPI** to classify new images in real-time.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks containing the full model training, evaluation, and deployment process.\r\n2. **Documentation**: A detailed report that includes the methodology, performance analysis, and findings from your model.\r\n3. **Presentation**: A slide deck summarizing the project and demonstrating the models performance and key findings.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Experience with deep learning concepts like Convolutional Neural Networks (CNNs).\r\n- Practical application of TensorFlow and Keras for building and training models.\r\n- Ability to apply data augmentation and regularization techniques to improve model performance.\r\n- Knowledge of model evaluation, tuning, and optimization in deep learning tasks.\r\n','09ffa3d0-3468-4cf9-8bf2-1acb59b10226','c0867c46-2229-41c4-ae70-818bc6ed439c','2025-01-11 07:38:00','2025-01-11 15:38:00'),('Big Data Analysis with Hadoop & Spark',' ','## Project: Analyzing Large Datasets Using Hadoop and Spark\r\n\r\n### Overview\r\nIn this project, you will use **Hadoop** and **Spark** to analyze large-scale datasets. The goal is to familiarize yourself with the tools and their capabilities in processing and analyzing big data efficiently.\r\n\r\n### Objective\r\n- Process large datasets using **Hadoop** for distributed storage and processing.\r\n- Utilize **Apache Spark** for data analysis and transformations at scale.\r\n- Work with real-world datasets to extract meaningful insights.\r\n\r\n---\r\n\r\n### Tasks\r\n\r\n#### 1. Setting Up the Environment\r\n- Set up a Hadoop cluster (can be done locally or using a cloud-based service like AWS EMR, Azure HDInsight, or Google Cloud Dataproc).\r\n- Install Apache Spark on top of Hadoop or configure Spark standalone mode.\r\n- Familiarize yourself with HDFS (Hadoop Distributed File System) and understand how Spark interacts with it.\r\n\r\n#### 2. Data Ingestion and Storage\r\n- Choose a large dataset for analysis. You can use publicly available datasets such as:\r\n  - **NYC Taxi Trips**: A dataset containing taxi trip data from New York City.\r\n  - **Wikipedia Clickstream Data**: Data representing page views and user interactions with Wikipedia.\r\n  - **Weather Data**: Historical weather data for analysis.\r\n- Ingest the data into HDFS and ensure the dataset is large enough to benefit from the distributed nature of Hadoop and Spark.\r\n\r\n#### 3. Data Processing with Hadoop\r\n- Use Hadoop\'s **MapReduce** to process the dataset:\r\n  - Implement a simple **Word Count** program using MapReduce.\r\n  - Explore other MapReduce operations like filtering, sorting, and aggregating large datasets.\r\n- Use **Hive** (optional) to query data in HDFS using SQL-like syntax, and explore Hives performance when interacting with large datasets.\r\n\r\n#### 4. Data Processing with Apache Spark\r\n- Load the dataset into **Spark** using the **SparkContext** or **SparkSession**.\r\n- Perform data transformations and actions using **Spark RDDs (Resilient Distributed Datasets)** and **DataFrames**:\r\n  - Clean the data (e.g., remove missing values, correct data types).\r\n  - Perform aggregations such as sum, mean, count, and more complex operations like joins.\r\n  - Filter and group data to extract meaningful insights.\r\n\r\n#### 5. Data Analysis\r\n- Apply **Spark MLlib** (machine learning library) for basic machine learning tasks such as:\r\n  - Classification (e.g., using decision trees or logistic regression).\r\n  - Clustering (e.g., using K-means clustering).\r\n- Visualize the results using **Matplotlib** or **PySpark**s built-in plotting capabilities.\r\n\r\n#### 6. Performance Tuning\r\n- Optimize performance in Spark by considering partitioning, caching, and tuning the number of executors, cores, and memory.\r\n- Measure the time taken for specific operations before and after optimization to understand the performance gains.\r\n\r\n#### 7. Report and Presentation\r\n- Write a detailed report summarizing the analysis, techniques used, and insights derived from the dataset.\r\n- Create a presentation showcasing the results, highlighting how Hadoop and Spark were used to process and analyze the data efficiently.\r\n\r\n---\r\n\r\n### Deliverables\r\n1. **Code**: A complete set of scripts or Jupyter Notebooks for the entire process (data ingestion, transformation, analysis, optimization).\r\n2. **Documentation**: A report that explains the steps taken, challenges faced, and insights gained from the analysis.\r\n3. **Presentation**: A slide deck that summarizes the project\'s results.\r\n\r\n---\r\n\r\n### Skills Gained\r\n- Experience with Hadoop\'s HDFS and MapReduce.\r\n- Proficiency in using Apache Spark for big data processing.\r\n- Understanding of the concepts of distributed computing and performance optimization.\r\n- Hands-on experience with big data analytics.\r\n','8634ec7e-c67d-47d3-8eed-337754f21202','c3ea64d5-bca8-4efb-a39e-cc48a644ddbf','2025-01-11 05:48:57','2025-01-11 13:48:57'),('Calculus for Housing Price Prediction',' ','## **Building a Machine Learning Model for Predicting and Optimizing Housing Prices Using Calculus**\r\n\r\n### **Overview**\r\nThis project explores the application of calculus principles, such as optimization, derivatives, and integrals, in building and optimizing a machine learning model. The goal is to predict housing prices using a dataset and demonstrate how calculus techniques are fundamental in machine learning, particularly in training and evaluating models.\r\n\r\n---\r\n\r\n### **Objectives**\r\n- Understand how calculus concepts are applied in machine learning.\r\n- Explore optimization techniques like gradient descent for model training.\r\n- Analyze feature importance using partial derivatives.\r\n- Use integrals to evaluate and interpret cumulative data distributions.\r\n- Build a complete machine learning pipeline with a strong mathematical foundation.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Problem Definition and Data Collection**\r\n- Define the problem of predicting housing prices based on features such as location, size, and amenities.\r\n- Collect and preprocess a dataset (e.g., from Kaggle or APIs like Zillow).\r\n\r\n#### 2. **Exploratory Data Analysis (EDA)**\r\n- Perform detailed analysis of trends, correlations, and distributions in the dataset.\r\n- Create visualizations, including scatter plots, histograms, and correlation heatmaps.\r\n\r\n#### 3. **Feature Engineering**\r\n- Analyze the sensitivity of features to the target variable using derivatives.\r\n- Engineer new features, such as rates of price change or transformations (e.g., logarithms).\r\n- Normalize and scale features for effective model training.\r\n\r\n#### 4. **Understanding Calculus in Machine Learning**\r\n- Explain and demonstrate the role of derivatives in gradient descent optimization.\r\n- Manually compute gradients for optimizing a linear regression model.\r\n\r\n#### 5. **Model Development**\r\n- Implement a linear regression model from scratch and optimize it using manually computed gradients.\r\n- Compare the custom implementation with library-based solutions (e.g., Scikit-learn).\r\n\r\n#### 6. **Optimization Techniques**\r\n- Explore different gradient descent variants, such as stochastic gradient descent and momentum.\r\n- Visualize the optimization process using loss curves to understand convergence behavior.\r\n\r\n#### 7. **Advanced Model Evaluation**\r\n- Use integral calculus to compute cumulative distribution functions (CDFs) of prediction errors.\r\n- Evaluate the reliability of the model using CDF analysis.\r\n\r\n#### 8. **Feature Importance Analysis**\r\n- Calculate the partial derivatives of the target variable with respect to features.\r\n- Visualize the importance of features using charts or bar plots.\r\n\r\n#### 9. **Model Fine-Tuning and Regularization**\r\n- Apply regularization techniques like Lasso and Ridge regression to control overfitting.\r\n- Analyze how penalty terms improve model generalization.\r\n\r\n#### 10. **Result Interpretation and Reporting**\r\n- Summarize findings, insights, and final model performance in a comprehensive report.\r\n- Create visualizations such as gradients, loss curves, and feature importance charts.\r\n- Present the project results with clear conclusions and future improvement suggestions.\r\n\r\n---\r\n\r\n### **Expected Deliverables**\r\n1. A cleaned and preprocessed dataset ready for analysis.\r\n2. Documented code for EDA, feature engineering, model development, and evaluation.\r\n3. Visualizations of gradients, loss curves, and feature importance.\r\n4. A detailed report summarizing findings and the role of calculus in the project.\r\n5. A presentation highlighting key insights and outcomes.\r\n\r\n---\r\n\r\n### **Tools and Libraries**\r\n- **Programming Language:** Python\r\n- **Libraries:** NumPy, Pandas, Matplotlib, Seaborn, Plotly, Scikit-learn\r\n- **Development Environment:** Jupyter Notebook or any preferred IDE\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Application of derivatives and integrals in machine learning.\r\n- Understanding of gradient descent and optimization techniques.\r\n- Hands-on experience in feature engineering and model evaluation.\r\n- Bridging mathematical theory with practical data science applications.\r\n','2e4a681c-5ea5-4269-ad08-fa671500a5d1','cf7d5ebb-7e11-4510-90cd-6435a4803118','2025-01-11 05:03:07','2025-01-11 13:03:07'),('Building a Q-Learning Agent for Grid World','  ','## **Project: Building a Simple Q-Learning Agent for Grid World**\r\n\r\n### **Overview**\r\nIn this project, you will implement a basic **Q-learning** agent to navigate through a **Grid World** environment. The agent will learn the optimal policy through trial and error by interacting with the environment and receiving rewards or penalties based on its actions.\r\n\r\n### **Objective**\r\n- Understand and apply the fundamentals of reinforcement learning, particularly Q-learning.\r\n- Implement an agent that learns to navigate a grid and reach a goal with minimal steps.\r\n- Explore key reinforcement learning concepts like exploration vs. exploitation, rewards, states, actions, and value functions.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Grid World Setup**\r\n- Create a 5x5 grid where each cell represents a state.\r\n- Define a starting position for the agent and a goal position that the agent needs to reach.\r\n- Assign rewards:\r\n  - Positive reward (+1) for reaching the goal.\r\n  - Negative reward (-1) for hitting obstacles or walls.\r\n  - Zero reward for empty spaces.\r\n  \r\n- Make sure the agent can move in four directions: up, down, left, and right.\r\n\r\n#### 2. **Implement Q-Learning Algorithm**\r\n- Initialize a Q-table with all zeros. The Q-table will store the **Q-values** (action-value function), where each cell corresponds to a state-action pair.\r\n- Define the **Q-learning update rule**:\r\n  \\[\r\n  Q(s, a) = Q(s, a) + \\alpha \\left[ R(s, a) + \\gamma \\max_a Q(s\', a\') - Q(s, a) \\right]\r\n  \\]\r\n  where:\r\n  - \\( \\alpha \\) is the learning rate,\r\n  - \\( \\gamma \\) is the discount factor,\r\n  - \\( R(s, a) \\) is the reward for taking action \\( a \\) from state \\( s \\),\r\n  - \\( s\' \\) is the next state after taking action \\( a \\),\r\n  - \\( \\max_a Q(s\', a\') \\) is the maximum Q-value for the next state.\r\n\r\n#### 3. **Exploration vs. Exploitation**\r\n- Implement the **epsilon-greedy policy** to balance exploration and exploitation. \r\n  - **Exploration**: The agent tries random actions (with probability \\( \\epsilon \\)).\r\n  - **Exploitation**: The agent chooses the best action based on the current Q-table (with probability \\( 1 - \\epsilon \\)).\r\n\r\n- Start with a high value of \\( \\epsilon \\) (exploration) and gradually decay it over time to focus more on exploitation.\r\n\r\n#### 4. **Training the Agent**\r\n- Implement the agents training loop where it repeatedly interacts with the environment:\r\n  - For each episode, the agent starts at the initial state and performs actions based on the epsilon-greedy policy.\r\n  - After each action, the agent receives a reward and updates the Q-table.\r\n  - The goal is to learn the optimal policy that maximizes long-term rewards.\r\n  \r\n#### 5. **Evaluation and Visualization**\r\n- After training, evaluate the agent\'s performance by running several test episodes and checking if the agent successfully navigates to the goal.\r\n- Visualize the learned Q-values and the optimal policy (the action with the highest Q-value in each state).\r\n- Track the agent\'s path and the total reward accumulated during the training process.\r\n\r\n#### 6. **Advanced (Optional)**\r\n- **Decay Function**: Implement a decaying learning rate and discount factor to see if the agents performance improves.\r\n- **Multiple Agents**: Expand the environment to support multiple agents and observe the interactions.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python script or Jupyter Notebook with the Q-learning agent, training loop, and environment setup.\r\n2. **Documentation**: A report explaining the steps, the Q-learning algorithm, and the results of the agent\'s learning.\r\n3. **Presentation**: A summary of the project, including results, challenges, and insights.\r\n\r\n---\r\n\r\n#### **Skills Gained**\r\n- Understanding of the Q-learning algorithm and its application to a simple problem.\r\n- Experience in balancing exploration and exploitation in reinforcement learning.\r\n- Hands-on practice in training and evaluating reinforcement learning agents.\r\n- Familiarity with reward structures, Q-tables, and the concept of an optimal policy in reinforcement learning.\r\n\r\n---','6b6ec940-2b7b-463f-9e45-ff201004f7d7','d1b582ff-39c9-4e46-8b93-8b5ad18eb27f','2025-01-11 07:46:12','2025-01-11 15:46:12'),('Interactive Sales Dashboard',' ','# **P#roject: Sales Performance Dashboard**\r\n\r\n### **Overview**\r\nIn this project, you will create an interactive sales performance dashboard using Tableau. The dashboard will allow stakeholders to analyze key metrics such as sales trends, customer demographics, and product performance. You will work with a sample dataset containing sales transactions, customer information, and product details to build a meaningful visualization.\r\n\r\n### **Objective**\r\n- Learn how to create interactive dashboards in Tableau.\r\n- Analyze sales data and identify key insights through data visualization.\r\n- Design an intuitive and interactive dashboard to support business decision-making.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection**\r\n- Download or use a publicly available dataset such as the **Superstore Sales dataset** or **Sales Performance dataset**.\r\n- The dataset should contain information about sales transactions, customer details, product categories, and dates.\r\n\r\n#### 2. **Data Preparation**\r\n- Import the dataset into Tableau and clean the data if necessary.\r\n  - Handle missing values, duplicates, and inconsistent entries.\r\n  - Format columns properly (e.g., dates, currencies).\r\n\r\n#### 3. **Exploratory Data Analysis (EDA)**\r\n- Conduct an initial analysis to understand the data and find relationships.\r\n  - Look for patterns such as high sales periods, popular product categories, or customer segments.\r\n  - Use basic Tableau visualizations like bar charts, line graphs, and pie charts to get a sense of the data.\r\n\r\n#### 4. **Creating Visualizations**\r\n- Create various visualizations that answer specific business questions, such as:\r\n  - Total sales by region or product category.\r\n  - Sales over time (yearly, quarterly, monthly trends).\r\n  - Profit vs. sales analysis for products.\r\n  - Customer demographics and their purchasing behavior.\r\n\r\n#### 5. **Building the Dashboard**\r\n- Combine the individual visualizations into a single interactive dashboard.\r\n  - Add filters to allow users to interact with the data, such as region, date, or product category filters.\r\n  - Use actions like highlighting and URL links for interactivity.\r\n  - Ensure that the dashboard is intuitive and easy to navigate.\r\n\r\n#### 6. **Advanced Features**\r\n- Add advanced Tableau features like:\r\n  - Calculated fields for new metrics (e.g., profit margins, growth percentages).\r\n  - Parameter controls to let users adjust views (e.g., adjusting date ranges or switching between different KPIs).\r\n  - Tooltips to display additional information when hovering over data points.\r\n\r\n#### 7. **Publishing and Sharing the Dashboard**\r\n- Publish the dashboard to Tableau Public or Tableau Server for easy sharing.\r\n  - Ensure its accessible to stakeholders or team members.\r\n  - Optionally, create a report or summary that accompanies the dashboard, explaining the key insights derived from it.\r\n\r\n#### 8. **Conclusion and Insights**\r\n- Summarize the key findings from the dashboard, such as:\r\n  - Best-performing products and regions.\r\n  - Seasonal sales trends.\r\n  - Customer segments contributing the most to sales.\r\n- Suggest actionable business decisions based on the dashboard insights.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Tableau Workbook**: The final Tableau workbook containing the interactive dashboard.\r\n2. **Documentation**: A report explaining the visualizations, insights, and design choices made in the dashboard.\r\n3. **Presentation**: A slide deck summarizing key findings and recommendations based on the dashboard analysis.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Mastery in using Tableau for data visualization and dashboard creation.\r\n- Understanding of how to design interactive dashboards to provide actionable insights.\r\n- Ability to work with large datasets and effectively communicate data-driven insights.\r\n- Knowledge of advanced Tableau features, including calculated fields, parameters, and filters.\r\n\r\n','76b7a4c4-161b-4ed5-81ec-27193a924685','e6c96694-6643-4d5a-8568-06e86188d73b','2025-01-11 07:26:48','2025-01-11 15:26:48'),('Flask-based ML Model Deployment on Heroku','  ','## **Project: Deploying a Machine Learning Model using Flask and Heroku**\r\n\r\n### **Overview**\r\nIn this project, you will deploy a trained machine learning model using **Flask** as a web framework and **Heroku** for cloud hosting. The goal is to create a web application that can take user input, process it through a machine learning model, and return predictions in real-time. This will provide hands-on experience in deploying machine learning models and creating a user-friendly interface for model interactions.\r\n\r\n### **Objective**\r\n- Train a machine learning model using a dataset.\r\n- Build a Flask web application to serve the model.\r\n- Deploy the web application on **Heroku** to make it accessible online.\r\n- Implement a user interface for input and display of results.\r\n- Understand the workflow of deploying a machine learning model into production.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Model Training**\r\n- Choose a machine learning problem (e.g., classification, regression) and dataset (e.g., Titanic dataset, house prices, or any other suitable dataset).\r\n- Preprocess the data (cleaning, feature engineering, etc.).\r\n- Train a machine learning model using popular libraries such as **scikit-learn** or **TensorFlow**.\r\n- Save the trained model using **joblib** or **pickle** so it can be easily loaded in the Flask app.\r\n\r\n#### 2. **Building the Flask API**\r\n- Create a **Flask** application that will serve the model.\r\n- Create an endpoint in Flask that accepts user input (e.g., a form with text boxes or a JSON API request).\r\n- Load the trained machine learning model in the Flask app.\r\n- Implement the model prediction logic inside the endpoint, where user input is processed and predictions are returned.\r\n- Ensure proper error handling and validation of user inputs.\r\n\r\n#### 3. **Building the User Interface**\r\n- Design a simple web interface using **HTML**, **CSS**, and **JavaScript** to collect input from the user.\r\n- Implement a form where the user can input data (e.g., age, sex, or other features depending on the chosen dataset).\r\n- Display the models prediction result on the webpage after submitting the form.\r\n\r\n#### 4. **Containerizing the Application with Docker**\r\n- Create a **Dockerfile** to containerize the Flask application.\r\n- Define all the necessary dependencies for your Flask app (Flask, scikit-learn, joblib, etc.) inside the **requirements.txt** and Dockerfile.\r\n- Build and test the Docker container locally to ensure that the app runs as expected in an isolated environment.\r\n\r\n#### 5. **Deploying on Heroku**\r\n- Create a Heroku account if you dont have one.\r\n- Initialize a Git repository for your project and commit your code.\r\n- Set up the necessary configuration files for deployment on Heroku (e.g., **Procfile**, **requirements.txt**).\r\n- Push the repository to Heroku to deploy the application online.\r\n- Test the live deployed app by visiting the provided Heroku URL and interacting with the model.\r\n\r\n#### 6. **Testing and Final Deployment**\r\n- Test the deployed model by sending sample inputs through the web interface and ensuring the predictions are correct.\r\n- Optimize the web interface for better user experience.\r\n- Ensure the model is deployed and accessible publicly.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Source Code**: Python code for the Flask application, model training script, and Dockerfile.\r\n2. **Web Interface**: Screenshots or a link to the deployed web interface showing the model in action.\r\n3. **Deployment Documentation**: Instructions on how to set up the app locally, deploy to Heroku, and test the model.\r\n4. **Presentation**: A summary of the project, including the trained model, how it was integrated with Flask, and how it was deployed on Heroku.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Experience in deploying machine learning models using **Flask** and **Heroku**.\r\n- Understanding of creating a web application that interacts with machine learning models.\r\n- Familiarity with model serialization (using joblib or pickle) for deployment.\r\n- Hands-on knowledge of containerization using **Docker** and deploying to the cloud.\r\n- Proficiency in building simple and user-friendly web interfaces for interacting with machine learning models.\r\n\r\n---','f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','f9d949ef-e6a9-4a10-b176-99cb8710b885','2025-01-11 07:53:49','2025-01-11 15:53:49'),('Pandas Data Insights',' ','## Project: Analyzing and Manipulating Real-World Data with Pandas\r\n\r\n### Overview\r\nIn this project, you will work with a real-world dataset and use **Pandas** to perform various data manipulation tasks, including cleaning, filtering, aggregating, and analyzing data. The goal is to practice using the powerful data manipulation capabilities of Pandas, one of the most popular libraries in Python for data science.\r\n\r\n---\r\n\r\n### Objectives\r\n- Perform data cleaning and preprocessing using Pandas.\r\n- Explore and summarize datasets using Pandas functions.\r\n- Apply various filtering, grouping, and aggregation techniques.\r\n- Handle missing values and outliers.\r\n- Create meaningful insights and visualizations from the data.\r\n\r\n---\r\n\r\n### Tasks\r\n\r\n#### 1. Dataset Selection\r\n- Choose a real-world dataset to work with. Some suggested datasets include:\r\n  - **Titanic Dataset**: Passenger data for the Titanic, including survival status, class, age, and other features.\r\n  - **Sales Data**: A dataset containing sales transactions, customer information, product categories, etc.\r\n  - **COVID-19 Data**: A dataset that contains COVID-19 cases, deaths, and other related statistics by region and date.\r\n  - **World Happiness Report**: Data on countries\' happiness scores, GDP per capita, social support, etc.\r\n\r\n#### 2. Data Import and Exploration\r\n- Import the dataset into a Pandas DataFrame.\r\n- Display basic statistics and structure of the data using:\r\n  - `df.info()`\r\n  - `df.describe()`\r\n  - `df.head()`\r\n  \r\n#### 3. Data Cleaning and Preprocessing\r\n- Handle missing values by:\r\n  - Filling missing values using `df.fillna()`.\r\n  - Dropping rows or columns with missing values using `df.dropna()`.\r\n- Convert columns to appropriate data types (e.g., dates, categories).\r\n- Remove duplicates from the dataset using `df.drop_duplicates()`.\r\n\r\n#### 4. Data Filtering and Subsetting\r\n- Filter data based on specific conditions (e.g., select rows where sales are greater than $1000).\r\n- Subset data using `loc[]` or `iloc[]` to extract specific rows and columns.\r\n- Use **boolean indexing** to filter the dataset.\r\n\r\n#### 5. Grouping and Aggregation\r\n- Group data by certain columns and perform aggregations such as:\r\n  - **SUM**, **MEAN**, **COUNT**, **MAX**, and **MIN** using `groupby()`.\r\n- Example: Find the average age of passengers grouped by survival status in the Titanic dataset.\r\n\r\n#### 6. Handling Outliers\r\n- Detect outliers in the data using methods like the **IQR** (Interquartile Range).\r\n- Remove or correct outliers where appropriate.\r\n\r\n#### 7. Merging and Joining Data\r\n- Merge multiple DataFrames using `merge()`.\r\n- Join DataFrames using different types of joins (e.g., `inner`, `left`, `right`).\r\n\r\n#### 8. Data Transformation\r\n- Apply functions to columns or rows using `apply()` and `map()`.\r\n- Create new columns based on existing ones (e.g., calculate profit from revenue and cost).\r\n- Transform categorical data into numerical data using **label encoding** or **one-hot encoding**.\r\n\r\n#### 9. Final Analysis and Insights\r\n- Create a summary report of your analysis, highlighting key insights.\r\n- Visualize your findings using **matplotlib** or **seaborn** (optional).\r\n\r\n---\r\n\r\n### Expected Deliverables\r\n1. A Jupyter notebook or Python script with all the steps and code used to analyze the dataset.\r\n2. A report or presentation summarizing key findings and insights.\r\n3. Optional: Visualizations of the analysis (graphs, charts, etc.).\r\n\r\n---\r\n\r\n### Tools and Libraries\r\n- **Python**: Pandas, NumPy, Matplotlib, Seaborn\r\n- **Environment**: Jupyter Notebook or any Python IDE\r\n\r\n---\r\n\r\n### Skills Gained\r\n- Proficiency in using Pandas for data manipulation and analysis.\r\n- Ability to clean and preprocess data for analysis.\r\n- Experience with grouping, aggregating, and transforming data.\r\n- Ability to work with real-world data and extract meaningful insights.\r\n','563d1df9-c558-496e-bcce-da2c8a2c681b','fcca8032-6157-417f-81e4-6cc829703bfb','2025-01-11 05:39:41','2025-01-11 13:39:41'),('entiment Analysis on Movie Reviews Using NLP',' ','## **Project: Sentiment Analysis on Movie Reviews**\r\n\r\n### **Overview**\r\nIn this project, you will analyze movie reviews using Natural Language Processing (NLP) techniques to determine the sentiment (positive, negative, or neutral) of each review. You will build a model that processes the text data, extracts features, and uses machine learning algorithms to predict the sentiment of unseen reviews.\r\n\r\n### **Objective**\r\n- Understand and apply NLP techniques for text preprocessing and feature extraction.\r\n- Use machine learning models to perform sentiment analysis.\r\n- Evaluate model performance using accuracy, precision, recall, and F1-score.\r\n\r\n---\r\n\r\n### **Tasks**\r\n\r\n#### 1. **Data Collection**\r\n- Collect a dataset of movie reviews. A popular dataset for this task is the **IMDB movie reviews** dataset, which contains thousands of labeled reviews.\r\n- Ensure the dataset includes reviews labeled as positive, negative, or neutral sentiment.\r\n\r\n#### 2. **Data Preprocessing**\r\n- **Text Cleaning**:\r\n  - Remove special characters, punctuation, and stop words.\r\n  - Convert text to lowercase and handle contractions (e.g., isn\'t to is not).\r\n  - Tokenize the text into words or phrases.\r\n  \r\n- **Text Vectorization**:\r\n  - Convert the text data into numerical form using techniques like **TF-IDF** (Term Frequency-Inverse Document Frequency) or **Word2Vec** to represent the text in a machine-readable format.\r\n  \r\n- **Handling Imbalanced Data**:\r\n  - If the dataset is imbalanced (i.e., one class dominates), use techniques like **SMOTE** or **undersampling/oversampling** to balance the classes.\r\n\r\n#### 3. **Feature Engineering**\r\n- Use techniques like **TF-IDF** or **Word2Vec** to represent the text data in vector form.\r\n- Explore other advanced techniques like **BERT embeddings** for richer, context-aware representations.\r\n\r\n#### 4. **Model Building**\r\n- **Logistic Regression**: Start with a simple logistic regression model for sentiment classification.\r\n- **Naive Bayes**: Implement the Naive Bayes classifier, which is commonly used for text classification tasks.\r\n- **Deep Learning**:\r\n  - Implement a **LSTM** (Long Short-Term Memory) model to capture the sequential nature of the text.\r\n  - Use a pre-trained model like **BERT** (Bidirectional Encoder Representations from Transformers) for transfer learning to improve model performance.\r\n\r\n#### 5. **Model Evaluation**\r\n- Evaluate the model performance using appropriate metrics such as:\r\n  - **Accuracy**\r\n  - **Precision**\r\n  - **Recall**\r\n  - **F1-score**\r\n\r\n- Visualize confusion matrices and classification reports to assess the model\'s strengths and weaknesses.\r\n\r\n#### 6. **Model Optimization (Optional)**\r\n- Experiment with hyperparameter tuning to optimize the models for better performance using techniques like **GridSearchCV** or **RandomizedSearchCV**.\r\n- Explore ensemble methods like **Random Forest** or **XGBoost** to combine multiple models.\r\n\r\n#### 7. **Deployment (Optional)**\r\n- Create an API using frameworks like **Flask** or **FastAPI** that allows users to input movie reviews and get sentiment predictions in real-time.\r\n\r\n#### 8. **Visualization**\r\n- Use visualization libraries like **Matplotlib** and **Seaborn** to show insights such as the most frequent words in positive and negative reviews.\r\n- Create word clouds to visualize the most common words in the dataset.\r\n\r\n---\r\n\r\n### **Deliverables**\r\n1. **Code**: Python scripts or Jupyter Notebooks containing all model training, evaluation, and sentiment analysis code.\r\n2. **Documentation**: A detailed report covering methodology, evaluation results, and insights from the data analysis.\r\n3. **Presentation**: A slide deck summarizing the project, methodology, key results, and findings.\r\n\r\n---\r\n\r\n### **Skills Gained**\r\n- Experience in text preprocessing, tokenization, and vectorization techniques.\r\n- Understanding of sentiment analysis and its applications in NLP.\r\n- Hands-on practice with machine learning algorithms like Naive Bayes, Logistic Regression, and deep learning models like LSTM and BERT.\r\n- Ability to evaluate and optimize machine learning models using standard metrics.\r\n- Exposure to advanced NLP techniques and state-of-the-art models like BERT for text classification.\r\n\r\n---\r\n\r\n','1899ca2c-be52-4920-a133-a46ad7933850','fda7c185-28d0-438d-a068-05d40d4988f6','2025-01-11 07:43:45','2025-01-11 15:43:45');
/*!40000 ALTER TABLE `project` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `quiz`
--

DROP TABLE IF EXISTS `quiz`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `quiz` (
  `title` varchar(45) NOT NULL,
  `question` text NOT NULL,
  `answer` text NOT NULL,
  `duration` int NOT NULL,
  `course_id` varchar(60) NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `course_id` (`course_id`),
  CONSTRAINT `quiz_ibfk_1` FOREIGN KEY (`course_id`) REFERENCES `course` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `quiz`
--

LOCK TABLES `quiz` WRITE;
/*!40000 ALTER TABLE `quiz` DISABLE KEYS */;
INSERT INTO `quiz` VALUES ('Question 1','Which of the following regression techniques helps to reduce overfitting by penalizing large coefficients in the model?','Lasso Regression',1,'160ee69a-9374-40d6-bd9e-9ec9e865da52','00f34e62-b38c-4ad8-994d-e41aa1715c1a','2025-01-11 07:33:30','2025-01-11 15:33:30'),('Question 2','What statistical method is most commonly used to predict whether a customer will churn (binary outcome)?','Logistic regression',1,'83e631af-5b05-4a16-bd42-a85bdb3b542e','026761c4-f068-4a87-92cb-bd69ac4a984f','2025-01-11 05:16:32','2025-01-11 13:16:32'),('Question 4','What is the main reason for using regularization techniques like Lasso and Ridge regression in machine learning?','To prevent overfitting by penalizing large coefficients',1,'2e4a681c-5ea5-4269-ad08-fa671500a5d1','032b83b7-ff53-4a00-b2ff-486f3a692d78','2025-01-11 05:07:30','2025-01-11 13:07:30'),('Question 2','What is the purpose of the groupby() function in Pandas?','To split data into groups based on a specific column and then perform an aggregation on each group.',2,'563d1df9-c558-496e-bcce-da2c8a2c681b','05cb4c04-4581-4aa5-baed-4f7ebe11b31d','2025-01-11 05:44:12','2025-01-11 13:44:12'),('Question 1','What Python library would you use to load a CSV file and perform data manipulation tasks like cleaning, filtering, and aggregating data?','Pandas',1,'f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','066bc28a-fc4c-4a1f-9133-526c14e938c9','2025-01-11 05:22:11','2025-01-11 13:22:11'),('Question 1','Which of the following AWS services is most commonly used for serverless data processing in a data pipeline?\r\n\r\n','Amazon Lambda',1,'28e0e1f6-2cd3-470e-8e05-19332a95bf21','0c1aceb6-f774-472e-a237-c77631fb4521','2025-01-11 07:51:58','2025-01-11 15:51:58'),('Question 3',' What is the rank of a matrix?','The number of linearly independent rows or columns',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','0e89070d-41b4-462b-ab0e-7613defb0c7e','2025-01-10 18:51:42','2025-01-11 02:51:42'),('Question 1','Which of the following features in Tableau allows users to filter data across multiple visualizations simultaneously on a dashboard?','Dashboard Filters',2,'76b7a4c4-161b-4ed5-81ec-27193a924685','156ee616-9c3d-4773-b2be-be71a08f221f','2025-01-11 07:28:10','2025-01-11 15:28:10'),('Question 1','Which of the following techniques is most suitable for handling time series data with both trend and seasonality?','Exponential Smoothing',1,'d75afbc3-5381-4d9c-9553-8a3a48021369','1cb38fb0-7085-420b-86fe-0907091daa66','2025-01-11 07:41:53','2025-01-11 15:41:53'),('Question 1','Which Pandas function would you use to merge two DataFrames?','df.merge()',1,'563d1df9-c558-496e-bcce-da2c8a2c681b','1e3efca0-48a3-439d-bc99-982a357c5f89','2025-01-11 05:45:16','2025-01-11 13:45:16'),('Question 1','Which of the following is NOT a key step when deploying a machine learning model using Flask and Heroku?','Publishing the model directly without using a web server',2,'f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','30e7dc37-d6e6-4676-9a48-753b981ed9eb','2025-01-11 07:54:34','2025-01-11 15:54:34'),('Question 3','Which of the following methods can help mitigate bias in data science models?','Regularly auditing algorithms for fairness and transparency',1,'af1709c2-9c15-4581-89bc-9acf66e06729','52cc279d-69b8-454e-81c0-7222ef392280','2025-01-11 07:06:44','2025-01-11 15:06:44'),('Question 1','Which of the following clustering algorithms is best suited for detecting clusters with varying shapes and identifying outliers in the data?','DBSCAN',1,'509fa487-d1ff-433c-b3d4-2665f2d495a7','53dad63d-aafc-4ef6-b000-7cf09c9f8287','2025-01-11 07:10:02','2025-01-11 15:10:02'),('Question 1','What is the primary purpose of using Convolutional Layers in a Convolutional Neural Network (CNN)?','To extract features such as edges, textures, and patterns from the input images',2,'09ffa3d0-3468-4cf9-8bf2-1acb59b10226','58fb606f-bbb0-499e-b632-50bdc927d6e8','2025-01-11 07:39:03','2025-01-11 15:39:03'),('Question 8','What is the role of the identity matrix in matrix operations?','It acts as a neutral element in matrix multiplication (i.e., multiplying any matrix by the identity matrix leaves the matrix unchanged',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','590280a5-7a53-4d08-a3b8-26434b767581','2025-01-10 18:59:01','2025-01-11 02:59:01'),('Question 1','In the context of customer churn prediction, what is the purpose of hypothesis testing?','To identify statistical relationships between features and churn',1,'83e631af-5b05-4a16-bd42-a85bdb3b542e','5d17d44d-557a-484c-acbd-ddffef22ec97','2025-01-11 05:15:40','2025-01-11 13:15:40'),('Question 2','Which R package is primarily used for data manipulation and visualization, and includes functions like ggplot() and filter()?','tidyverse',2,'e95b2987-b1fc-4731-80cc-49f535b522fc','67e21b13-db92-4030-929e-d7f5628fc017','2025-01-11 05:32:08','2025-01-11 13:32:08'),('Question 7',' Which of the following operations is a key application of linear algebra in machine learning?','Performing feature extraction using Principal Component Analysis (PCA)',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','6fa5d6fb-b048-4bef-8745-28c71ffc9e7e','2025-01-10 18:57:52','2025-01-11 02:57:52'),('Question 1','Which of the following is a common technique used during EDA to understand the distribution of a numerical variable?','Histogram',1,'47973087-99e9-44c2-b87e-fe185054256d','752b1707-9577-462e-837d-c2044a97a932','2025-01-11 07:19:01','2025-01-11 15:19:01'),('Question 1','Which of the following machine learning techniques is specifically designed to handle imbalanced datasets by adjusting the weights of different classes?','SMOTE (Synthetic Minority Over-sampling Technique)',1,'020578cb-0b83-465e-959d-400cecfb415e','7c870830-47dd-427b-87c9-9474cb9f91d0','2025-01-11 07:36:11','2025-01-11 15:36:11'),('Question 1','Which of the following is the correct function for creating a Spark DataFrame from a CSV file in PySpark?','spark.read.csv()',2,'5d35a77a-0981-45da-81b6-24aa87b34b09','7e226eb9-ca97-43d4-b14c-ba139d9f3c04','2025-01-11 07:56:51','2025-01-11 15:56:51'),('Question 10','What does the term \"orthogonal vectors\" mean?','Vectors that are perpendicular to each other',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','8ae03fa3-c7c7-4887-b3e0-abda52780622','2025-01-10 19:00:58','2025-01-11 03:00:58'),('Question 1','Which of the following data structures is most efficient for implementing a priority queue?','Binary Heap',1,'8df31585-b33f-40f8-a95a-b6f20a3435df','91f55617-3e4a-4935-9d8c-cc7332c9a4ba','2025-01-11 05:25:36','2025-01-11 13:25:36'),('Question 2','Which of the following operations is performed when multiplying a matrix by a scalar?','Scalar multiplication',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','924daf1c-d96d-4f5b-b75b-15b8278eb17b','2025-01-10 18:50:41','2025-01-11 02:50:41'),('Question 4','In the context of machine learning, why is it important to understand the concept of eigenvectors and eigenvalues?','They are used in dimensionality reduction techniques like PCA',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','a86af94e-a87e-41a2-9b2f-0e4a49bcc8e2','2025-01-10 18:52:50','2025-01-11 02:52:50'),('Question 1','Which of the following metrics is most appropriate for evaluating a classification model\'s performance on an imbalanced dataset?','Precision',2,'d25498d9-4c3c-4bf4-bf20-330c59578aa9','b80e2450-b531-4112-bdea-83054a3ce989','2025-01-11 07:22:51','2025-01-11 15:22:51'),('Question 5','Which of the following describes the matrix multiplication rule for two matrices A (of size m x n) and B (of size n x p)?','The result will be a matrix of size m x p',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','ba8f4b3c-33af-4998-8f22-8a528527b689','2025-01-10 18:53:50','2025-01-11 02:53:50'),('Question 6','What is Singular Value Decomposition (SVD) used for in data science?','Performing matrix factorization for dimensionality reduction',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','c79f5023-0180-4646-9ecf-35e4819dcba6','2025-01-10 18:56:57','2025-01-11 02:56:57'),('Question 1','1. Which Pandas function is used to handle missing values in a DataFrame?','df.fillna()',1,'563d1df9-c558-496e-bcce-da2c8a2c681b','cce41205-ba4d-4d1e-9cc4-65ba6b74d19d','2025-01-11 05:42:01','2025-01-11 13:42:01'),('Question 2','What is \"algorithmic bias\"?','When an algorithm systematically favors certain groups over others based on biased data',1,'af1709c2-9c15-4581-89bc-9acf66e06729','ce9b6bc5-8267-4e89-8955-e06dddc83bb3','2025-01-11 07:05:34','2025-01-11 15:05:34'),('Question 1','Which SQL clause is used to filter records after performing an aggregation in a GROUP BY query?','HAVING',1,'8f9a4075-56f3-4a24-bca0-084cb2c1df0a','cf5dddf0-5e2e-4211-b0bf-f48ef0f7268c','2025-01-11 05:36:28','2025-01-11 13:36:28'),('Question 1','What function in R is commonly used to load a dataset from a CSV file?','read.csv()',1,'e95b2987-b1fc-4731-80cc-49f535b522fc','d25242e9-dbde-4d65-bed7-256d4dba7caf','2025-01-11 05:31:04','2025-01-11 13:31:04'),('Question 1','Which of the following Matplotlib functions is used to create a line plot?','plt.plot()',2,'7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','dcb6084e-f432-44d6-932f-c31e9cb50ff3','2025-01-11 05:55:21','2025-01-11 13:55:21'),('Question 3','When using integrals in data analysis, which of the following is typically being computed?','The area under a curve, representing the cumulative value',1,'2e4a681c-5ea5-4269-ad08-fa671500a5d1','dcd98717-8f23-4303-ac04-80ccc5658f58','2025-01-11 05:06:36','2025-01-11 13:06:36'),('Question 9','In machine learning, how does matrix factorization help with recommendation systems?','By splitting user-item interaction matrices into lower-dimensional matrices to identify patterns',1,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','dd2e2057-9fe9-444f-87cf-2255a84f786a','2025-01-10 18:59:53','2025-01-11 02:59:53'),('Question 1','In Q-learning, which of the following is updated to learn the optimal policy?','Q-table',1,'6b6ec940-2b7b-463f-9e45-ff201004f7d7','de38e5e8-9bc3-4a7a-b054-db1bb58a71c4','2025-01-11 07:46:55','2025-01-11 15:46:55'),('Question 1','Which of the following is a common technique used for representing text data as numeric vectors for machine learning tasks?','TF-IDF',2,'1899ca2c-be52-4920-a133-a46ad7933850','e9e63bf8-558c-4a40-b166-6ea52d6da75d','2025-01-11 07:44:22','2025-01-11 15:44:22'),('Question 1','In the context of machine learning, what is the primary purpose of using gradient descent?','To minimize the loss function by updating model parameters',1,'2e4a681c-5ea5-4269-ad08-fa671500a5d1','ea96ea25-d825-48a1-b823-1213a002f1e2','2025-01-11 05:04:43','2025-01-11 13:04:43'),('Question 1','Which of the following is used to process large datasets in a distributed manner across a Hadoop cluster?','MapReduce',1,'8634ec7e-c67d-47d3-8eed-337754f21202','ebe549e5-4600-4c7b-8c23-27dfb205a5d9','2025-01-11 05:49:53','2025-01-11 13:49:53'),('Question 1','Which of the following is an ethical concern in data science?','Ensuring data privacy and protection',1,'af1709c2-9c15-4581-89bc-9acf66e06729','ecdc73ec-8484-4037-af4a-cb73b7d21b56','2025-01-11 07:04:47','2025-01-11 15:04:47'),('Question 1','Which of the following metrics is commonly used to evaluate the performance of a regression model?','Mean Squared Error (MSE)',1,'4012e606-0ada-41c4-923b-3e128463e27a','efe5bf2a-42bb-4d85-9b5a-23cbe6d1595e','2025-01-11 07:16:19','2025-01-11 15:16:19'),('Question 1','What is a vector?','A one-dimensional array of numbers',2,'d756bb07-e5c7-40d9-8b31-dc9a859672a5','f145621b-08ed-4d89-8e06-c8ee99885b3e','2025-01-10 18:47:55','2025-01-11 02:47:55'),('Question 2','Which of the following describes the role of partial derivatives in machine learning?','They measure how much the loss function changes with respect to each feature',1,'2e4a681c-5ea5-4269-ad08-fa671500a5d1','fbeaf118-19a7-402f-b21b-07319c40d904','2025-01-11 05:05:36','2025-01-11 13:05:36');
/*!40000 ALTER TABLE `quiz` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `resource`
--

DROP TABLE IF EXISTS `resource`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `resource` (
  `title` varchar(45) NOT NULL,
  `description` text NOT NULL,
  `course_id` varchar(60) NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `course_id` (`course_id`),
  CONSTRAINT `resource_ibfk_1` FOREIGN KEY (`course_id`) REFERENCES `course` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `resource`
--

LOCK TABLES `resource` WRITE;
/*!40000 ALTER TABLE `resource` DISABLE KEYS */;
INSERT INTO `resource` VALUES ('Intro to Machine Learning Algorithms','Learn the fundamentals of machine learning algorithms and their applications in data science.','d25498d9-4c3c-4bf4-bf20-330c59578aa9','04543bcd-a64b-4a05-91f1-8190507d267f','2025-01-10 18:27:56','2025-01-11 02:27:56'),('Intro to Big Data with Spark and Hadoop','Learn Big Data concepts focusing on Hadoop & Spark. Understand HDFS, MapReduce, and Apache Spark for data analysis.','8634ec7e-c67d-47d3-8eed-337754f21202','084b6433-90d1-4d43-adb3-7b73b2d3e8b8','2025-01-10 18:26:32','2025-01-11 02:26:32'),('Data Visualization with Python','Create stunning plots and charts using Matplotlib in Python.','7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','08845a59-a9c3-4c7d-8588-27c0c0ef43e3','2025-01-10 18:37:18','2025-01-11 02:37:18'),('Clustering for Data Science - Udemy','This Udemy course covers various clustering techniques, including K-means, DBSCAN, and Gaussian Mixture Models. Learn to apply these methods using Python and scikit-learn to group similar data points and understand clustering metrics. [Enroll on Udemy](https://www.udemy.com/course/clustering-for-data-science/).','509fa487-d1ff-433c-b3d4-2665f2d495a7','0969e9d9-d88d-4a2f-81e2-1d3ac7ddf751','2025-01-10 18:11:01','2025-01-11 02:11:01'),('Data Science and Statistical Modeling','Explore how to use statistical modeling for data science tasks.','160ee69a-9374-40d6-bd9e-9ec9e865da52','099f9506-aef2-4e7d-85ed-f13b45632c27','2025-01-10 18:35:15','2025-01-11 02:35:15'),('SQL Essential Training - LinkedIn Learning','This LinkedIn Learning course offers a comprehensive introduction to SQL, covering fundamental concepts like SELECT statements, JOINs, and subqueries. It also includes advanced topics such as data aggregation and grouping. Perfect for beginners or anyone looking to improve their SQL skills. [Enroll on LinkedIn Learning](https://www.linkedin.com/learning/sql-essential-training-3).','8f9a4075-56f3-4a24-bca0-084cb2c1df0a','0bc27051-47b0-4ed9-bc25-6f75c875cbdc','2025-01-10 18:08:23','2025-01-11 02:08:23'),('Intro to R for Data Analysis - Coursera','This Coursera course, offered by the University of California, teaches the basics of R for data analysis, covering key topics like data manipulation, visualization, and statistical modeling. You will learn how to use R for real-world data analysis, with practical assignments and projects. It includes hands-on exercises in RStudio and video tutorials on topics such as data cleaning, exploratory data analysis, and basic statistical concepts. [Enroll on Coursera](https://www.coursera.org/learn/r-programming). \nAdditionally, there are several great resources to complement this course: \n- [R for Data Science Book](https://r4ds.had.co.nz/)\n- [R Programming - YouTube Playlist](https://www.youtube.com/playlist?list=PL9H2mDA9FYt1kNqFWVwmVBoocX0QdA8dH)\n- [RStudio Cheat Sheets](https://posit.co/resources/cheatsheets/)','e95b2987-b1fc-4731-80cc-49f535b522fc','10d34ef6-740d-4bc6-8f43-c4a3e36410b3','2025-01-10 18:02:33','2025-01-11 02:02:33'),('Data Science on Cloud Platforms','Learn how to deploy data science projects using cloud platforms.','28e0e1f6-2cd3-470e-8e05-19332a95bf21','11983323-7be1-4c36-8131-e870cc1769fe','2025-01-10 18:30:07','2025-01-11 02:30:07'),('Data Analysis Using PySpark','Learn how to set up Google Colab for distributed data processing and apply different queries to your dataset to extract useful information.','5d35a77a-0981-45da-81b6-24aa87b34b09','18fc0269-bc8b-4689-aae9-6d9bab0712e1','2025-01-10 18:16:13','2025-01-11 02:16:13'),('ML Model Deployment - Databricks','This Databricks course introduces three primary machine learning deployment strategies and illustrates the implementation of each on Databricks. Topics include model serving, scaling, and monitoring. [Start Learning on Databricks](https://www.databricks.com/training/catalog/machine-learning-model-deployment-2398).','f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','190266e5-54dd-467b-8503-65301394b983','2025-01-10 18:12:44','2025-01-11 02:12:44'),('Deep Learning with TensorFlow and Keras','Master deep learning with TensorFlow and Keras.','09ffa3d0-3468-4cf9-8bf2-1acb59b10226','1920326d-1ee5-4cd4-a9d4-c78d79063aa8','2025-01-10 18:32:05','2025-01-11 02:32:05'),('Time Series Analysis in R - DataCamp','This DataCamp course provides a comprehensive introduction to time series analysis in R. It covers various time series models and forecasting techniques, including ARIMA and ETS. You will work with R packages like `forecast` to build and evaluate time series models. [Start Learning on DataCamp](https://www.datacamp.com/courses/time-series-analysis-in-r).','d75afbc3-5381-4d9c-9553-8a3a48021369','1a3343d2-7261-4f2e-8d28-874229876efc','2025-01-10 18:09:58','2025-01-11 02:09:58'),('Probability and Statistics - Coursera','This Coursera course, offered by Duke University, teaches the basics of probability and statistics with a focus on data science applications. Topics covered include descriptive statistics, probability distributions, and inferential statistics. [Enroll on Coursera](https://www.coursera.org/learn/probability-statistics-data-science).','f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','1b02fe97-02a2-4102-9282-c8260f2ee507','2025-01-10 17:45:45','2025-01-11 01:45:45'),('Pandas for Data Manipulation','Learn powerful techniques to manipulate data using Pandas.','563d1df9-c558-496e-bcce-da2c8a2c681b','1e91c7f8-845d-4756-aade-9f1129bfd34b','2025-01-10 18:38:14','2025-01-11 02:38:14'),('Unsupervised Learning - Coursera','This Coursera course, offered by the University of California, covers unsupervised learning techniques with a focus on clustering algorithms like K-means, hierarchical clustering, and DBSCAN. Learn how to apply clustering algorithms to real-world datasets, along with dimensionality reduction techniques like PCA. [Enroll on Coursera](https://www.coursera.org/learn/unsupervised-learning).','509fa487-d1ff-433c-b3d4-2665f2d495a7','1f655f59-690a-451c-9d2f-dfa04048fad4','2025-01-10 18:10:58','2025-01-11 02:10:58'),('SQL for Data Science - edX','This edX course offered by UC Berkeley is designed for individuals with no prior knowledge of SQL. It covers how to retrieve data from databases using SQL, focusing on data analysis and manipulation. The course includes exercises on using SQL for data wrangling, filtering, and aggregation. [Enroll on edX](https://www.edx.org/course/sql-for-data-science).','8f9a4075-56f3-4a24-bca0-084cb2c1df0a','21ed198c-e3c4-4aa3-aae9-15de747942e0','2025-01-10 18:08:22','2025-01-11 02:08:22'),('Data Exploration with Python and Pandas','A deep dive into data exploration using Python and Pandas.','47973087-99e9-44c2-b87e-fe185054256d','237a4c5b-a4cd-431c-a69f-e3f193c7307f','2025-01-10 18:36:17','2025-01-11 02:36:17'),('RL Basics: Foundations & Algorithms','Learn the foundations of reinforcement learning and how basic algorithms work.','6b6ec940-2b7b-463f-9e45-ff201004f7d7','27f755d1-2285-4828-bbab-684adb2d6848','2025-01-10 18:28:59','2025-01-11 02:28:59'),('Understanding ML Algorithms','This course dives into the math and statistics behind machine learning algorithms and how they work.','d25498d9-4c3c-4bf4-bf20-330c59578aa9','2bd9b7ba-8a68-410a-8404-883265dfc7ae','2025-01-10 18:27:58','2025-01-11 02:27:58'),('Deploying ML Models - Coursera','This Coursera course, offered by the University of California, teaches best practices for deploying machine learning models and monitoring their performance. Topics include project structure for interactive Python data applications, Python web server frameworks like Flask and Django, and deployment scripts for serializing models and creating APIs. [Enroll on Coursera](https://www.coursera.org/learn/deploying-machine-learning-models).','f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','2bdac376-c4e6-4af7-9b46-33ea5a5b1493','2025-01-10 18:12:40','2025-01-11 02:12:40'),('Cloud Basics for Data Science Projects','Master the essentials of using cloud computing in data science workflows.','28e0e1f6-2cd3-470e-8e05-19332a95bf21','2e5ee8a7-3d2e-4b4d-be0e-b52140225a1f','2025-01-10 18:30:08','2025-01-11 02:30:08'),('R Programming for Beginners - YouTube','This free YouTube playlist offers a comprehensive introduction to R programming for beginners, covering essential topics such as working with RStudio, data manipulation, and basic data visualizations. The instructor provides a detailed walkthrough of each concept, making it easier for learners to grasp. Start with these videos: \n- [R Programming Tutorials - YouTube](https://www.youtube.com/playlist?list=PLGLfVvz_LVvSPjvmIjlRrZnD0DQen9-9y)\n- [R Programming for Data Science](https://www.youtube.com/watch?v=7cP1lDAIkmk)','e95b2987-b1fc-4731-80cc-49f535b522fc','2f6b0d26-6bc3-4a38-b46b-31db5644c6ca','2025-01-10 18:02:37','2025-01-11 02:02:37'),('Hands-on ML Foundations','Explore machine learning concepts through practical exercises and examples, perfect for beginners.','d25498d9-4c3c-4bf4-bf20-330c59578aa9','3352c171-7294-4cc8-a7d6-228afc72c8f7','2025-01-10 18:28:00','2025-01-11 02:28:00'),('Introduction to Big Data with PySpark','An introduction to the underlying concepts behind big data with a practical and hands-on approach using PySpark.','5d35a77a-0981-45da-81b6-24aa87b34b09','353ccc64-902f-4155-8200-1190d856c309','2025-01-10 18:16:14','2025-01-11 02:16:14'),('Machine Learning Foundations','An introductory course covering key machine learning algorithms and how they are applied to real-world problems.','d25498d9-4c3c-4bf4-bf20-330c59578aa9','3564e1ff-f52c-4e49-97c0-0fb393c6bb20','2025-01-10 18:27:57','2025-01-11 02:27:57'),('Big Data & Spark Full Course','Explore Hadoop and Spark technologies in this full course designed for data analysis and big data solutions.','8634ec7e-c67d-47d3-8eed-337754f21202','38edb0f5-e0b4-43c4-a998-3f28a9bfffdf','2025-01-10 18:26:36','2025-01-11 02:26:36'),('Exploring Cloud Computing for Data Science','Gain a strong understanding of how cloud computing powers data science applications.','28e0e1f6-2cd3-470e-8e05-19332a95bf21','3b30c8b7-28bc-49f6-b1fa-23c8d79fc4ab','2025-01-10 18:30:09','2025-01-11 02:30:09'),('Tableau Design and Data Visualization','Learn how to evaluate the effectiveness of a data visualization and build interactive and engaging Tableau dashboards.','76b7a4c4-161b-4ed5-81ec-27193a924685','3b8a1dd5-a3b9-4cb6-a5cc-69de5100a718','2025-01-10 18:22:00','2025-01-11 02:22:00'),('Matrix Operations in Python - DataCamp','DataCamp offers a course on matrix operations in Python, teaching efficient matrix manipulations with *NumPy*. The course covers matrix **addition**, **multiplication**, **determinants**, and **inverses**, making it ideal for applying linear algebra in *data science* and *machine learning*. [Take the Course on DataCamp](https://www.datacamp.com/courses/matrix-operations-in-python).','d756bb07-e5c7-40d9-8b31-dc9a859672a5','3e0edbb4-a65b-4a5c-8fa3-5bb4c0d12369','2025-01-10 16:58:08','2025-01-11 00:58:08'),('Time Series Analysis - Coursera','This Coursera course, offered by the University of Colorado, focuses on the techniques and tools used in time series analysis and forecasting. You will learn how to handle temporal data, perform time series decomposition, and use models like ARIMA and exponential smoothing. The course also covers forecasting methods and applications. [Enroll on Coursera](https://www.coursera.org/learn/time-series-analysis).','d75afbc3-5381-4d9c-9553-8a3a48021369','3e45dd9f-5763-4c17-97b4-d2984cc0e712','2025-01-10 18:09:55','2025-01-11 02:09:55'),('Learn R for Data Science - DataCamp','Course DataCamp offers a step-by-step approach to learning R programming for data science. You will get to grips with R syntax, functions, data structures like vectors, matrices, and data frames, and data wrangling techniques. The course also covers data visualization and regression modeling. DataCamp includes interactive exercises and quizzes to test your knowledge. Additional resources include: \n- [DataCamp: R for Data Science](https://www.datacamp.com/courses/intro-to-r)\n- [R Programming Tutorials on YouTube](https://www.youtube.com/watch?v=LpUvcYk49sY)\n- [RStudio Community](https://community.rstudio.com/)','e95b2987-b1fc-4731-80cc-49f535b522fc','3ed6c991-6374-4316-a6d0-e91e9a7555e0','2025-01-10 18:02:35','2025-01-11 02:02:35'),('Statistical Models in Data Science','A hands-on approach to statistical models in data science.','160ee69a-9374-40d6-bd9e-9ec9e865da52','4221b902-2067-4801-9677-0c256b2fda1a','2025-01-10 18:35:17','2025-01-11 02:35:17'),('Data Science Ethics - Coursera','This course, offered by the University of Michigan, explores the ethical considerations in data science, including privacy, data ownership, and algorithmic bias. [Enroll on Coursera](https://www.coursera.org/learn/data-science-ethics).','af1709c2-9c15-4581-89bc-9acf66e06729','49532356-1116-4e2d-ab57-982a1274b1c9','2025-01-10 18:14:00','2025-01-11 02:14:00'),('Multivariable Calculus - Khan Academy','Khan Academy offers a free, beginner-friendly course on multivariable calculus, covering topics such as **partial derivatives**, **multiple integrals**, and **vector fields**. These concepts are crucial for machine learning, particularly for optimization in high-dimensional spaces. The course includes interactive videos and exercises to help learners master these essential concepts. [Visit Khan Academy](https://www.khanacademy.org/math/multivariable-calculus).','2e4a681c-5ea5-4269-ad08-fa671500a5d1','49f931f5-4f56-4a95-a210-db45ac1b0706','2025-01-10 17:19:43','2025-01-11 01:19:43'),('Intro to Time Series - edX','This edX course by Microsoft provides an introduction to time series analysis, focusing on data patterns, forecasting, and methods such as moving averages and exponential smoothing. You will apply time series models to real-world datasets. [Enroll on edX](https://www.edx.org/course/intro-to-time-series-analysis).','d75afbc3-5381-4d9c-9553-8a3a48021369','4db906f8-9e7c-48db-b831-eccb21ce6908','2025-01-10 18:09:57','2025-01-11 02:09:57'),('Big Data, Hadoop, and Spark Basics','Offered by IBM, this course covers Hadoop ecosystem, PySpark, and Spark SQL for analyzing Big Data.','8634ec7e-c67d-47d3-8eed-337754f21202','4e5199e5-3ab2-4030-8ba0-88a3ea708396','2025-01-10 18:26:33','2025-01-11 02:26:33'),('Visualizing Data with Matplotlib','Learn how to create powerful visualizations using Matplotlib.','7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','4f429bc4-d16a-44a9-ad92-c7273e710659','2025-01-10 18:37:17','2025-01-11 02:37:17'),('Dashboards in Tableau','Learn to apply dashboard-composition best practices, add interactive elements, and use dashboard actions to make your dashboard interactive.','76b7a4c4-161b-4ed5-81ec-27193a924685','528d783d-e33c-4abe-b295-f65ff7ca6426','2025-01-10 18:21:59','2025-01-11 02:21:59'),('Linear Algebra - Khan Academy','Khan Academy offers a beginner-friendly introduction to the fundamentals of *linear algebra*, covering concepts like **vectors**, **matrices**, **determinants**, and **eigenvalues**. The course includes video tutorials, interactive exercises, and real-world examples to illustrate how linear algebra is used in *data science*, *machine learning*, and *computer graphics*. [Visit Khan Academy](https://www.khanacademy.org/math/linear-algebra).','d756bb07-e5c7-40d9-8b31-dc9a859672a5','52ddd90c-9bb3-4199-934f-5793126ea3a0','2025-01-10 16:57:59','2025-01-11 00:57:59'),('Model Deployment at Scale - NVIDIA','This course teaches how to scale machine learning models for production environments using NVIDIA Triton Inference Server. Topics include deploying models for inference at production scale and optimizing performance. [Enroll on NVIDIA](https://courses.nvidia.com/courses/course-v1%3ADLI%2BS-FX-03%2BV1/).','f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','5a02b93d-6c23-4894-8243-2ee965bc51fc','2025-01-10 18:12:43','2025-01-11 02:12:43'),('Time Series Forecasting - Udemy','This Udemy course teaches how to use machine learning models for time series forecasting. Topics covered include ARIMA, SARIMA, and Prophet models. The course also includes hands-on examples of time series forecasting using Python libraries like pandas, numpy, and statsmodels. [Enroll on Udemy](https://www.udemy.com/course/time-series-forecasting/).','d75afbc3-5381-4d9c-9553-8a3a48021369','5b9af7d1-13d7-487c-a956-c81c2cf6b2d9','2025-01-10 18:09:56','2025-01-11 02:09:56'),('Linear Regression for Machine Learning','Learn to apply linear regression to real-world problems.','4012e606-0ada-41c4-923b-3e128463e27a','5c5a996a-8d39-4ca2-ae00-e7bdaa2123e3','2025-01-10 18:34:12','2025-01-11 02:34:12'),('Supervised Learning with Regression','Learn the basics of supervised learning with regression models.','4012e606-0ada-41c4-923b-3e128463e27a','60de057a-4610-4f6f-9c97-eee198f9345a','2025-01-10 18:34:09','2025-01-11 02:34:09'),('Mastering Advanced Machine Learning Models','Learn how to implement and optimize advanced models.','020578cb-0b83-465e-959d-400cecfb415e','62e13cd7-0eef-4207-9efa-d90bd507a182','2025-01-10 18:33:08','2025-01-11 02:33:08'),('Intro to R Programming - Codecademy','Codecademy free course on R programming provides a beginner-friendly approach to learning how to analyze data using R. You will explore key concepts like variables, data structures, functions, and how to work with datasets. Along with coding challenges, you can also explore video tutorials. Extra learning resources are available for in-depth knowledge. Visit: \n- [Codecademy R Course](https://www.codecademy.com/learn/learn-r)\n- [RStudio: Introduction to R](https://www.rstudio.com/resources/training/)\n- [R for Beginners - YouTube](https://www.youtube.com/watch?v=kPti_5dA5J4)','e95b2987-b1fc-4731-80cc-49f535b522fc','676a331a-f49b-4b89-8e18-861164c2ffdd','2025-01-10 18:02:36','2025-01-11 02:02:36'),('Matplotlib for Data Visualization','Master Matplotlib to visualize and present data in Python.','7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','681da738-d5cc-4d07-a2ea-c4f526aee989','2025-01-10 18:37:16','2025-01-11 02:37:16'),('Calculus for ML - Machine Learning Mastery','In this tutorial, *Jason Brownlee* from *Machine Learning Mastery* provides a detailed, practical introduction to how calculus is used in machine learning. Key topics include **gradient descent**, **optimization**, and how **partial derivatives** are used in training machine learning models. The article includes step-by-step examples and code snippets to help learners understand how to apply these mathematical concepts to real-world machine learning problems. [Read More on Machine Learning Mastery](https://machinelearningmastery.com/calculus-for-machine-learning/).','2e4a681c-5ea5-4269-ad08-fa671500a5d1','6a0af241-9a87-4d8e-a148-fe4f8ae3bf6c','2025-01-10 17:19:31','2025-01-11 01:19:31'),('Data Structures & Algorithms - Coursera','This Coursera course from Princeton University covers foundational algorithms and data structures, including linked lists, binary trees, and dynamic programming. [Enroll on Coursera](https://www.coursera.org/learn/data-structures-and-algorithms).','8df31585-b33f-40f8-a95a-b6f20a3435df','6b14ed48-e5a7-42b5-9abb-bb9ed43a2ef8','2025-01-10 17:50:54','2025-01-11 01:50:54'),('Data Wrangling with Pandas','Master the art of cleaning and transforming data with Pandas.','563d1df9-c558-496e-bcce-da2c8a2c681b','6ba848cc-b258-4e64-97f2-9ee7c7703e14','2025-01-10 18:38:15','2025-01-11 02:38:15'),('Introduction to Natural Language Processing','Learn the fundamentals of NLP and its applications.','1899ca2c-be52-4920-a133-a46ad7933850','6efd321f-51c3-4fe7-88c5-1df83017a637','2025-01-10 18:31:07','2025-01-11 02:31:07'),('Data Science Ethics - edX','Offered by the University of Michigan, this course teaches how to think through the ethics surrounding privacy, data sharing, and algorithmic decision-making. [Enroll on edX](https://www.edx.org/learn/business-ethics/the-university-of-michigan-data-science-ethics).','af1709c2-9c15-4581-89bc-9acf66e06729','72399084-bf33-44f5-ac33-e52f86148161','2025-01-10 18:14:02','2025-01-11 02:14:02'),('Interactive Dashboards with Tableau','This course teaches how to create a variety of fully interactive and actionable Tableau dashboards that will inform and impress your audience.','76b7a4c4-161b-4ed5-81ec-27193a924685','74f7d0f5-fb62-4022-a737-9cf14e8976a0','2025-01-10 18:21:57','2025-01-11 02:21:57'),('Advanced Deep Learning with TensorFlow','Explore advanced deep learning models and techniques.','09ffa3d0-3468-4cf9-8bf2-1acb59b10226','76f420eb-141b-4ba8-882e-37d1e7b9320e','2025-01-10 18:32:06','2025-01-11 02:32:06'),('Fundamentals of Machine Learning','A comprehensive introduction to machine learning techniques and algorithms, including supervised learning.','d25498d9-4c3c-4bf4-bf20-330c59578aa9','774d58f7-b8d3-4169-85ad-06cb320f29f6','2025-01-10 18:27:59','2025-01-11 02:27:59'),('Data Structures - Udacity','Udacity course focuses on core data structures such as stacks, queues, trees, and graphs, and their applications in solving problems efficiently. [Enroll on Udacity](https://www.udacity.com/course/data-structures-and-algorithms-in-python--ud513).','8df31585-b33f-40f8-a95a-b6f20a3435df','7aab8325-e726-4e42-83c3-4d8d5725a06c','2025-01-10 17:51:03','2025-01-11 01:51:03'),('Ethics of Data Science - NYU Steinhardt','This course builds ethical skills for collecting, storing, sharing, and analyzing data derived from human subjects, covering informed consent, discrimination, and privacy. [Learn more at NYU Steinhardt](https://steinhardt.nyu.edu/courses/ethics-data-science).','af1709c2-9c15-4581-89bc-9acf66e06729','7b6fd89b-4ed9-482e-a3f3-49dd2dcd75dc','2025-01-10 18:14:01','2025-01-11 02:14:01'),('Multivariable Calculus for ML - Udacity','Udacity offers a specialized course in multivariable calculus, focusing on concepts such as **partial derivatives**, **gradient vectors**, and **optimization** techniques that are used in machine learning. This course is ideal for students who already have a basic understanding of single-variable calculus and are looking to deepen their knowledge to apply it to more complex machine learning models. The course includes interactive quizzes, projects, and coding assignments to reinforce learning. [Check out Udacity](https://www.udacity.com/course/multivariable-calculus-for-machine-learning--ud185).','2e4a681c-5ea5-4269-ad08-fa671500a5d1','7c72d6e4-e9d9-4511-9c3d-907675ff0104','2025-01-10 17:19:37','2025-01-11 01:19:37'),('Probabilistic Graphical Models - Coursera','This Coursera course from Stanford University delves into probabilistic graphical models, which combine probability theory with machine learning. It explores how to use these models for real-world data analysis. [Enroll on Coursera](https://www.coursera.org/learn/probabilistic-graphical-models).','f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','7e18911a-2c2a-4c77-801f-6cf4366cf4f1','2025-01-10 17:45:51','2025-01-11 01:45:51'),('Reinforcement Learning: Intro to RL','Learn the basics of reinforcement learning, including key algorithms and problem-solving approaches.','6b6ec940-2b7b-463f-9e45-ff201004f7d7','7fbee778-22a9-482f-b194-8f1f4a001357','2025-01-10 18:29:01','2025-01-11 02:29:01'),('Mastering Exploratory Data Analysis','Master EDA techniques for data analysis and insights.','47973087-99e9-44c2-b87e-fe185054256d','8b018593-8025-43b0-be4a-27a33ea210dd','2025-01-10 18:36:18','2025-01-11 02:36:18'),('Introduction to Deep Learning with TensorFlow','Learn the basics of deep learning using TensorFlow.','09ffa3d0-3468-4cf9-8bf2-1acb59b10226','8b0fa0fd-0578-4afe-bea6-e315e38d1f2b','2025-01-10 18:32:04','2025-01-11 02:32:04'),('Calculus: Single Variable - Coursera','This *Coursera* course from the *University of Pennsylvania* covers the basics of calculus, including **limits**, **derivatives**, and **applications**. The course is part of a larger sequence that introduces calculus concepts essential for machine learning, with a focus on understanding the mathematical foundation of algorithms. The course also includes practical examples, exercises, and quizzes to ensure a solid understanding. [Enroll on Coursera](https://www.coursera.org/learn/calculus-1).','2e4a681c-5ea5-4269-ad08-fa671500a5d1','8d0ed84b-ef7e-4198-af11-3c63a67f873c','2025-01-10 17:19:34','2025-01-11 01:19:34'),('Mastering Big Data Analytics with PySpark','This course teaches how to achieve scalable, high-throughput, and fault-tolerant processing of data streams using Spark Streaming.','5d35a77a-0981-45da-81b6-24aa87b34b09','9387bf35-5684-4dcc-bba3-0f9c3054dcbf','2025-01-10 18:16:11','2025-01-11 02:16:11'),('Data Visualization with Matplotlib','Learn how to create beautiful visualizations using Matplotlib.','7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','9573d48b-de99-45cc-a53a-d76280453523','2025-01-10 18:37:15','2025-01-11 02:37:15'),('Probability & Stats - Khan Academy','Khan Academy provides free, accessible tutorials and exercises on probability and statistics, covering topics such as random variables, probability theory, and statistical inference. [Explore on Khan Academy](https://www.khanacademy.org/math/statistics-probability).','f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','96e7089e-fcd7-43c0-be04-c59eda63521d','2025-01-10 17:45:48','2025-01-11 01:45:48'),('Introduction to Cloud Computing','Learn the basics of cloud computing and its applications in data science.','28e0e1f6-2cd3-470e-8e05-19332a95bf21','97c45f5c-5a01-4149-ab43-8b8712d8a6d8','2025-01-10 18:30:05','2025-01-11 02:30:05'),('Statistical Modeling in Data Science','Learn how to apply statistical models to data science problems.','160ee69a-9374-40d6-bd9e-9ec9e865da52','98893b4d-7786-4b74-b4e2-7ca150ab7bf8','2025-01-10 18:35:13','2025-01-11 02:35:13'),('MLOps Deployment - DataCamp','This DataCamp course explores the modern MLOps framework, focusing on the lifecycle and deployment of machine learning models. Learn to write ML code that minimizes technical debt, deploy models, monitor performance metrics, and maintain models in production. [Start Learning on DataCamp](https://www.datacamp.com/courses/mlops-deployment-and-life-cycling).','f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','99bee6e4-da58-465d-96b0-96355923b5e0','2025-01-10 18:12:42','2025-01-11 02:12:42'),('Data Manipulation with Pandas in Python','Learn how to manipulate data in Python using Pandas.','563d1df9-c558-496e-bcce-da2c8a2c681b','9b395209-7986-4a7d-a70f-68d58230f50e','2025-01-10 18:38:16','2025-01-11 02:38:16'),('Mastering RL: The Basics','A deep dive into the core principles of reinforcement learning and how to apply them.','6b6ec940-2b7b-463f-9e45-ff201004f7d7','9c0e1d05-c6d6-4c8a-b3c4-a3cac73f65b5','2025-01-10 18:29:02','2025-01-11 02:29:02'),('SQL for Data Science - Coursera','This Coursera course by the University of California teaches SQL for data science. You will learn how to work with databases and perform SQL queries to analyze real-world data. It covers basic and advanced SQL topics including data manipulation, filtering, and aggregation. The course includes hands-on practice using SQLite and practical assignments. [Enroll on Coursera](https://www.coursera.org/learn/sql-for-data-science).','8f9a4075-56f3-4a24-bca0-084cb2c1df0a','9ddad9ab-ce9b-41db-b635-7bfc100534b1','2025-01-10 18:08:21','2025-01-11 02:08:21'),('Math for ML: Multivariable Calc - Coursera','This *Coursera* course from *Imperial College London* focuses on multivariable calculus, which is essential for understanding machine learning algorithms. The course covers key topics such as **partial derivatives**, **gradient descent**, and **multivariable optimization**. Real-world examples and Python coding exercises are included to help learners apply the concepts in machine learning. [Enroll on Coursera](https://www.coursera.org/learn/mathematics-machine-learning-multivariable-calculus).','2e4a681c-5ea5-4269-ad08-fa671500a5d1','9ed6086a-677c-4347-b035-3775fa9138fd','2025-01-10 17:19:40','2025-01-11 01:19:40'),('Clustering and Association - edX','This edX course by UC San Diego introduces unsupervised learning with a focus on clustering and association techniques. It covers methods like K-means, DBSCAN, and hierarchical clustering, as well as applications of these techniques in data mining. [Enroll on edX](https://www.edx.org/course/clustering-and-association).','509fa487-d1ff-433c-b3d4-2665f2d495a7','9fa7c861-a0a0-4560-ad5c-e86bc451dd15','2025-01-10 18:10:59','2025-01-11 02:10:59'),('Exploratory Data Analysis in Python','Learn how to explore and visualize data using Python.','47973087-99e9-44c2-b87e-fe185054256d','a4244c4c-360b-4023-ae82-93c1f4545de8','2025-01-10 18:36:14','2025-01-11 02:36:14'),('Statistics for Data Science - edX','This *edX* course by *UC San Diego* covers the fundamentals of statistics necessary for data science, including **probability theory**, **descriptive statistics**, **inferential statistics**, and **data visualization**. It provides a comprehensive understanding of how statistical methods are applied in analyzing data and making decisions. [Enroll on edX](https://www.edx.org/course/statistics-for-data-science).','83e631af-5b05-4a16-bd42-a85bdb3b542e','a5a29a4d-2e93-4a39-82b2-2e9b0959be0a','2025-01-10 17:25:03','2025-01-11 01:25:03'),('Introduction to Statistical Modeling','An introduction to statistical modeling techniques in data science.','160ee69a-9374-40d6-bd9e-9ec9e865da52','a9ca0fb4-4c9d-4170-83e2-83a60e44695e','2025-01-10 18:35:14','2025-01-11 02:35:14'),('SQL for Data Science - Udemy','This Udemy course is designed to teach SQL skills for data science. You will learn how to perform data analysis using SQL, covering topics like filtering, aggregating, and joining data. The course also includes exercises using SQL in real-world data analysis scenarios. [Enroll on Udemy](https://www.udemy.com/course/sql-for-data-science/).','8f9a4075-56f3-4a24-bca0-084cb2c1df0a','aaf4fd7f-507d-4f49-b770-f2f373904ee1','2025-01-10 18:08:25','2025-01-11 02:08:25'),('ML Model Deployment - Udemy','This Udemy course provides comprehensive coverage of deploying machine learning models, including over 100 lectures with hands-on Python code examples. Topics include deployment strategies, creating REST APIs, and scaling models for production environments. [Enroll on Udemy](https://www.udemy.com/course/deployment-of-machine-learning-models/).','f2de27e0-f8db-46d4-b183-71bf3e2e0c3a','adcb89ed-7194-472f-9116-5e1b769bb999','2025-01-10 18:12:41','2025-01-11 02:12:41'),('Exploratory Data Analysis in R','Learn how to perform EDA using R programming language.','47973087-99e9-44c2-b87e-fe185054256d','b14836b4-f01b-4723-8aa2-d30b1fc05747','2025-01-10 18:36:16','2025-01-11 02:36:16'),('Mastering Data Manipulation with Pandas','Become an expert in data manipulation with Pandas.','563d1df9-c558-496e-bcce-da2c8a2c681b','b16969e1-d983-4cee-b781-114bc733d1df','2025-01-10 18:38:13','2025-01-11 02:38:13'),('Fundamentals of Reinforcement Learning','A beginner-friendly course that introduces the key concepts and techniques of reinforcement learning.','6b6ec940-2b7b-463f-9e45-ff201004f7d7','b2c1e851-a1e4-4521-a6a6-5b47282034e6','2025-01-10 18:29:00','2025-01-11 02:29:00'),('Probability & Statistics - Coursera','This *Coursera* course from *Stanford University* offers an in-depth introduction to probability theory and statistics, focusing on real-world applications in data science and machine learning. Topics include **probability distributions**, **hypothesis testing**, **regression analysis**, and **Bayesian statistics**. The course also includes hands-on coding assignments using Python. [Enroll on Coursera](https://www.coursera.org/learn/probability-statistics).','83e631af-5b05-4a16-bd42-a85bdb3b542e','b7c32d19-0fd4-433f-9fe5-13956091ba9b','2025-01-10 17:25:00','2025-01-11 01:25:00'),('Clustering in Python - DataCamp','In this DataCamp course, learn to apply clustering algorithms like K-means, hierarchical clustering, and DBSCAN in Python using the scikit-learn library. You will also explore advanced clustering techniques and how to evaluate clustering results. [Start Learning on DataCamp](https://www.datacamp.com/courses/unsupervised-learning-in-python).','509fa487-d1ff-433c-b3d4-2665f2d495a7','b933b91b-4d88-4b09-81f7-3952abe23205','2025-01-10 18:11:00','2025-01-11 02:11:00'),('Introduction to Reinforcement Learning','This course covers the basic concepts of reinforcement learning and its applications.','6b6ec940-2b7b-463f-9e45-ff201004f7d7','ba5c2750-0f1b-43d2-b578-ff5e16f99254','2025-01-10 18:28:58','2025-01-11 02:28:58'),('Big Data with PySpark','Advance your data skills by mastering Apache Spark. Using the Spark Python API, PySpark, you will leverage parallel computation with large datasets.','5d35a77a-0981-45da-81b6-24aa87b34b09','baa0027e-8bd1-4a8e-8333-6110c0600883','2025-01-10 18:16:12','2025-01-11 02:16:12'),('Advanced Statistical Modeling Techniques','Learn advanced statistical modeling techniques for data analysis.','160ee69a-9374-40d6-bd9e-9ec9e865da52','bae25030-770c-4e51-bbd5-d574fe587f2b','2025-01-10 18:35:16','2025-01-11 02:35:16'),('Probability and Statistics - Khan Academy','Khan Academy offers a free, beginner-friendly course on **probability** and **statistics**, covering topics like **descriptive statistics**, **probability distributions**, and **regression**. The course includes interactive exercises and videos to help learners understand how statistical concepts are applied in various fields. [Visit Khan Academy](https://www.khanacademy.org/math/statistics-probability).','83e631af-5b05-4a16-bd42-a85bdb3b542e','bd96da99-6e4e-4c92-a989-27c874ef773c','2025-01-10 17:25:09','2025-01-11 01:25:09'),('Dashboards and Storytelling with Tableau','Learn how to create dashboards that help you identify the story within your data, and use Storypoints to create a powerful narrative.','76b7a4c4-161b-4ed5-81ec-27193a924685','be956927-d821-4b9e-85ac-fe4d4d3746a4','2025-01-10 18:21:58','2025-01-11 02:21:58'),('Data Structures and Algorithms - edX','This edX course from UC San Diego and UC Irvine provides an in-depth introduction to data structures and algorithms, covering topics like sorting, searching, and graph algorithms. [Enroll on edX](https://www.edx.org/course/data-structures-and-algorithms).','8df31585-b33f-40f8-a95a-b6f20a3435df','bf3e672e-3204-425f-9d4c-1ea337ed607c','2025-01-10 17:50:51','2025-01-11 01:50:51'),('Advanced Machine Learning with Python','Master advanced machine learning techniques using Python.','020578cb-0b83-465e-959d-400cecfb415e','bfd7d12b-c0bb-4a0b-81b3-efdcfdcb40e8','2025-01-10 18:33:05','2025-01-11 02:33:05'),('Learn SQL - Codecademy','Codecademy offers an interactive course on SQL that covers topics such as SELECT statements, WHERE clauses, and JOIN operations. The course includes interactive coding challenges and projects to help you practice SQL skills in real-world scenarios. [Start Learning on Codecademy](https://www.codecademy.com/learn/learn-sql).','8f9a4075-56f3-4a24-bca0-084cb2c1df0a','c09a8425-6e15-4e11-b805-2469d2141c12','2025-01-10 18:08:24','2025-01-11 02:08:24'),('Data Science: Probability - edX','This edX course from Harvard University covers fundamental probability concepts and their applications in data science, including random variables, conditional probability, and Bayes Theorem. [Enroll on edX](https://www.edx.org/course/data-science-probability).','f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','c0ec4717-875d-435f-b780-d8154be08165','2025-01-10 17:45:54','2025-01-11 01:45:54'),('Advanced Machine Learning Algorithms','Dive into advanced algorithms for machine learning models.','020578cb-0b83-465e-959d-400cecfb415e','c61da2e3-e995-4e34-89f8-52ba9fced5c8','2025-01-10 18:33:06','2025-01-11 02:33:06'),('Big Data Fundamentals with PySpark','Learn to conduct Big Data analysis using the Python package for Spark programming, PySpark, as well as higher-level libraries like SparkSQL and MLlib.','5d35a77a-0981-45da-81b6-24aa87b34b09','c6bb9272-c1c2-4d03-b898-66c45dd06c47','2025-01-10 18:16:15','2025-01-11 02:16:15'),('EDA for Data Science with Python','Master the art of Exploratory Data Analysis for data science.','47973087-99e9-44c2-b87e-fe185054256d','c733c347-ed3c-4e11-9bc3-b1243f9353d0','2025-01-10 18:36:15','2025-01-11 02:36:15'),('Linear Algebra for ML - Jason Brownlee','Jason Brownlee provides a detailed guide to linear algebra for machine learning, covering key topics like **matrix decomposition**, **SVD**, **eigenvalues**, and **eigenvectors**. The course includes Python code examples to help learners understand both the theory and implementation of these concepts in machine learning. [Read More on Machine Learning Mastery](https://machinelearningmastery.com/linear-algebra-for-machine-learning/).','d756bb07-e5c7-40d9-8b31-dc9a859672a5','c8a44ed3-dd45-4161-9036-1d67ae3e6023','2025-01-10 16:58:11','2025-01-11 00:58:11'),('Algorithms - Khan Academy','Khan Academy offers free tutorials on algorithms, including sorting, searching, and graph algorithms, as well as their applications. [Explore on Khan Academy](https://www.khanacademy.org/computing/computer-science/algorithms).','8df31585-b33f-40f8-a95a-b6f20a3435df','ca171a43-b7ab-4fa1-bf7f-c5a0fc50e4e1','2025-01-10 17:50:57','2025-01-11 01:50:57'),('R Programming for Data Science - edX','This edX course from Harvard University introduces R programming with a focus on statistical analysis and data visualization. Topics include working with data structures, performing statistical tests, and visualizing data with ggplot2. The course also includes hands-on coding assignments using RStudio, making it easy to practice as you learn. For further resources, check out: \n- [Harvard Data Science: R Programming](https://www.edx.org/course/data-science-r-programming)\n- [R Tutorial - YouTube](https://www.youtube.com/watch?v=_V8eKsto3Ug)\n- [R Documentation](https://www.rdocumentation.org/)','e95b2987-b1fc-4731-80cc-49f535b522fc','cb3e9b9c-183d-4147-a68f-5fa6f8f52480','2025-01-10 18:02:34','2025-01-11 02:02:34'),('Big Data Engineering with Hadoop & Spark','This specialization focuses on Hadoop & Spark to solve real-world Big Data problems with hands-on projects.','8634ec7e-c67d-47d3-8eed-337754f21202','cbd6369c-8250-4804-8a10-18c6f0a9149f','2025-01-10 18:26:35','2025-01-11 02:26:35'),('Cloud Computing for Data Science','Explore the use of cloud computing technologies in the field of data science.','28e0e1f6-2cd3-470e-8e05-19332a95bf21','cd45db74-a1de-4330-bf7b-9bf091673ac3','2025-01-10 18:30:06','2025-01-11 02:30:06'),('Natural Language Processing with Python','Learn NLP using Python programming language and libraries.','1899ca2c-be52-4920-a133-a46ad7933850','cf07ee13-a12b-4cff-a309-a22536ff884c','2025-01-10 18:31:10','2025-01-11 02:31:10'),('Data Manipulation with Pandas','Learn to manipulate, clean, and analyze data using Pandas.','563d1df9-c558-496e-bcce-da2c8a2c681b','d00f672d-617d-4367-9f6c-2bbb02b9ce8f','2025-01-10 18:38:12','2025-01-11 02:38:12'),('Linear Algebra with Python - Coursera','This Coursera course covers linear algebra concepts like **matrix operations**, **vector spaces**, **eigenvalue problems**, and **SVD**, all through practical Python examples using *NumPy* and *SciPy*. Perfect for learners who want to understand theory while solving problems in data science and machine learning. [Enroll on Coursera](https://www.coursera.org/learn/linear-algebra-with-python).','d756bb07-e5c7-40d9-8b31-dc9a859672a5','d0530f1b-69a7-4e25-9014-bce251118d59','2025-01-10 16:58:05','2025-01-11 00:58:05'),('Deep Dive into Advanced Machine Learning','Explore advanced techniques for deep learning and AI.','020578cb-0b83-465e-959d-400cecfb415e','d4c1f7ab-9218-4a56-a836-53a61b0df4fa','2025-01-10 18:33:07','2025-01-11 02:33:07'),('Matplotlib: Data Visualization Mastery','Become an expert in visualizing data with Matplotlib in Python.','7c3fe3cc-729a-4f64-9ffa-dda9bc94368e','d72f7743-33dc-4f60-a927-807f5b8c64e2','2025-01-10 18:37:19','2025-01-11 02:37:19'),('Intro to Linear Algebra - MIT OCW','MITs *OpenCourseWare* offers a free introduction to linear algebra, covering **vector spaces**, **matrix operations**, **linear transformations**, and **eigenvectors**. Taught by *Professor Gilbert Strang*, this course emphasizes both theory and practical application, providing downloadable materials, video lectures, and assignments. [Access MIT OCW](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/).','d756bb07-e5c7-40d9-8b31-dc9a859672a5','d9e86f09-2d73-4fb6-8ceb-c61d2c8687d1','2025-01-10 16:58:02','2025-01-11 00:58:02'),('Big Data Hadoop & Spark Developer Cert','Learn HDFS, YARN, MapReduce, and Apache Spark in this certification program for real-time data processing.','8634ec7e-c67d-47d3-8eed-337754f21202','dc27c917-8855-425d-a328-467f155b3825','2025-01-10 18:26:34','2025-01-11 02:26:34'),('Supervised Learning: Regression Basics','Learn the foundations of regression techniques in ML.','4012e606-0ada-41c4-923b-3e128463e27a','dcb5c8cf-ed76-40b5-b2e7-041b208033ae','2025-01-10 18:34:11','2025-01-11 02:34:11'),('Building Neural Networks with TensorFlow','Learn how to build and train neural networks in TensorFlow.','09ffa3d0-3468-4cf9-8bf2-1acb59b10226','e685dff4-5d16-4158-bb79-cecb72828043','2025-01-10 18:32:07','2025-01-11 02:32:07'),('Advanced Topics in Machine Learning','Study advanced topics like reinforcement learning.','020578cb-0b83-465e-959d-400cecfb415e','ea9cd87a-eb0a-4448-bd5a-ba42045fbf75','2025-01-10 18:33:09','2025-01-11 02:33:09'),('Statistics with Python - Coursera','This *Coursera* course from *University of Michigan* focuses on applying statistical methods using Python, including **hypothesis testing**, **confidence intervals**, and **data visualization**. It is designed for learners who want to enhance their data science skills by learning the statistical techniques used to analyze data. [Enroll on Coursera](https://www.coursera.org/specializations/statistics-with-python).','83e631af-5b05-4a16-bd42-a85bdb3b542e','ef661ce0-e2ac-4f5a-b824-328cd69660b3','2025-01-10 17:25:06','2025-01-11 01:25:06'),('Intro to Probability and Statistics - edX','This edX course from UC Berkeley offers an introduction to probability and statistics, focusing on concepts such as probability distributions, hypothesis testing, and regression analysis. The course includes interactive exercises and real-world examples. [Enroll on edX](https://www.edx.org/course/introduction-to-probability-and-statistics).','f5e6fb93-0ed1-47aa-92c9-38c45c840dc6','efcd1abd-3af1-4095-a60f-a9c78f7354c1','2025-01-10 17:45:42','2025-01-11 01:45:42'),('Algorithms Specialization - Coursera','This Coursera specialization from Stanford University dives deeper into algorithms, covering advanced topics like string algorithms, graph search, and NP-completeness. [Enroll on Coursera](https://www.coursera.org/specializations/algorithms).','8df31585-b33f-40f8-a95a-b6f20a3435df','f1cdde7f-28ec-4cfc-9936-719493ae123d','2025-01-10 17:51:00','2025-01-11 01:51:00'),('NLP with Deep Learning','Explore how deep learning models are used for NLP tasks.','1899ca2c-be52-4920-a133-a46ad7933850','f3ef57c0-7daf-4ff6-af17-f601229e5448','2025-01-10 18:31:08','2025-01-11 02:31:08'),('Advanced NLP with Python','Dive deeper into advanced NLP concepts and techniques.','1899ca2c-be52-4920-a133-a46ad7933850','f73cc091-f8cd-4eaa-8fdc-70ae4a92fed3','2025-01-10 18:31:11','2025-01-11 02:31:11'),('Text Mining and NLP','Discover text mining techniques and applications of NLP.','1899ca2c-be52-4920-a133-a46ad7933850','fc528d7a-48f6-44b6-9571-8208ce308829','2025-01-10 18:31:09','2025-01-11 02:31:09'),('Calculus for ML - Coursera','This Coursera course, *Calculus for Machine Learning*, is designed for learners looking to understand the calculus concepts that are foundational to machine learning. Topics covered include **differentiation**, **integration**, **multivariable calculus**, and **optimization**. With real-world examples, the course emphasizes how calculus is applied to machine learning algorithms such as gradient descent, cost functions, and optimization methods. The course also features practical coding assignments to implement these concepts using Python. [Enroll on Coursera](https://www.coursera.org/learn/calculus-machine-learning).','2e4a681c-5ea5-4269-ad08-fa671500a5d1','fcec4b87-1d40-4b8c-b750-9ca025f866ee','2025-01-10 17:19:28','2025-01-11 01:19:28'),('Regression Models in Supervised Learning','Master regression models for predictive analytics.','4012e606-0ada-41c4-923b-3e128463e27a','ffeeb0c5-6ff0-4ea6-b6ad-2b76b46c74b3','2025-01-10 18:34:10','2025-01-11 02:34:10');
/*!40000 ALTER TABLE `resource` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `room`
--

DROP TABLE IF EXISTS `room`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `room` (
  `title` varchar(250) NOT NULL,
  `duration` int NOT NULL,
  `link` text NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `room`
--

LOCK TABLES `room` WRITE;
/*!40000 ALTER TABLE `room` DISABLE KEYS */;
/*!40000 ALTER TABLE `room` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `user`
--

DROP TABLE IF EXISTS `user`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `user` (
  `name` varchar(224) NOT NULL,
  `email` varchar(224) NOT NULL,
  `address` varchar(224) NOT NULL,
  `phone_number` varchar(224) NOT NULL,
  `password_hash` varchar(1024) NOT NULL,
  `role` varchar(10) NOT NULL,
  `profile_image` varchar(65) NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `email` (`email`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `user`
--

LOCK TABLES `user` WRITE;
/*!40000 ALTER TABLE `user` DISABLE KEYS */;
INSERT INTO `user` VALUES ('Nouhan DOUMBOUYA','nouhandoumbouya@gmail.com','05200, Alor Setar, Kedah','01156686155','scrypt:32768:8:1$2nihgeg0U3nnHCrl$58d291306ce007dddffdd0d349860f7df672e283aef65dd3960602bd74dd37eac542d0b293b8b4ab16d21b07900a35e366f03977c7b650258a64a950fe988db9','Student','user.avif','6f66cd04-9deb-4295-bd1d-a5090238997e','2025-01-10 15:35:02','2025-01-10 23:35:02'),('Alisena DANISHWER','alisena@gmail.com','Jln Tun Razak, Bandar Alor Setar, 05200 Alor Setar, Kedah','01156686155','scrypt:32768:8:1$7JaA3X8s7Z4foQR6$402340472b3ff803e39b8d1652eb15a5db0c9cdb6379bb67101fa95a899b4de1aa915e07119cce04b9d1c51936323eda93f747926cd51c22d1688eb8acb67acb','Student','user.avif','d082550c-5b2e-49dd-9d67-7d4503787460','2025-01-10 15:37:06','2025-01-10 23:37:06'),('Antoine LENO','edupathwayadmin@gmail.com','05200 Alor Setar, Kedah','+60 11 56 68 61 55','scrypt:32768:8:1$IzQFkTL6K8hcE8k3$beedb822930815540837b08e4fd134ea13ac9e4a259010fd7125946ec5dc1f70af0dd79e07c9f086f0779d70c14efa5c05e060e926bd11abf84365a91e178c3b','Admin','fcff5d35-4d4f-4afa-a0dd-35d399d3884c.jpg','fcff5d35-4d4f-4afa-a0dd-35d399d3884c','2025-01-06 08:19:02','2025-01-06 16:19:02');
/*!40000 ALTER TABLE `user` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `user_quiz`
--

DROP TABLE IF EXISTS `user_quiz`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `user_quiz` (
  `user_id` varchar(60) NOT NULL,
  `course_id` varchar(60) NOT NULL,
  `score` int NOT NULL,
  `id` varchar(60) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `user_id` (`user_id`),
  KEY `course_id` (`course_id`),
  CONSTRAINT `user_quiz_ibfk_1` FOREIGN KEY (`user_id`) REFERENCES `user` (`id`) ON DELETE CASCADE,
  CONSTRAINT `user_quiz_ibfk_2` FOREIGN KEY (`course_id`) REFERENCES `course` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `user_quiz`
--

LOCK TABLES `user_quiz` WRITE;
/*!40000 ALTER TABLE `user_quiz` DISABLE KEYS */;
/*!40000 ALTER TABLE `user_quiz` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2025-01-12  1:05:30
